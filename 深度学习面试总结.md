# 调参经验



### 调参细节

```
学习率 / BatchSize
L1 L2正则化 / weight decany
DropOut / DropBlock
Batch Normalization / Group Normalization；
OneCycleLR + SGD / Adam(3e-4)
Warm Up / Early stopping
Multi Scale Training/Testing
DenseNet训练样本比较少的情况下收敛的比ResNet更好 -> Dense Connections
3x3 conv是CNN主流组件（3x3Conv有利于保持图像性质）
卷积核权重初始化使用xavier（Tanh）或者He normal（ReLU，PyTorch默认） 
cv2要比os读取图片速度快
加速训练pin_memory=true,work_numbers=x(卡的数量x4)，data.to(device,  no_blocking=True)

优化器+学习率策略+momentum：
	1.SGD+momentum在大学习率+大动量的时候效果更好;
	2.不管是SGD还是Adam还是AdamW，学习率的调整都对他们有帮助;
	3.带有momentum的SGD加余弦退火收敛更快且更加稳定;
	4.学习率最好设定好下限，不然后期会训练不动;
```



------



### 过拟合 & 欠拟合



**过拟合**指的是在训练集error越来越低，但是在验证集和测试集error越来越高；

```
数据增强
交叉验证
正则化（Weight Decay， L1，L2）
Early Stopping
Dropout / BN
early stopping
```

**欠拟合**指的是训练集提取特征较少，导致模型不能很好拟合训练集；

```
让模型更大  eg.ResNet-50 -> resNet-101；
减少正则化
错误分析：（训练集和测试集的分布偏差）测试时候出现问题进行分析，训练集缺少哪些情况导致错误，后续将在训练集中加入此类数据纠正偏差；
加入更多特征：添加其他特征项，添加多项式特征
```



------







------









# Pytorch & Python



```
from torch.utils import data
from torch import nn   # loss， optim， layer
```



------



### Pytorch默认梯度累积



1.当显存小，batchsize不够时，可以使用梯度累积变相增大batchsize；

2.weight在不同模型之间交互时候有好处；（动手学习深度学习v2）



------



### nn.Module & nn.Functional



**nn.Module**实现的layer是由class Layer(nn.Module)定义的特殊类，**会自动提取可学习参数nn.Parameter**

**nn.functional**中的函数更像是**纯函数**，由def function(input)定义，一般只定义一个操作，因为其无法保存参数



**Function**需要定义三个方法：**init, forward, backward**（需要自己写求导公式） 

**Module**只需定义 __init__和**forward**，而backward的计算由自动求导机制



**对于激活函数和池化层，由于没有可学习参数，一般使用nn.functional完成，其他的有学习参数的部分则使用nn.Module**

但是**Droupout**由于在训练和测试时操作不同，所以**建议使用nn.Module实现**，它能够通过**model.eval**加以区分



https://blog.csdn.net/wzy_zju/article/details/81262472?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242

https://blog.csdn.net/andyjkt/article/details/107428618



------



### DataLoader & Sampler & DataSet 



假设我们的数据是一组图像，每一张图像对应一个index，那么如果我们要读取数据就只需要对应的index即可，即上面代码中的`indices`，而选取index的方式有多种，有按顺序的，也有乱序的，所以这个工作需要`Sampler`完成，`DataLoader`和`Sampler`在这里产生关系

我们已经拿到了indices，那么下一步我们只需要根据index对数据进行读取即可了，这时`Dataset`和`DataLoader`产生关系

```
-------------------------------------
| DataLoader						|				
|									|							
|			Sampler -----> Indices	|  													
|                       |			|	
|      DataSet -----> Data			|
|						|			|			
------------------------|------------                    
						|s						
                        Training
```



DataLoader 的源代码初始化参数里有两种sampler：`sampler`和`batch_sampler`，都默认为`None`。前者的作用是生成一系列的index，而batch_sampler则是将sampler生成的indices打包分组，得到一个又一个batch的index



Pytorch中已经实现的`Sampler`有如下几种：`SequentialSampler` 	`RandomSampler`	 `WeightedSampler` 	`SubsetRandomSampler`



- 如果你自定义了`batch_sampler`,那么这些参数都必须使用默认值：`batch_size`, `shuffle`,`sampler`,`drop_last`.
- 如果你自定义了`sampler`，那么`shuffle`需要设置为`False`
- 如果`sampler`和`batch_sampler`都为`None`,那么`batch_sampler`使用Pytorch已经实现好的`BatchSampler`,而`sampler`分两种情况：
  - 若`shuffle=True`,则`sampler=RandomSampler(dataset)`
  - 若`shuffle=False`,则`sampler=SequentialSampler(dataset)`



Dataset定义方式如下：

```python
class Dataset(object):
	def __init__(self):
		...
		
	def __getitem__(self, index): # 能让该类可以像list一样通过索引值对数据进行访问
		return ...
	
	def __len__(self):
		return ...
```



https://www.cnblogs.com/marsggbo/p/11308889.html



------



### BN & Dropout





**一、PyTorch中BN层在训练和测试中有什么不同，如何实现？**



网络中定义了BN层，**BN层在训练过程中，会将一个Batch的中的数据转变成正太分布，在推理过程中使用训练过程中的参数对数据进行处理**，然而网络并不知道你是在训练还是测试阶段,所以使用`model.train()`和`model.eval()`来进行区分，



------



**二、Dropout在训练和测试时候的区别**



**Dropout在层与层之间加噪声，是一种正则**；**在全连接层使用，CNN用BN**

Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0来控制模型复杂度，以提高模型的泛化能力，减少过拟合；



Dropout 在**训练时**采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险；而在**测试时**，应该用整个训练好的模型，因此不需要dropout；



在测试时如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么**一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的**。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望；



`mask = (torch.Tensor(X.shape).uniform_(0, 1) > dropout).float()`

`return X * Mask / (1 - p)`

------



**三、PyTorch中BN细节**



在PyTorch中**将gamma和beta改叫weight、bias**，使得打印网络参数时候只会打印出weight和bias（PyTorch中只有可学习的参数才称为Parameter）,但是`Net.state_dict()`是有running_mean和running_var的**因为running_mean和running_var不是可以学习的变量，只是训练过程对很多batch的数据统计;**



BN层的**输出Y与输入X之间的关系**：**Y = (X - running_mean) / sqrt(running_var + eps) * gamma + beta**，其中**gamma、beta为可学习参数（在PyTorch中分别改叫weight和bias），训练时通过反向传播更新**；而**running_mean、running_var则是在前向时先由X计算出mean和var，再由mean和var以动量momentum来更新running_mean和running_var**，所以**在训练阶段，running_mean和running_var在每次前向时更新一次**；在**测试阶段，则通过`net.eval()`固定该BN层的running_mean和running_var，此时这两个值即为训练阶段最后一次前向时确定的值，并在整个测试阶段保持不变；**


先前batch的滑动统计值记为：mean_old, var_old，当前batch的统计值记为：mean_new, var_new：

```
训练时：
running_mean = (1 - momentum) * mean_old + momentum * mean_new
running_var = (1 - momentum) * var_old + momentum * var_new
```

```
测试时：
running_mean = mean_old
running_var = var_old
```


先更新running_mean和running_var，再计算BN；



------



**四、BN原理及细节**



**B x H x W，不涉及Channel，数据归一化方法**



**提出原因**

**解决Internal Covariate Shift**：训练数据在经过网络的每一层后其分布都发生了变化；

**缓解过拟合**：



```
批规范化（Batch Normalization，BN）：在minibatch维度上在每次训练iteration时对隐藏层进行归一化
标准化（Standardization）：对输入数据进行归一化处理
正则化（Regularization）：通常是指对参数在量级和尺度上做约束，缓和过拟合情况，L1 L2正则化
```



**作用**

可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失，并且起到一定的正则化作用，防止过拟合；



**归一化的目的**

将数据规整到统一区间，减少数据的发散程度，降低网络的学习难度；**BN的精髓在于归一之后，使用$\gamma, \beta$作为还原参数，在一定程度上保留原数据的分布；**



 $Input: B=\left\{x_{1 \ldots m}\right\} ; \gamma, \beta($ parameters to be learned $)$

$\text { Output }:\left\{y_{i}=B N_{\gamma, \beta}\left(x_{i}\right)\right\} \\$
$$
\begin{array}{r}

\mu_{B} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\
\sigma_{B}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2} \\
\tilde{x}_{i} \leftarrow \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}} \\
y_{i} \leftarrow \gamma \tilde{x}_{i}+\beta
\end{array}
$$

均值的计算，就是在一个批次内，将每个通道中的数字单独加起来，再除以 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+H+%5Ctimes+W) 。举个例子：该批次内有10张图片，每张图片有三个通道RBG，每张图片的高、宽是H、W，那么均值就是计算**10张图片R通道的像素数值总和**除以**![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)** ，再计算B通道全部像素值总和除以![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)，最后计算G通道的像素值总和除以![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)。方差的计算类似；

可训练参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%E3%80%81%5Cbeta) 的维度等于**张量的通道数**，在上述例子中，RBG三个通道分别需要一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) ，所以 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec%7B%5Cgamma%7D%E3%80%81%5Cvec%7B%5Cbeta%7D) 的维度等于3；



**训练**时，均值、方差分别是**该批次**内数据相应维度的均值与方差；

**推理**时，均值、方差是**基于所有批次**的期望计算所得，公式如下：
$$
\begin{aligned}
E[x] & \leftarrow E_{B}\left[\mu_{B}\right] \\
\operatorname{Var}[x] & \leftarrow \frac{m}{m-1} E_{B}\left[\sigma_{B}^{2}\right]
\end{aligned}
$$

------



**五、卷积层和BN层的融合**



**BN层最酷的地方是它可以用一个1x1卷积等效替换，更进一步地，我们可以将BN层合并到前面的卷积层中；**
$$
\begin{aligned}
y_{\text {conv }} &=w \cdot x+b \\
y_{b n} &=\gamma \cdot\left(\frac{y_{\text {conv }}-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
&=\gamma \cdot\left(\frac{w x+b-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
\hat{w} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot w \\
\hat{b} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot(b-E[x])+\beta \\
y_{b n} &=\hat{w} \cdot x+\hat{b}
\end{aligned}
$$


推导时, E[x] 为 runnin_mean(滑动均值), Var[x] 为 running_var(滑动方差)；



------



**六、BN和Dropdout同时使用出现的问题及解决方法**



**方差偏移现象**

Dropout 与 BN 之间冲突的关键是**网络状态切换过程中存在神经方差的不一致行为**。试想若有神经响应 X，当网络从训练转为测试时，Dropout 可以通过其随机失活保留率（即 p）来缩放响应，并在学习中改变神经元的方差，而 BN 仍然维持 X 的统计滑动方差。这种方差不匹配可能导致数值不稳定。而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能。事实上，如果没有 Dropout，那么实际前馈中的神经元方差将与 BN 所累计的滑动方差非常接近，这也保证了其较高的测试准确率。



**解决方案**

1.在所有 BN 层后使用 Dropout；

2.修改 Dropout 的公式让它对方差并不那么敏感，就是高斯Dropout、均匀分布Dropout；



------



### model.eval()  vs  torch.no_grad()



**两者都在Inference时候使用，但是作用不相同：**

```
model.eval() 负责改变batchnorm、dropout的工作方式，如在eval()模式下，dropout是不工作的；
torch.no_grad() 负责关掉梯度计算，节省eval的时间；
```

只进行Inference时，`model.eval()`是必须使用的，否则会影响结果准确性。 而`torch.no_grad()`并不是强制的，只影响运行效率；



------



### 迭代器 & 生成器



**迭代器：**实现了__iter__和__next__方法的对象都称为迭代器。迭代器是一个有状态的对象，在调用next() 的时候返回下一个值，如果容器中没有更多元素了，则抛出StopIteration异常；



**生成器：**其实是一种特殊的迭代器，但是不需要像迭代器一样实现__iter__和__next__方法，**自动实现迭代器协议**；Python有两种不同的方式提供生成器：

​	1.**生成器函数：**常规函数定义，但是，使用yield语句而不是return语句返回结果。yield语句一次返回一个结果，在每个结果中间，挂起函数的状态，以便下次重它离开的地方继续执行

​	2.**生成器表达式：**类似于列表推导，但是，生成器返回按需产生结果的一个对象，而不是一次构建一个结果列表



**生成器的好处： **

​	1.**延迟计算：**一次返回一个结果，它不会一次生成所有的结果，这对于大数据量处理，将会非常有用；

​	2.**提高代码可读性**；



**生成器注意事项：**

​	**生成器只能遍历一次**



------



### 类实例方法 & 类方法 & 类静态方法



**采用 @classmethod 修饰的方法为类方法；**

**采用 @staticmethod 修饰的方法为类静态方法；**

**不用任何修改的方法为实例方法；**

其中 @classmethod 和 @staticmethod 都是函数装饰器



**Python类实例方法:**

在类中定义的方法默认都是实例方法，类的构造方法理论上也属于实例方法；

```python
class CLanguage:
    #类构造方法，也属于实例方法
    def __init__(self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
    # 下面定义了一个say实例方法
    def say(self):
        print("正在调用 say() 实例方法")
```

实例方法最大的特点就是，它最少也要包含一个 self 参数，用于绑定调用此方法的实例对象。实例方法通常会用类对象直接调用，例如：

```python
clang = CLanguage()
clang.say()
```



**Python类方法：**

Python 类方法和实例方法相似，它最少也要包含一个参数，只不过类方法中通常将其命名为 cls，Python 会自动将类本身绑定给 cls 参数（注意，绑定的不是类对象）。也就是说，我们在调用类方法时，无需显式为 cls 参数传参；

和实例方法最大的不同在于，类方法需要使用`＠classmethod`修饰符进行修饰（如果没有` ＠classmethod`，则 Python 解释器会将 info() 方法认定为实例方法，而不是类方法），例如：

```python
class CLanguage:
    #类构造方法，也属于实例方法
    def __init__(self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
    #下面定义了一个类方法
    @classmethod
    def info(cls):
        print("正在调用类方法",cls)
```

类方法推荐使用类名直接调用，当然也可以使用实例对象来调用：

```python
#使用类名直接调用类方法
CLanguage.info()
#使用类对象调用类方法
clang = CLanguage()
clang.info()
```



**Python类静态方法：**

静态方法，其实就是我们学过的函数，和函数唯一的区别是，静态方法定义在类命名空间中，而函数则定义在程序所在的全局命名空间中；

静态方法没有类似`self`、`cls` 这样的特殊参数，因此 Python 解释器不会对它包含的参数做任何类或对象的绑定。也正因为如此，类的静态方法中无法调用任何类属性和类方法；

静态方法需要使用`＠staticmethod`修饰，例如：

```python
class CLanguage:
    @staticmethod
    def info(name,add):
        print(name,add)
```

静态方法的调用，既可以使用类名，也可以使用类对象，例如：

```python
#使用类名直接调用静态方法
CLanguage.info("C语言中文网","http://c.biancheng.net")
#使用类对象调用静态方法
clang = CLanguage()
clang.info("Python教程","http://c.biancheng.net/python")
```









# 深度学习



------



### 为什么现在深度学习火热



```
1.大规模数据集
2.计算机算力的提升
3.特征提取能力
```



------



### 梯度消失 & 梯度爆炸



目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要**链式法则（Chain Rule）**的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。

而链式法则是一个**连乘的形式**，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度**反向传播**的方式对深度网络权值进行更新时，得到的**梯度值接近0**或**特别大**，也就是**梯度消失**或**爆炸**。梯度消失或梯度爆炸在本质原理上其实是一样的。



**产生原因：**

```
梯度消失：1.深层网络 2.损失函数（Sigmoid）

梯度爆炸：1.深层网络 2.Weights初始化值太大
```



**解决方法：**

```
梯度剪切：对梯度设定阈值
权重正则化
ReLU等
ShortCut
Batch Normalization
参数初始化（He）
LSTM门机制
```



------



### L1 L2正则化



**深度学习一般正则化不放在optim里面，权重衰减通过控制L2正则项使得模型参数不会过大，从而控制模型复杂度**

**正则化其他方法：1. EMA of Weights  2. Label Smoothing   3. RandAugment  4. Dropout on FC  5.Decrease weight decay**



正则化之所以能够降低过拟合的原因在于，**正则化是结构风险最小化的一种策略实现**

给loss function加上正则化项，能使得新得到的优化目标函数**h = f(w, b)+normal(w)**，需要在f和normal中做一个权衡，如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，normal引入使得最优解向原点移动，因此可以看出**加正则项能让解更加简单，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度**；

 

**L1正则化**就是在loss function后边所加**正则项为L1范数**，**加上L1范数容易得到稀疏解**（0比较多），**L1范数趋向于产生少量的特征，而其他的特征都是0**；

**L2正则化**就是loss function后边所加**正则项为L2范数的平方**，**加上L2正则相比于L1正则来说，得到的解比较平滑**（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多；**L2范数可以防止过拟合，提升模型的泛化能力，L2会选择更多的特征，这些特征都会接近于0**；



**正则项权重是控制模型复杂的超参数；**



------



### BN & LN & IN



对于[B,C,W,H]这样的训练数据而言，

BN是在[B,W,H]维度求均值方差进行规范化 -> CNN

LN是对[C,W,H]维度求均值方差进行规范化   -> RNN

IN是对[W,H]维度求均值方差进行规范化  -> 图像风格化

GN先对通道进行分组，每个组内的所有[$C_i$,W,H]维度求均值方差进行规范化，与BatchSize无关



------



### ResNet & DenseNet



**一. ResNet和DenseNet比较**



**ResNet：稀疏连接，  Add， 训练速度快， 参数量相对较多**

**DenseNet：密集连接，Concat， 训练速度慢（concat需要频繁读取内存）， 参数量相对较少**

- **DenseNet比传统的卷积网络所需要的参数更少：**密集连接带来了特征重用，不需要重新学习冗余的特征图，而且维度拼接的操作，带来了丰富的特征信息，利用更少的卷积就能获得很多的特征图;

- **DenseNet提升了整个网络中信息和梯度的流动，对训练十分有利：**密集连接使得每一层都可以直接从损失函数和原始输入信号获得梯度，对于训练更深的网络十分有利，同时密集连接的网络结构有正则化的效果，能够减少过拟合风险；

- **对显存需求**



**二. ResNet解决了什么问题**



- **网络性能退化能力，**单纯的堆积网络正确率不升反降：按理说，当我们堆叠一个模型时，理所当然的会认为效果会越堆越好。因为，假设一个比较浅的网络已经可以达到不错的效果，那么即使之后堆上去的网络什么也不做，模型的效果也不会变差。然而事实上，这却是问题所在。“什么都不做”恰好是当前神经网络最难做到的东西之一； ->  恒等映射

- 即使BN过后**梯度的模**稳定在了正常范围内，但梯度的相关性实际上是随着层数增加持续衰减的。而经过证明，**ResNet可以有效减少梯度相关性的衰减**；
    对于L层的网络来说，没有残差表示的Plain Net梯度相关性的衰减在1 / 2^L ，而ResNet的衰减却只有 1 / sqrt(L)

- **梯度弥散观点**：在输出引入一个输入x的恒等映射，则梯度也会对应地引入一个常数1，这样的网络的确不容易出现梯度值异常，在某种意义上，起到了**稳定梯度**的作用；

- **shortcut相加可以实现不同分辨率特征的组合**，因为浅层容易有高分辨率但是低级语义的特征，而深层的特征有高级语义，但分辨率就很低了；

- **shortcut实际上让模型自身有了更加“灵活”的结构**，即在训练过程本身，模型可以选择在每一个部分是“更多进行卷积与非线性变换”还是“更多倾向于什么都不做”，抑或是将两者结合；



**三. ResNet两种结构具体怎么实现，BottleNeck的作用**



- **两个3x3卷积和一个shortcut**  
- **两个1x1卷积中间加一个3x3卷积，然后再加一个shortcut**



​	**BottleNeck作用：降低维度，模型压缩，减少计算量**

​	卷积核的尺寸是Dk×Dk×M，一共有N个，每一个都要进行Dw×Dh次运算，所以标准卷积的计算量是：Dk x Dk x M x N x Dw x Dh

​	FLOPs：10^9 级别



**四. DenseNet和ResNet哪个比较好**

​	**在小数据集，DenseNet比ResNet要好，因为小数据集的时候容易产生过拟合，但是DenseNet能够很好的解决过拟合的问题。**DenseNet 具有非常好的抗过拟合性能，尤其适合于训练数据相对匮乏的应用。这一点从论文中 DenseNet 在不做数据增强（data augmentation）的 CIFAR 数据集上的表现就能看出来。对于 DenseNet 抗过拟合的原因有一个比较直观的解释：神经网络每一层提取的特征都相当于对输入数据的一个非线性变换，而随着深度的增加，变换的复杂度也逐渐增加（更多非线性函数的复合）。相比于一般神经网络的分类器直接依赖于网络最后一层（复杂度最高）的特征，DenseNet 可以综合利用浅层复杂度低的特征，因而更容易得到一个光滑的具有更好泛化性能的决策函数；



------



### Inception



**Inception使用split-transform-merge策略把multi-scale filter生成的不同感受野的特征融合到一起，**有利于识别不同尺度的对象；

 现在这种思想已经作为基础组件使用，和它特别像的是SPP；

- v1：1x1 3x3 5x5 不同感受野；
- v2/v3：两个3x3代替一个5x5；
- v4：和ResNeXt特别像，唯一的区别是merge和1x1的位置；



------



### 移动端/轻量化模型



**MobileNet：**

- v1：
  - **深度可分离卷积 + ReLU6**：3x3深度卷积， 1x1升维；


- v2：
  - 深度卷积训出来的卷积核有不少是空的 -> 在低维度ReLU使得信息丢失，所以**Inverted residuals：**1x1先升维，3x3深度卷积， 1x1降维，最后的ReLU6替换为Add；


- v3：

  - 使用NAS 

  - v2的Inverted residuals
  - SE注意力 4.h-swish激活函数




**ShuffleNet：**

- v1：
  - **pointwise group convolution**（降低1x1卷积的计算量）
  - **channel shuffle**（解决不同组之间的特征图不通信）
- v2：
  - **平衡输入输出通道**(in = out) 
  - **谨慎使用组卷积** 
  - **避免网络碎片化**(一些操作可以合并) 
  - **减少元素级运算**(concat代替concat)



------



### 数据集划分



**验证集要和训练集来自于同一个分布（shuffle数据集然后开始划分），测试集尽可能贴近真实数据**

6 训练 + 2 验证 + 2 测试

刚开始的时候，用训练集训练，验证集验证，确定超参数和一些细节；在验证集调到最优后，再把验证集丢进来训练，在测试集上测试；



------



### 卷积



**一、卷积神经网络特点**

```
1.局部连接 2.权值共享 3.层次结构
```



------



**二、卷积有哪些**



- **3x3卷积**

   - 底层专门做过优化，已经成为主流组件；



- **1x1卷积**
  - 升维降维，减少参数；
  - 通道融合；
  - 增加非线性（利用后接的非线性激活函数如ReLU）；



- **空洞卷积**

  - kernal之间增加空洞，增加感受野，图森组针对空洞卷组专门做过研究：[1, 3, 5, 1, 3, 5]这样的空洞率；
  - 虽然增大了感受野，但是使得特征更加稀疏；
  - 解决了网格效应；



- **转置卷积**

  **会出现棋盘效应：**由于转置卷积的“不均匀重叠” -> **1.采取可以被步长整除的卷积核长度  2.插值**；

  对于同一个卷积核（因非其稀疏矩阵不是正交矩阵），结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状，上采样常用双线性插值；

  ![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpDiaSQ8lkIWnNEAtPYtPBXKm0Txqvm0BamZB6bTvAqibFMlgeSyHwJakB8M4fia1ibh4ekhwKKkkdx0w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  ![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpDiaSQ8lkIWnNEAtPYtPBXKkJYoWEF0iaPRTFbg33KhticTOwwTHhhEAmN5ZGyAsiayGNMWxPRQtPbGg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  

- **深度可分离卷积**

  **计算成本仅仅是2D卷积的12%左右**：对于规模较小的模型，如果将普通卷积替换为深度可分卷积，其模型大小可能会显著降低，模型的能力可能会变得不太理想，因此得到的模型可能是次优的；但如果使用得当，深度可分卷积能在不牺牲模型性能的前提下显著提高效率；

  

  **MobileNet v1中提出：深度卷积 + 1x1卷积**



**参数量**：卷积核的尺寸是Dk×Dk×M，一共有N个

```
**标准卷积**的参数量是：Dk x Dk x M x N
**深度卷积**的卷积核尺寸Dk×Dk×M；逐点卷积的卷积核尺寸为1×1×M，一共有N个，所以深度可分离卷积的参数量是：Dk x Dk x M + M x N
```

**计算量：**普通卷积核的尺寸是Dk×Dk×M，一共有N个，每一个都要进行Dw×Dh次运算

```
**标准卷积**的计算量是：Dk x Dk x M x N x Dw x Dh
**深度卷积**的卷积核尺寸Dk×Dk×M，一共要做Dw×Dh次乘加运算；逐点卷积的卷积核尺寸为1×1×M，有N个，一共要做Dw×Dh次乘加运算，所以深度可分离卷积的计算量是：Dk x Dk x M x Dw x Dh +  M x N x Dw x Dh
```



**参数量和运算量均下降为原来的：**$\frac{1}{N}+\frac{1}{D_k^2}$



------



**三、卷积和互相关的关系**



**卷积**是透过两个函数$f$和$g$生成第三个函数的一种数学算子，表征函数$f$与经过翻转和平移的$g$的乘积函数所围成曲边梯形的面积；

**互相关**是两个函数之间的滑动点积或滑动内积，互相关中的过滤不经过反转，而是直接滑过函数$f$，$f$与$g$之间的交叉区域即是互相关；

**严格意义上来说，深度学习中的“卷积”是互相关(Cross-correlation)运算，本质上执行逐元素乘法和加法**。但在之所以习惯性上将其称为卷积，是因为过滤器的权值是在训练过程中学习得到的；



------



### 感受野



感受野大小计算公式：
$$
r_{l}=r_{l-1}+\left(k_{l}-1\right) * \prod_{i=0}^{l-1} s_{i}
$$
其中 $r_{l-1}$ 为第 $l-1$ 层的感受野大小, $k_{l}$ 为第l层的卷积核大小 $($ 也可以是Pooling $), s_{i}$ 为第 $i$ 层的卷 积步长。一般来说 $r_{0}=1, s_{0}=1$ ；



**一、CNN中感受野定义及性质**

​	在深度神经网络中，每个神经元节点都对应着输入图像的某个确定区域，仅该区域的图像内容能对相应神经元的激活产生影响，那么这个区域称为该神经元的感受野；

- **越靠近感受野中心的区域越重要**
- **各向同性**
- **由中心向周围的重要性衰减**速度可以通过网络结构控制





有效感受野感受图像中间部分



------



### Pooling



**作用：**

- **保持不变性**：平移，旋转，尺度；
- **保留主要的特征**同时**减少参数**(降维，效果类似PCA)和**计算量**，防止过拟合，提高模型**泛化能力**；



**Max & Mean pooling前向和反向传播**



**Mean Pooling：**

 在forward的时候，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值；**backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了；**（假设pooling的窗大小是2x2）

```
forward: [1 3; 2 2] -> [2]
backward: [2] -> [0.5 0.5; 0.5 0.5]
```



**Max Pooling：**

在forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，**backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0；**

```
forward: [1 3; 2 2] -> [3]
backward: [3] -> [0 3; 0 0]
```



------



### 过采样 / 欠采样

 https://www.zhihu.com/question/269698662/answer/352279936

原始数据大小为![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B1831%5Ctimes21%7D)，1831条数据，每条数据有21个特征：其中正例176个（9.6122%），反例1655个（90.3878%），类别不平衡;



**欠采样：**从反例中随机选择176个数据，与正例合并（ ![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B352%5Ctimes21%7D) ）

**过采样：**从正例中反复抽取并生成1655个数据（势必会重复），并与反例合并（ ![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B3310%5Ctimes21%7D) ）



- 采样方法一般比直接调整阈值的效果要好；
- 使用采样方法（过采样和欠采样）一般可以提升模型的泛化能力，但有一定的过拟合的风险，应搭配使用正则化模型；
- 过采样的结果较为稳定，过采样大部分时候比欠采样的效果好；




**过采样**带来**更大的运算开销**，当数据中噪音过大时，结果反而可能会更差因为**噪音也被重复使用**；

尝试**半监督学习**的方法；注意积累样本；数据增强；欠采样的时候可以训练多个模型，最后尝试模型投票的方法；



------



### Kaiming初始化

```
1. 前向传播的时候, 每一层的卷积计算结果的方差为1；
2. 反向传播的时候, 每一 层的继续往前传的梯度方差为1(因为每层会有两个梯度的计算, 一个用来更新当前层的权重, 一个继续传播,用于前面层的梯度的计算)；
```



------



### 优化器

https://blog.csdn.net/google19890102/article/details/69942970

https://ruder.io/optimizing-gradient-descent/index.html



**1. 二阶导是向量，学术上比较好，但难算，一阶实用   2. 只关注收敛在哪个地方，SGD一步一步来，二阶收敛效果不一定比一阶收敛效果好**



**SGD**（小批量梯度下降）

对于含有n个训练样本的数据集，每次参数更新，选择一个大小为m ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft+%28+m+%3C+n+%5Cright+%29) 的mini-batch数据样本计算其梯度，其参数更新公式如下：
$$
\theta_{t+1}=\theta_{t}-\alpha \cdot \frac{1}{m} \cdot \sum_{i=x}^{i=x+m-1} \cdot \nabla_{\theta} J_{i}\left(\theta, x^{i}, y^{i}\right)
$$

- 选择合适的learning rate比较困难 ，学习率太低会收敛缓慢，学习率过高会使收敛时的波动过大
- 所有参数都是用同样的learning rate
- SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点



**Adam**

除了像Adadelta和RMSprop一样存储一个指数衰减的历史平方梯度的平均vt，Adam同时还保存一个历史梯度的指数衰减均值mt，类似于动量：

$m_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t}$
$v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}$

**mt和vt分别是对梯度的一阶矩（均值）和二阶矩（非确定的方差）的估计**，正如该算法的名称。当mt和vt初始化为0向量时，Adam的作者发现它们都偏向于0，尤其是在初始化的步骤和当衰减率很小的时候（例如β1β1和β2β2趋向于1）。通过计算**偏差校正**的一阶矩和二阶矩估计来抵消偏差：

$\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}$
$\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}$

正如我们在Adadelta和RMSprop中看到的那样，他们利用上述的公式更新参数，由此生成了Adam的更新规则：
$$
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}
$$



------



### 激活函数



- **Sigmoid**

  更倾向于更新靠近输出层的参数，而不是靠近输入层的参数

  导数为$f(x) * (1 - (f(x)))$

  导数取值范围【0，0.25】

  左右两侧都是**近似饱和区**，导数太小，容易造成梯度消失

  涉及指数运算，容易溢出

  输出值不以零为中心，会导致模型收敛速度慢

  激活函数的**偏移现象**

  

- **ReLU**

  Dead ReLU：当 $x <0$ 时，ReLU 输出恒为零。反向传播时，梯度恒为零，参数永远不会更新；

  激活部分神经元，增加稀疏性；

  计算简单，收敛速度快；



-  **ReLu6**  

  1. ReLU6比ReLU能更早学习到稀疏特征 
  2. 增强浮点数的小数位表达能力（整数位最大是6，所以只占3个bit，其他bit全部用来表达小数位）

  

- **Leaky ReLU**

  $LeakyReLU(x) = max(0.01x, x)$

  解决ReLU Dead；

  

- **Swish**

  $f(x) = x * sogmoid(\beta x)$

  可以看做是**介于线性函数与ReLU函数之间的平滑函数。**β是个常数或可训练的参数，Swish 具备**无上界有下界、平滑、非单调**的特性;



- **Hard-Swish**（[Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)）
  $$
  \mathrm{h}-\mathrm{swish}(x)=x \frac{\operatorname{ReLU} 6(x+3)}{6}
  $$



- **GELU**  

   $Φ(x)$ 是符合高斯分布的累积分布函数
  $$
  \operatorname{GELU}(x)=x * \Phi(x)
  $$

  

------



### **分类损失函数**



A loss function is a part of a cost function which is a type of an objective function.



**Binary Cross Entropy Loss**

 $BCELoss:-\Sigma y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(p_{i}\right)$

$\mathrm{CE}(p, y)=\left\{\begin{array}{ll}-\log (p) & \text { if } y=1 \\ -\log (1-p) & \text { otherwise }\end{array}\right.$

其中![[公式]](https://www.zhihu.com/equation?tex=y%5Cin%5C%7B-1%2C+1%5C%7D)为真实标签，1表示为正例，-1表示为负例；而![[公式]](https://www.zhihu.com/equation?tex=p%5Cin%5B0%2C+1%5D)为模型预测为正例的概率值；



**Focal Loss**

与抽样方法不同，Focal Loss从另外的视角来解决样本不平衡问题，那就是根据置信度动态调整交叉熵loss，当预测正确的置信度增加时，loss的权重系数会逐渐衰减至0，这样模型训练的loss更关注难例，而大量容易的例子其loss贡献很低

解决了one-stage算法中**正负样本的比例失衡**：在CE基础上增加了一个调节因子$(1-p_t)^{\gamma}$
$$
F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log p_{t}
$$
${\gamma=2}$最好，FL相比CE可以大大降低简单例子的loss，使模型训练更关注于难例；



------



### 回归损失函数



Smooth L1 Loss![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) IoU Loss![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) GIoU Loss ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) DIoU Loss ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) CIoU Loss

------



**一、Smooth L1 Loss**
$$
\text { smooth }_{L_{1}}(x)=\left\{\begin{array}{cc}
0.5 x^{2} & i f|x|<1 \\
|x|-0.5 & \text { otherswise }
\end{array}\right.
$$

从损失函数对x的导数可知：

 ![[公式]](https://www.zhihu.com/equation?tex=L_%7B1%7D)损失函数对x的导数为常数，在训练后期，x很小时，如果学习率不变，损失函数会在稳定值附近波动，很难收敛到更高的精度；

 ![[公式]](https://www.zhihu.com/equation?tex=L_%7B2%7D) 损失函数对x的导数在x值很大时，其导数也非常大，在训练初期不稳定；

![[公式]](https://www.zhihu.com/equation?tex=smooth_%7BL_%7B1%7D%7D%5Cleft%28+x+%5Cright%29+) **完美的避开了** ![[公式]](https://www.zhihu.com/equation?tex=L_%7B1%7D)**和** ![[公式]](https://www.zhihu.com/equation?tex=L_%7B2%7D)**损失的缺点**



上面的三种Loss用于计算目标检测的Bounding Box Loss时，独立的求出4个点的Loss，然后进行相加得到最终的Bounding Box Loss，这种做法的假设是4个点是相互独立的，实际是有一定相关性的；



------



**二、IoU Loss**

![img](https://pic2.zhimg.com/80/v2-090938dacc24098c4e54aa18968f375d_1440w.jpg)



------



**三、GIoU Loss**



- **当预测框和目标框不相交时**，IoU(A,B)=0，不能反映A,B距离的远近，此时损失函数不可导，IoU Loss 无法优化两个框不相交的情况；
- 假设预测框和目标框的大小都确定，只要两个框的相交值是确定的，**其IoU值是相同时，IoU值不能反映两个框是如何相交的**；




$$
G I o U=I o U-\frac{|C \backslash(A \cup B)|}{|C|}
$$

- GIoU具有尺度不变性；
- 当 ![[公式]](https://www.zhihu.com/equation?tex=A%5Crightarrow+B) 时，两者相同都等于1，此时 ![[公式]](https://www.zhihu.com/equation?tex=GIoU) 等于1；当 ![[公式]](https://www.zhihu.com/equation?tex=A%E5%92%8CB) 不相交时， ![[公式]](https://www.zhihu.com/equation?tex=GIoU%5Cleft%28+A%2CB+%5Cright%29+%3D+-1)



------



**四、DIoU Loss**



![img](https://pic2.zhimg.com/80/v2-d32d8fd6e32ecca603ea9678695b7241_1440w.jpg)

​											**当目标框完全包裹预测框的时候，IoU和GIoU的值都一样，此时GIoU退化为IoU, 无法区分其相对位置关系；**



好的目标框回归损失应该考虑三个重要的几何因素：**重叠面积，中心点距离，长宽比；**

**DIoU Loss**，相对于GIoU Loss**收敛速度更快**，该Loss考虑了**重叠面积和中心点距离**，但没有考虑到长宽比；

**CIoU Loss**，**其收敛的精度更高**，以上**三个因素都考虑到了**；


$$
L_{D I o U}=1-I o U+\frac{\rho^{2}\left(b, b^{g t}\right)}{c^{2}}
$$
其中 ![[公式]](https://www.zhihu.com/equation?tex=b%E5%92%8Cb%5E%7Bgt%7D) 分别表示 ![[公式]](https://www.zhihu.com/equation?tex=B%E5%92%8CB%5E%7Bgt%7D) 的中心点， ![[公式]](https://www.zhihu.com/equation?tex=%5Crho%5Cleft%28+%5Ccdot+%5Cright%29) 表示欧式距离， ![[公式]](https://www.zhihu.com/equation?tex=c) 表示 ![[公式]](https://www.zhihu.com/equation?tex=B%E5%92%8CB%5E%7Bgt%7D) 的最小外界矩形的对角线距离；



- 尺度不变性；
- 当两个框完全重合时， ![[公式]](https://www.zhihu.com/equation?tex=L_%7BIoU%7D%3DL_%7BGIoU%7D%3DL_%7BDIoU%7D%3D0) ,当2个框不相交时![[公式]](https://www.zhihu.com/equation?tex=L_%7BGIoU%7D%3DL_%7BDIoU%7D%5Crightarrow+2)；
- DIoU Loss可以直接优化2个框直接的距离，比GIoU Loss收敛速度更快；



------


**五、CIoU Loss**



CIoU的惩罚项是在DIoU的惩罚项基础上加了一个影响因子 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha%5Cupsilon) ，这个因子把**预测框长宽比拟合目标框的长宽比**考虑进去；

![[公式]](https://www.zhihu.com/equation?tex=L_%7BCIoU%7D+%3D+1-+IoU+%2B%5Cfrac%7B%5Crho%5E%7B2%7D%5Cleft%28+b%2Cb%5E%7Bgt%7D+%5Cright%29%7D%7Bc%5E%7B2%7D%7D+++%2B+%5Calpha%5Cupsilon)



------



### 提高模型速度



- TensorRT
- amp 混合精度 + float16
- 将训练时候的3x3conv， 1x1conv， Identity在推理时融合为一个3x3conv



------



### 降低网络复杂度但不影响精度



- 模型压缩：通道剪枝 / 权重剪枝
- 蒸馏
- 重新设计卷积代替普通卷积：深度可分离卷积，RepVGG



------



# 目标检测



## Anchor Free（One-Stage）



- Anchor-Based检测性能**对于anchor的大小，数量，长宽比都非常敏感**，这些固定的anchor极大地损害了检测器的普适性，导致对于不同任务，其anchor都必须重新设置大小和长宽比；
- 为了去匹配真实框，需要生成大量的anchor，但是**大部分的anchor在训练时标记为negative，所以就造成了样本间的不平衡**；
- 在训练中，需要计算**所有anchor与真实框的IOU，这样就会消耗大量内存和时间**；



**感受野可以看作是天然的Anchor**



### FOCS



**语义分割的思想逐像素点的来解决检测问题**



**正负样本：**

如果一个location(x, y)落到了任何一个GT box中，那么它就为正样本并标签为类别 $c^{*}$ 用于分类问题，我们还可以得到一个4D的向量 $\left(l^{*}, t^{*}, r^{*}, b^{*}\right)$, 也就是这个 点到左、上、右、下边的距离

因此根据我们训练目标，最后一层的网络输出为一个80D的向量 $p$ 用于分类层, 4D的向量也就是 $(\mathrm{I}, \mathrm{t}, \mathrm{r}, \mathrm{b})$, 用于回归层。跟RetinaNet一样，我们使用4个卷积层的backbone用于分类和回归, 接着由于回归目标 $(\mathrm{I}, \mathrm{t}, \mathrm{r}, \mathrm{b})$ 都是正的, 作者使用exp()将其映射到范围 (0, 正无穷)



**损失函数分为三个分支：**

-  centerness，交叉熵loss
- 对于分类层使用的为focal loss用于平衡样本
- 而回归层使用IOU loss



1.**FCOS通过直接限定不同特征级别的边界框的回归范围来进行分配：**作者为了减少尺度差异大的物体重叠，引入参数 $m_{i}$ 为特征层 $i$ 的最大距离，如果一个 location $(\mathrm{x}, \mathrm{y})$ 满足 $\max \left(l^{*}, t^{*}, r^{*}, b^{*}\right)>m_{i}$ 或者 $\max \left(l^{*}, t^{*}, r^{*}, b^{*}\right)<m_{i-1}$, 那么我们在这个特征层就将其视为负样本，不在进行回归。其中 $m_{i}$ 分别设置为0,64,128,256,512和正无穷，正好可以形成5个区间，在5个层上进行限制尺寸以减少重叠区域

2.如果**一个像素点在同一层落到了多个GT区域，那么我们就取最小的那个当做回归目标**（利用FPN进行多尺度预测会极大地减少这种情况的发生）



**FCOS距离目标中心较远的位置产生很多低质量的预测边框：**这是由于我们把中心点的区域扩大到整个物体的边框，经过模型优化后可能有很多中心离GT box中心很远的预测框，为了增加更强的约束，作者提出了Center-ness的方法来解决这个问题，Center-ness取值为0,1之间，使用交叉熵损失进行训练

**测试时，将预测的中心度与相应的分类分数相乘，计算最终得分**
$$
\text { centerness* }=\sqrt{\frac{\min \left(l^{*}, r^{*}\right)}{\max \left(l^{*}, r^{*}\right)} \times \frac{\min \left(t^{*}, b^{*}\right)}{\max \left(t^{*} b^{*}\right)}}
$$
中心度可以降低远离对象中心的边界框的权重：当loss越小时，centerness就越接近1，也就是说回归框的中心越接近真实框。那么为什么要使用这么大的区域呢？

作者解释，从anchor-based角度，我们通过两个IOU阈值将标定的anchor分为negative、ignored和positive三种，这样不能充分的利用正样本。而在FCOS中，只要没有location没有落在GT Box区域，那么就是负样本，反之为正样本。增大区域为了获得更多的正样本以更好地用于回归器中。



**改进**

```
1.centerness分支与cls分支共享前面几个卷积 -> centerness分支与reg分支共享前面几个卷积 :0.5 mAP
2.GT bbox内的点，分类时均作为正样本 -> 只有GT bbox中心附近的一定范围内的小bbox内的点，分类时才作为正样本
3.bbox loss weight：所有正样本点的 weight 平权 -> 将样本点对应的 centerness 作为权重，离 GT 中心越近，权重越大
4.IoU loss -> GIoU loss -> CIoU loss
5.centerness 分支利用 l，t，r，b 计算 centerness -> 直接用 IoU :目标的 centerness 值比较小，最终cls分数很容易被阈值卡掉

```



### CornerNet



**CornerNet 是根据一对关键点来进行物体的检测的（左上点坐标（top-left corner）和右下角坐标）**



**如何匹配同一物体bounding box的左上角和右下角**

```
cornerNet在进行预测的时候，会为每个点分配一个embedding vector，属于同一物体的点的vector的距离较小
```

![img](https://pic3.zhimg.com/80/v2-387bd6fcd1ee56e68f160982e51f2f5e_1440w.jpg)



**主要创新**

- anchor-free 的目标检测新思路
-  corner pooling的提出
-  cornerNet网络的提出



**corner pooling**

1.**目的**是为了建立点corner和目标的位置关系
2.当求解某一个点的 top-left corner pooling时 ，就是以该点为起点，水平向右看遇到的最大值以及竖直向下看最大的值之和

实现：方向颠倒后，我们每次都将**沿着该方向**上**遇到**的**最大的值**作为**填充值**即可快速实现 corner pooling（log(1)时间内获得栈最小元素思路一样）



**网络输出及作用**



-  **heatmaps**

```
top-left corners和bottom-right corners的两类heatmaps，每类heatmaps有c个channals(c是种类数量，没有背景类)，大小为HxW，每个channel表示的是一个类别的binary mask位置
```



-  **embeddings**

```
衡量左上corner和右下corner的距离的，从而判断某一对角点是否属于同一个物体的两个角点
```



-  **offsets**

```
主要解决downsample之后，upsample造成的精度损失，这会影响到小物体位置框的位置精度，所以offsets会缓解这种问题
```

![img](https://pic2.zhimg.com/80/v2-2ac27ad6e5d50102c180fe0e1c987829_1440w.jpg)



**cornerNet的网络结构**主要分为以下几个部分：

- backbone: hourglass Network 
- head: 二分支输出 Top-left corners 和 Bottom-right corners，每个分支包含了各自的corner pooling以及三分支输出



通过两个hourglass module后的特征图，需要各自再通过一个3x3卷积后才能获得用于预测左上点和右下点的两个分支module（prediction module)后，每个predicition module分别进行如下操作

-  corner pooling
- 三分支的输出



**prediction module具体实现**

![img](https://pic3.zhimg.com/80/v2-54d2cd1bcd3bc9b44d26df490eedc752_1440w.jpg)





------



## Anchor Based



### RetinaNet



类别不平衡（class imbalance）是目标检测模型训练的一大难点，其中最严重的是**正负样本不平衡**（1.正负样本固定比例抽样 2.OHEM ）；

SSD的策略是采用hard mining，从大量的负样本中选出loss最大的topk的负样本以保证正负样本比例为1:3



RPN本质上也是one stage检测模型，RPN训练时所采取的策略也是抽样，从一张图像中抽取固定数量N（RPN采用的是256）的样本，正负样本分开来随机抽样N/2，如果正样本不足，那就用负样本填充；

![img](https://pic3.zhimg.com/80/v2-d7b763949316b8e5f100b0dd6aa836aa_1440w.jpg)



**FPN**

RetinaNet特征为$P_3, P_4, P_5$,去掉了$P_2$（stride=4，特征很大，去掉它可以减少计算量），同时新增两个特征$P_6, P_7$，$P_{7}, \quad P_{6}$ 在 $P_{5}$ 上加一个stride=2的3x3 卷积得到, $P_{7}$ 是在 $P_{6}$ 后面加ReLU和一个stride=2的3x3卷积得到；这样RetinaNet的 backbone得到特征也是5个level, 分别为 $P_{3}, P_{4}, P_{5}, P_{6}, P_{7}$, 其stride分别为 $8,16,32,64,128$ ；



**Anchor**

RetinaNet输入特征 $P_{3}, P_{4}, P_{5}, P_{6}, P_{7}$每个Level的anchor$ 32^{2}, 64^{2}, 128^{2}, 256^{2}, 512^{2}$，每个level的特征每个位置放置3种scale的anchor$\left\{2^{0}, 2^{1 / 3}, 2^{2 / 3}\right\}$设置3种长宽比$\left\{1:1, 1:2, 2:1\right\}$,这样每个位置共有A=9个anchor，所有level中anchor size的最小值是32，最大值是813

训练过程：计算anchor与所有GT的IoU，取IoU最大值，若大于0.5，则认为此anchor为正样本，且负责预测IoU最大的那个GT；若低于0.4，则认为此anchor为负样本；若IoU值在$[0.4, 0.5]$之间，则忽略不参与训练



**Detection**

检测模块主要包括**分类分支(分类loss采用focal loss)和box回归分支(box回归loss采用smooth L1)**

```
分类分支用来预测每个位置的各个anchor(数量为A) 的类别概率（类别数为K)：分类分支包括4个3x3的卷积 (ReLU激活函数, channel是256)，最后是一个3x3的卷积 , 输出channel为KA， 最后sigmoid激活就可以得到各个anchor预测每个类别的概率, 对于 RetinaNet来说, 每个位置相当于KA个二分类问题；

由于采用的K个二分类，某个位置的某个anchor可能最后会输出几个类别不同但是box一样的detections
```

```
box回归分支用来预测每个位置各个anchor和GT之间的 offset：box回归分支与分类分支类似，只不过最后输出channel是4A, 这也表明RetinaNet的box回归是类别无关的；
```

detection模块在FPN各个 level的特征是参数共享的，这点和RPN类似，但是RetinaNet的detection模块是多分类的



**训练与测试**

```
分类loss是sum所有的focal loss，然后除以类别为正例的anchors总数


在inference阶段，对各个level的预测首先取top 1K的detections，然后用0.05的阈值过滤掉负类，此时得到的detections已经大大降低，此时再对detections的box进行解码而不是对模型预测所有detections解码可以提升推理速度。最后把level的detections结果concat在一起，通过IoU=0.5的NMS过滤重叠框就得到最终结果
```



------



### YOLO v5



------



### EfficientDet



------



### SSD



**SSD与YOLO区别：**

- SSD采用CNN来直接进行检测，而不是像YOLO那样在全连接层之后做检测
- SSD提取了不同尺度的特征图来做检测，大尺度特征图可以用来检测小物体，而小尺度特征图用来检测大物体
- SSD采用了不同尺度和长宽比的先验框



SSD的检测值也与Yolo不太一样。对于每个单元的每个先验框，其都输出一套独立的检测值，对应一个边界框，主要分为两个部分。第一部分是各个类别的置信度或者评分，值得注意的是SSD将背景也当做了一个特殊的类别，如果检测目标共有$c$个类别，SSD其实需要预测$c+1$ 个置信度值，其中第一个置信度指的是不含目标或者属于背景的评分。在预测过程中，置信度最高的那个类别就是边界框所属的类别，特别地，当第一个置信度值最高时，表示边界框中并不包含目标。第二部分就是边界框的location，包含4个值$(cx, cy, w, h)$ ，分别表示边界框的中心坐标以及宽高。但是真实预测值其实只是边界框相对于先验框的转换值.先验框位置用$d=\left(d^{c x}, d^{c y}, d^{w}, d^{h}\right)$表示，其对应的边界框用$b=\left(b^{c x}, b^{c y}, b^{w}, b^{h}\right)$，那么边界框的预测值$l$其实是$b$ 相对于$d$的转换值：

$l^{c x}=\left(b^{c x}-d^{c x}\right) / d^{w}, l^{c y}=\left(b^{c y}-d^{c y}\right) / d^{h}$
$l^{w}=\log \left(b^{w} / d^{w}\right), l^{h}=\log \left(b^{h} / d^{h}\right)$

检测值包含两个部分：类别置信度和边界框位置，各采用一次$3 * 3$卷积来进行完成。综上所述，对于一个大小$m * n$的特征图，共有$mn$个单元，每个单元设置的先验框数目记为$k$，那么每个单元共需要$(c+4)k$个预测值，所有的单元共需要$(c+4)kmn$个预测值，由于SSD采用卷积做检测，所以就需要$(c+4)k$个卷积核完成这个特征图的检测过程;



提取了6个特征图，其大小分别是 $(38,38),(19,19),(10,10),(5,5),(3,3),(1,1)$，但是不同特征图设置的先验框数目不同；

对于**先验框的尺度**，随着特征图大小降低，先验框尺度线性增加；各个特征图的先验框尺度为$30,60,111,162,213,264$ （第一层单独设置，后边尺度遵守一个线性递增规则）。

对于长宽比，一般选取$a_{r} \in\left\{1,2,3, \frac{1}{2}, \frac{1}{3}\right\}$，默认情况下, 每个特征图会有一个 $a_{r}=1$ 且尺度为 $s_{k}$ 的先验框, 除此之外, 还会设置一个尺 度为 $s_{k}^{\prime}=\sqrt{s_{k} s_{k+1}}$ 且 $a_{r}=1$ 的先验框, 这样每个特征图都设置了两个长宽比为1但大小不 同的正方形先验框。注意最后一个特征图需要参考一个虚拟 $s_{m+1}=300 \times 105 / 100=315$ 来计算 $s_{m}^{\prime}$ 。因此, 每个特征图一共有 6 个先验框 $\left\{1,2,3, \frac{1}{2}, \frac{1}{3}, 1^{\prime}\right\}$, 但是在实现时, Conv4_3, Conv10_2和Conv11_2层仅使用4个先验框, 它们不使用长宽比为 $3, \frac{1}{2}$ 的先验框。**每个单元的先验框的中心点分布在各个单元的中心**



**训练过程：**

- 先验框匹配

在训练过程中，首先要确定训练图片中的ground truth与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它

```
1.对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样，可以保证每个ground truth一定与某个先验框匹配，反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配；

（一个图片中ground truth是非常少的， 而先验框却很多，如果仅按第一个原则匹配，很多先验框会是负样本，正负样本极其不平衡，所以需要第二个原则）

2.对于剩余的未匹配先验框，若与某个ground truth的IoU大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配；

尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3
```



- 损失函数

损失函数定义为位置误差（locatization loss， loc）与置信度误差（confidence loss, conf）的加权和：
$$
L(x, c, l, g)=\frac{1}{N}\left(L_{c o n f}(x, c)+\alpha L_{l o c}(x, l, g)\right)
$$
其中 $N$ 是先验框的正样本数量，这里 $x_{i j}^{p} \in\{1,0\}$ 为一个指示参数, 当 $x_{i j}^{p}=1$ 时表示第 $i$ 个先验框与第 $j$ 个ground truth匹配, 并且ground truth的类别为 $p_{\circ} \quad c$ 为类别置信度预 测值。 $l$ 为先验框的所对应边界框的位置预测值, 而 $g$ 是ground truth的位置参数。权重系数 $\alpha$ 通过交叉验证设置为1 



对于**位置误差,** 其采用**Smooth L1 loss**, 定义如下:
$$
\begin{aligned}
L_{l o c}(x, l, g)=\sum_{i \in P o s}^{N} \sum_{m \in\{c x, c y, w, h\}} & x_{i j}^{k} \operatorname{smooth}_{L 1}\left(l_{i}^{m}-\hat{g}_{j}^{m}\right) \\
\hat{g}_{j}^{c x}=\left(g_{j}^{c x}-d_{i}^{c x}\right) / d_{i}^{w} & \hat{g}_{j}^{c y}=\left(g_{j}^{c y}-d_{i}^{c y}\right) / d_{i}^{h} \\
& \hat{g}_{j}^{w}=\log \left(\frac{g_{j}^{w}}{d_{i}^{w}}\right) \quad \hat{g}_{j}^{h}=\log \left(\frac{g_{j}^{h}}{d_{i}^{h}}\right)
\end{aligned}
$$

$$
\operatorname{smooth}_{L_{1}}(x)=\left\{\begin{array}{ll}
0.5 x^{2} & \text { if }|x|<1 \\
|x|-0.5 & \text { otherwise }
\end{array}\right.
$$
由于 $x_{i j}^{p}$ 的存在, 所以位置误差仅针对正样本进行计算。值得注意的是, 要先对ground truth的 $g$ 进行编码得到 $\hat{g}$, 因为预测值 $l$ 也是编码值, 



对于**置信度误差**, 其采用softmax loss:
$$
L_{\text {conf }}(x, c)=-\sum_{i \in \text { Pos }}^{N} x_{i j}^{p} \log \left(\hat{c}_{i}^{p}\right)-\sum_{i \in \text { Neg }} \log \left(\hat{c}_{i}^{0}\right) \quad \text { where } \quad \hat{c}_{i}^{p}=\frac{\exp \left(c_{i}^{p}\right)}{\sum_{p} \exp \left(c_{i}^{p}\right)}
$$


**预测过程：**

对于每个预测框，首先根据类别置信度确定其类别（置信度最大者）与置信度值，并过滤掉属于背景的预测框。然后根据置信度阈值（如0.5）过滤掉阈值较低的预测框。对于留下的预测框进行解码，根据先验框得到其真实的位置参数（解码后一般还需要做clip，防止预测框位置超出图片）。解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k（如400）个预测框。最后就是进行NMS算法，过滤掉那些重叠度较大的预测框。最后剩余的预测框就是检测结果了。



------



### Faster RCNN





## Transformer



### DETR



### ViT



------



## Tricks



### 数据预处理

```
Mix up / Cut up / CutMix
Label Smoothing
Flip
Random earse
物体的随机粘贴（增加数据集）
多尺度训练（很有效果）
随机crop，随机expansion ，随机水平翻转，随机缩放，亮度，色调，饱和度，对比度
```



### 难例挖掘

```
OHEM
```



### ROI

```
ROI Pooling
ROI Align
```



### Backbone

**Backbone的核心能力在于能为检测提供若干种感受野大小和中心步长的组合以满足对不同类别和尺度的目标检测**

```
ResNet / DenseNet / ResNeXt
CSPDarknet / CSPResNeXt
EfficientNet / EfficientDet
MobileNet / ShuffleNet
GhostNet
```



------



### Neck



**Neck将来自于Backbone上的多个层级的特征图进行融合加工，增强其表达能力的同时，输出加工后并具有相同宽度的特征图以供Head使用；**



1. 特征融合 2. 分之思想来进行检测

```
SPP
FPN：PyramidBox -> FPN结构在高层的语义特征进行融合效果并不好，所以构建FPN没有必要使用所有的卷积层;
BiFPN
PANet
```





------



### Head



Head里面不使用BN

Head共享一套Weights

```
RetinaNet -Head    Anchorbased 	没有quality分支
FCOS - head   Anchor-free 有quality分支（centerness）
```



------



### 漏检 & 误检

```
正样本数据增强
hard sample策略在loss上进行处理
增加包含检测背景的负样本图片数量
采集和正样本有相似度的负样本
对训练好的模型进行测试，记录误检测的背景特征并作为负样本添加进负样本库重新训练
正负样本数量多多益善，并保证一定差异性
增加正样本轻微局部特征作为负样本
```



------



### YOLO系列

- YOLO v1

https://zhuanlan.zhihu.com/p/32525231

```
物体定位不准确
召回率低
不管一个单元格预测多少个边界框，其只预测一组类别概率值
一个单元格有多个目标无法检测出来
一个单元仅预测两个边界框，而且属于一个类别

loss使用均方误差
```



- YOLO v2 / 9000

  https://zhuanlan.zhihu.com/p/35325884

  ```
  使用别的论文技巧
  采用Multi-Scale Training
  loss使用均方误差
  
  提出了一种分类和检测的联合训练策略：对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类
  
  WordTree
  ```

  

- YOLO v3：

  https://zhuanlan.zhihu.com/p/76802514

  ```
  正例产生置信度loss、检测框loss、类别loss；忽略样例不产生任何loss；负例只有置信度产生loss
  
  YOLO v3训练，不再按照ground truth中心点，严格分配指定cell，而是根据预测值寻找IOU最大的预测框作为正例，
  Yolov1中的置信度标签，就是预测框与真实框的IOU，Yolov3是1（置信度意味着该预测框是或者不是一个真实物体，是一个二分类）
  
  ```

  v2 / v3区别：

  1. v3使用了FPN
  2. 类别置信度使用Sigmoid代替v2中的Softmax，取消了类别之间的互斥
  3. 除预测框仍使用均方误差外，其他置信度、类别使用交叉熵误差（二分类）

  

- YOLO v4
- YOLO v5

------



# 数学推导



### Sigmoid & CE & Softmax溢出

------



sigmoid，CE，softmax函数在计算中，都会用到指数运算$e^{-x}$ 或$e^{x}$,如果**在 $e^{-x}$中-x是一个很小的负数，或者在 $e^{x}$中x是一个很大的正数，这时有溢出的风险**；



**Sigmoid：**
$$
\begin{array}{l}
\text { 1.如果 } x>0 \text { 则 } y=\frac{1}{1+e^{-x}} \\
\text { 2.如果 } x<0 \text { 则 } y=\frac{e^{x}}{1+e^{x}}
\end{array}
$$
**CE：**

$crossentropy=y * −log(sigmoid(x)) + (1−y) * −log(1−sigmoid(x))$

**对于$x < 0$有溢出风险时**，变换x为$log(e^x)$ 



**Softmax:**
$$
y=\frac{e^{x_{i}}{ }^{}}{\sum_{i=1}^{n} e^{x_{i}}}
$$


取所有$x_{i}$中的最大值M，分子分母同时除以$e^{M}$即可，**解决了上溢出的问题**，因为中间项都是小于等于1的值
$$
\ y=\frac{e^{x_{i}-M}}{\sum_{i=1}^{n} e^{x_{i}-M}}
$$




# 手写代码



------



### ✅P & R & Accuracy & F1Score

```python
import numpy as np


def PRF1(data_list, thd):
    data = np.array(data_list)

    pre = data[:, 1]
    pre[np.where(data[:, 1] >= thd)] = 1
    pre[np.where(data[:, 1] < thd)] = 0

    y_true = data[:, 2]
    y_pre = data[:, 1]

    TP = np.sum(np.logical_and(np.equal(y_true, 1), np.equal(y_pre, 1)))
    FP = np.sum(np.logical_and(np.equal(y_true, 0), np.equal(y_pre, 1)))
    FN = np.sum(np.logical_and(np.equal(y_true, 1), np.equal(y_pre, 0)))
    TN = np.sum(np.logical_and(np.equal(y_true, 0), np.equal(y_pre, 0)))

    Precision = TP / (TP + FP)
    Recall = TP / (TP + FN)
    Accuracy = (TP + TN) / (TP + FP + TN + FN)
    F1_score = 2 * Precision * Recall / (Precision + Recall)

    print("精确率为:", Precision)
    print("召回率为:", Recall)
    print("总体精度为:", Accuracy)
    print("F1分数为:", F1_score)


data = [[0, 0.76, 1], [1, 0.3, 0], [2, 0.5, 0], [3, 0.76, 1]]
print(PRF1(data, 0.5))
```



------



### ✅计算根号N，精确到小数点后m位

```python
def sqrtN(N, m):
    return round(math.sqrt(N), m)
```



------



### MSGD

```python

```





### ✅IoU

```python
import numpy as np


def get_max_IoU(pre_bboxs, gt_bbox):
    """
        given 1 gt bbox, >1 pred bboxes, return max iou score for the given gt bbox and pred_bboxes
        :param pred_bbox: predict bboxes coordinates, we need to find the max iou score with gt bbox for these pred bboxes
        :param gt_bbox: ground truth bbox coordinate
        :return: max iou score
    """
    if pred_bboxes.shape[0] > 0:
        # -----0---- get coordinates of inters, but with multiple predict bboxes
        xmin = np.maximum(pred_bboxes[:, 0], gt_bbox[0])
        ymin = np.maximum(pred_bboxes[:, 1], gt_bbox[1])
        xmax = np.minimum(pred_bboxes[:, 2], gt_bbox[2])
        ymax = np.minimum(pred_bboxes[:, 3], gt_bbox[3])

        w = np.maximum(xmax - xmin + 1, 0)
        h = np.maximum(ymax - ymin + 1, 0)

        # -----1----- intersection
        inters = w * h

        # -----2----- union, uni = S1 + S2 - inters
        unions = (gt_bbox[2] - gt_bbox[0] + 1) * (gt_bbox[3] - gt_bbox[1] + 1) + (
                    pred_bboxes[:, 2] - pred_bboxes[:, 0] + 1) * (pred_bboxes[:, 3] - pred_bboxes[:, 1] + 1) - inters

        # -----3----- iou, get iou scores, max score and max iou index
        overlaps = inters / unions
        overlaps_max = np.max(overlaps)
        overlaps_max_idx = np.argmax(overlaps)

        return overlaps, overlaps_max, overlaps_max_idx


if __name__ == "__main__":
    gt_bbox = np.array([70, 80, 120, 150])

    pred_bboxes = np.array([[15, 18, 47, 60],    # top-left: <50, 50>, bottom-down: <90, 100>, <x-axis, y-axis>
                            [50, 50, 90, 100],
                            [70, 80, 120, 145],
                            [130, 160, 250, 280],
                            [25.6, 66.1, 113.3, 147.8]])
    overlaps, overlaps_max, overlaps_max_idx = get_max_IoU(pred_bboxes, gt_bbox)
    print("get_max_IoU:", "overlaps:",overlaps, "\noverlaps_max:", overlaps_max, "\noverlaps_max_idx:", overlaps_max_idx)
```



------





### 梯度反向传播



https://zhuanlan.zhihu.com/p/40378224

手动模拟：https://github.com/Inmessionant/Deep-Learning-Tricks/blob/main/Back%20Propagation.pdf



------



### mAP & AUC



https://github.com/Inmessionant/Deep-Learning-Tricks/blob/main/mAP.pdf

https://www.zhihu.com/question/30643044/answer/1205433761



```
mAP: 各类别AP的平均值

AP: PR曲线下面积

PR曲线: Precision-Recall曲线

Precision: TP / (TP + FP)

Recall: TP / (TP + FN)

TP: IoU>0.5的检测框数量（同一Ground Truth只计算一次）

FP: IoU<=0.5的检测框，或者是检测到同一个GT的多余检测框的数量

FN: 没有检测到的GT的数量
```



```
mAP计算：在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值
```



**ROC曲线下面的面积越大，模型就越好；**这个曲线下面积就称为 **AUC**；    **纵坐标：TP和横坐标：FP**

**PR曲线下面的面积越大，模型就越好**；这个曲线下面积就称为 **AP**；	**纵坐标：P和横坐标：R**



**什么时候用 ROC，什么时候用 Precision-Recall ？**

```
Data Imbalanced 的情况用 Precision-Recall ；
如果 Positive，Negative 的数量基本是平衡的，那 ROC 就更常用一些；
```



**Facebook开源的Detectron开源mAP计算：**

```python
# 按照置信度降序排序
sorted_ind = np.argsort(-confidence)
BB = BB[sorted_ind, :]   # 预测框坐标
image_ids = [image_ids[x] for x in sorted_ind] # 各个预测框的对应图片id

# 便利预测框，并统计TPs和FPs
nd = len(image_ids)
tp = np.zeros(nd)
fp = np.zeros(nd)
for d in range(nd):
    R = class_recs[image_ids[d]]
    bb = BB[d, :].astype(float)
    ovmax = -np.inf
    BBGT = R['bbox'].astype(float)  # ground truth

    if BBGT.size > 0:
        # 计算IoU
        # intersection
        ixmin = np.maximum(BBGT[:, 0], bb[0])
        iymin = np.maximum(BBGT[:, 1], bb[1])
        ixmax = np.minimum(BBGT[:, 2], bb[2])
        iymax = np.minimum(BBGT[:, 3], bb[3])
        iw = np.maximum(ixmax - ixmin + 1., 0.)
        ih = np.maximum(iymax - iymin + 1., 0.)
        inters = iw * ih

        # union
        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +
               (BBGT[:, 2] - BBGT[:, 0] + 1.) *
               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)

        overlaps = inters / uni
        ovmax = np.max(overlaps)
        jmax = np.argmax(overlaps)
    # 取最大的IoU
    if ovmax > ovthresh:  # 是否大于阈值
        if not R['difficult'][jmax]:  # 非difficult物体
            if not R['det'][jmax]:    # 未被检测
                tp[d] = 1.
                R['det'][jmax] = 1    # 标记已被检测
            else:
                fp[d] = 1.
    else:
        fp[d] = 1.

# 计算precision recall
fp = np.cumsum(fp)
tp = np.cumsum(tp)
rec = tp / float(npos)
# avoid divide by zero in case the first detection matches a difficult
# ground truth
prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
```

这里最终得到一系列的precision和recall值，并且这些值是按照置信度降低排列统计的，可以认为是取不同的置信度阈值（或者rank值）得到的。然后据此可以计算AP：

```python
def voc_ap(rec, prec, use_07_metric=False):
    """Compute VOC AP given precision and recall. If use_07_metric is true, uses
    the VOC 07 11-point method (default:False).
    """
    if use_07_metric:  # 使用07年方法
        # 11 个点
        ap = 0.
        for t in np.arange(0., 1.1, 0.1):
            if np.sum(rec >= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec >= t])  # 插值
            ap = ap + p / 11.
    else:  # 新方式，计算所有点
        # correct AP calculation
        # first append sentinel values at the end
        mrec = np.concatenate(([0.], rec, [1.]))
        mpre = np.concatenate(([0.], prec, [0.]))

        # compute the precision 曲线值（也用了插值）
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

        # to calculate area under PR curve, look for points
        # where X axis (recall) changes value
        i = np.where(mrec[1:] != mrec[:-1])[0]

        # and sum (\Delta recall) * prec
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap
```

https://github.com/facebookresearch/Detectron/blob/05d04d3a024f0991339de45872d02f2f50669b3d/lib/datasets/voc_eval.py



**mAP的指标在实际项目中是否科学？**

```
实际业务通常不会选用mAP来衡量一个detector的性能，一般用FPPI（每张图片错几个），或者相同Recall下标胶Precision

1.应用场景中一般0.5的IoU足够了，并不需要过度严格的指标
2.AP会被一些涨recall的方法推上去，比如用soft-nms，focal loss等方法测试或训出来的模型Recall会很高，mAP相应的通常会涨一点，但是都是涨的低Precision的区域，低精度区对应用场景来说没用，一般用的时候都是卡高Precision，涨回来的Recall其实并没有什么用；
3.应用的时候会卡单一的阈值，比如0.5，mAP对阈值做了平均，这时候就更不能用了；
```



------



### ✅NMS



**算法输入：**算法对一幅图产生的所有的候选框，每个框有坐标与对应的置信度；

**算法输出：**输入的一个子集，同样是一组5维数组，表示筛选后的边界框；

**算法流程：**

1. 将所有的框按类别划分，并剔除背景类，因为无需NMS；
2. 对每个物体类中的边界框(B_BOX)，按照分类置信度降序排列；
3. 在某一类中，选择置信度最高的边界框B_BOX1，将B_BOX1从输入列表中去除，并加入输出列表；
4. 逐个计算B_BOX1与其余B_BOX2的交并比IoU，若IoU(B_BOX1,B_BOX2) > 阈值TH，则在输入去除B_BOX2；
5. 重复步骤3~4，直到输入列表为空，完成一个物体类的遍历；
6. 重复2~5，直到所有物体类的NMS处理完成；
7. 输出列表，算法结束；

```python
import numpy as np


def NMS(dets, thresh):
    # dets某个类的框，x1、y1、x2、y2、以及置信度score
    # eg:dets为[[x1,y1,x2,y2,score],[x1,y1,y2,score]……]]
    # thresh是IoU的阈值
    x1, y1 = dets[:, 0], dets[:, 1]
    x2, y2 = dets[:, 2], dets[:, 3]
    scores = dets[:, 4]
    # 每一个检测框的面积
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    # 按照score置信度降序排序
    order = np.argsort(scores)[::-1]
    keep = []  # 保留的结果框集合
    while order:
        i = order[0]  # i表示box得分最高的索引
        keep.append(i)  # 保留该类剩余box中得分最高的一个
        # 得到相交区域,左上及右下
        xmin = np.maximum(x1[i], x1[order[1:]])
        ymin = np.maximum(y1[i], y1[order[1:]])
        xmax = np.minimum(x2[i], x2[order[1:]])
        ymax = np.minimum(y2[i], y2[order[1:]])
        # 计算相交的面积,不重叠时面积为0
        w = np.maximum(0, xmax - xmin + 1)
        h = np.maximum(0, ymax - ymin + 1)
        inter = w * h
        # 计算IoU：重叠面积 /（面积1+面积2-重叠面积）
        ovr = inter / (areas[i] + areas[order[1:]] - inter)
        # 保留IoU小于阈值的box
        inds = np.where(ovr <= thresh)[0]  # 返回坐标
        order[:] = order[inds + 1]  # 因为ovr数组的长度比order数组少一个（0是选取的基准）,所以这里要将所有下标后移一位
    return keep
```



NMS算法中的最大问题就是它将相邻检测框的分数均强制归零(即将重叠部分大于重叠阈值Nt的检测框移除)。在这种情况下，如果一个真实物体在重叠区域出现，则将导致对该物体的检测失败并降低了算法的平均检测率;

经典的NMS算法将IOU大于阈值的窗口的得分全部置为0，可表述如下：

$s_{i}=\left\{\begin{array}{ll}s_{i}, & \operatorname{iou}\left(\mathcal{M}, b_{i}\right)<N_{t} \\ 0, & \operatorname{iou}\left(\mathcal{M}, b_{i}\right) \geq N_{t}\end{array}\right.$



------

**Soft NMS**



- soft-NMS在训练中采用传统的NMS方法，仅在推断代码中实现soft-NMS
- Soft-NMS可以很方便地引入到object detection算法中，不需要重新训练原有的模型、代码容易实现，不增加计算量（计算量相比整个object detection算法可忽略）



Soft-NMS吸取了NMS的教训，在算法执行过程中不是简单的对IoU大于阈值的检测框删除，而是降低得分。算法流程同NMS相同，但是对原置信度得分使用函数运算，目标是降低置信度得分;



置信度重置函数有两种形式改进，一种是线性加权的：

$s_{i}=\left\{\begin{array}{ll}s_{i}, & \operatorname{iou}\left(\mathcal{M}, b_{i}\right)<N_{t} \\ s_{i}\left(1-\operatorname{iou}\left(\mathcal{M}, b_{i}\right)\right), & \operatorname{iou}\left(\mathcal{M}, b_{1}\right) \geq N_{t}\end{array}\right.$



一种是高斯加权形式：

$s_{i}=s_{i} e^{-\frac{\operatorname{iou}\left(\mathcal{M}, b_{i}\right)^{2}}{\sigma}}, \forall b_{i} \notin \mathcal{D}$



------



### ✅Focal loss

```python
def sigmoid_focal_loss(inputs: torch.Tensor, targets: torch.Tensor, gamma: float = 2) -> torch.Tensor:

    p = torch.sigmoid(inputs)
    ce_loss = nn.BCEWithLogitsLoss(inputs, targets)
    p_t = p * targets + (1 - p) * (1 - targets)
    loss = ce_loss * ((1 - p_t) ** gamma)

    return loss
```

