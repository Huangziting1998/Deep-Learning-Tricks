 

# Pytorch & Python



```
from torch.utils import data
from torch import nn   # loss， optim， layer
```



------



### Pytorch默认梯度累积



- 当显存小，batchsize不够时，可以使用梯度累积变相增大batchsize；
- weight在不同模型之间交互时候有好处；（动手学习深度学习v2）

```py
accumulation_steps = batch_size // opt.batch_size

loss = loss / accumulation_steps
running_loss += loss.item()
loss.backward()

if ((i + 1) % accumulation_steps) == 0:
	optimizer.step()
	scheduler.step()
	optimizer.zero_grad()
```



------



### nn.Module & nn.Functional



**nn.Module**实现的layer是由class Layer(nn.Module)定义的特殊类，**会自动提取可学习参数nn.Parameter**

**nn.functional**中的函数更像是**纯函数**，由def function(input)定义，一般只定义一个操作，因为其无法保存参数



**Function**需要定义三个方法：**init, forward, backward**（需要自己写求导公式） 

**Module**只需定义 __init__和**forward**，而backward的计算由自动求导机制



**对于激活函数和池化层，由于没有可学习参数，一般使用nn.functional完成，其他的有学习参数的部分则使用nn.Module**

但是**Droupout**由于在训练和测试时操作不同，所以**建议使用nn.Module实现**，它能够通过**model.eval**加以区分



- [x] https://blog.csdn.net/wzy_zju/article/details/81262472?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242


- [x] https://blog.csdn.net/andyjkt/article/details/107428618




------



### nn.Sequential & nn.ModuleList



**区别：**

- **nn.Sequential内部实现了forward函数**，因此可以不用写forward函数（或者继承nn.Module类的话，就要写出forward函数）；而nn.ModuleList则没有实现内部forward函数； -> 如果完全直接用 nn.Sequential，确实是可以的，但这么做的代价就是失去了部分灵活性，不能自己去定制 forward 函数里面的内容了;
- **nn.Sequential可以使用OrderedDict对每层进行命名**;
- **nn.Sequential里面的模块按照顺序进行排列的**，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而**nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言**。



**nn.Sequential**

- nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的；
- nn.Sequential中可以使用OrderedDict来指定每个module的名字，而不是采用默认的命名方式；



```python
import torch
import torch.nn as nn
import torch.nn.functional as F
class net_seq(nn.Module):
    def __init__(self):
        super(net2, self).__init__()
        self.seq = nn.Sequential(
                        nn.Conv2d(1,20,5),
                         nn.ReLU(),
                          nn.Conv2d(20,64,5),
                       nn.ReLU()
                       )      
    def forward(self, x):
        return self.seq(x)
net_seq = net_seq()
```

```python
from collections import OrderedDict

class net_seq(nn.Module):
    def __init__(self):
        super(net_seq, self).__init__()
        self.seq = nn.Sequential(OrderedDict([
                        ('conv1', nn.Conv2d(1,20,5)),
                         ('relu1', nn.ReLU()),
                          ('conv2', nn.Conv2d(20,64,5)),
                       ('relu2', nn.ReLU())
                       ]))
    def forward(self, x):
        return self.seq(x)
net_seq = net_seq()
```

**nn.ModuleList**

- nn.ModuleList，它是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。你可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中。使用 Python 的 list 添加的卷积层和它们的 parameters 并没有自动注册到我们的网络中；
- 

```python
class net_modlist(nn.Module):
    def __init__(self):
        super(net_modlist, self).__init__()
        self.modlist = nn.ModuleList([
                       nn.Conv2d(1, 20, 5),
                       nn.ReLU(),
                        nn.Conv2d(20, 64, 5),
                        nn.ReLU()
                        ])

    def forward(self, x):
        for m in self.modlist:
            x = m(x)
        return x

net_modlist = net_modlist()
```



------



### DataLoader & Sampler & DataSet 



假设我们的数据是一组图像，每一张图像对应一个index，那么如果我们要读取数据就只需要对应的index即可，即上面代码中的`indices`，而选取index的方式有多种，有按顺序的，也有乱序的，所以这个工作需要`Sampler`完成，`DataLoader`和`Sampler`在这里产生关系

我们已经拿到了indices，那么下一步我们只需要根据index对数据进行读取即可了，这时`Dataset`和`DataLoader`产生关系

```
-------------------------------------
| DataLoader						|				
|									|							
|			Sampler -----> Indices	|  													
|                       |			|	
|      DataSet -----> Data			|
|						|			|			
------------------------|------------                    
						|s						
                        Training
```



DataLoader 的源代码初始化参数里有两种sampler：`sampler`和`batch_sampler`，都默认为`None`；前者的作用是生成一系列的index，而batch_sampler则是将sampler生成的indices打包分组，得到一个又一个batch的index



Pytorch中已经实现的`Sampler`有如下几种：`SequentialSampler` 	`RandomSampler`	 `WeightedSampler` 	`SubsetRandomSampler`

- 如果你自定义了`batch_sampler`,那么这些参数都必须使用默认值：`batch_size`, `shuffle`,`sampler`,`drop_last`.
- 如果你自定义了`sampler`，那么`shuffle`需要设置为`False`
- 如果`sampler`和`batch_sampler`都为`None`,那么`batch_sampler`使用Pytorch已经实现好的`BatchSampler`,而`sampler`分两种情况：
  - 若`shuffle=True`,则`sampler=RandomSampler(dataset)`
  - 若`shuffle=False`,则`sampler=SequentialSampler(dataset)`



Dataset定义方式如下：

```python
class Dataset(object):
	def __init__(self):
		...
		
	def __getitem__(self, index): # 能让该类可以像list一样通过索引值对数据进行访问
		return ...
	
	def __len__(self):
		return ...
```



- [x] https://www.cnblogs.com/marsggbo/p/11308889.html




------



### BN & Dropout



#### **Dropout在训练和测试时候的区别**



`torch.nn.Dropout2d(p=0.5, inplace=False)`：input shape: (N, C, H, W)， output shape: (N, C, H, W)



**Dropout在层与层之间加噪声，是一种正则**；**在全连接使用，CNN用BN**

Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0来控制模型复杂度，以提高模型的泛化能力，减少过拟合；



**Dropout 在训练时采用**，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险；而在测试时，应该用整个训练好的模型，因此**测试时不需要dropout**；

=> 在测试时如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，用户可能认为模型预测不准。那么**一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的**。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望；

```python
mask = (torch.Tensor(X.shape).uniform_(0, 1) > dropout).float()
return X * Mask / (1 - p)
```



------



#### PyTorch中BN



在PyTorch中**将gamma和beta改叫weight、bias**，使得打印网络参数时候只会打印出weight和bias（PyTorch中只有可学习的参数才称为Parameter）,但是`Net.state_dict()`是有running_mean和running_var的**因为running_mean和running_var不是可以学习的变量，只是训练过程对很多batch的数据统计;**



BN层的**输出Y与输入X之间的关系**：**Y = (X - running_mean) / sqrt(running_var + eps) * gamma + beta**，其中**gamma、beta为可学习参数（在PyTorch中分别改叫weight和bias），训练时通过反向传播更新**；而**running_mean、running_var则是在前向时先由X计算出mean和var，再由mean和var以动量momentum来更新running_mean和running_var**，所以**在训练阶段，running_mean和running_var在每次前向时更新一次**；在**测试阶段，则通过`net.eval()`固定该BN层的running_mean和running_var，此时这两个值即为训练阶段最后一次前向时确定的值，并在整个测试阶段保持不变；**



```
训练时：
running_mean = (1 - momentum) * running_mean + momentum * mean_cur
running_var = (1 - momentum) * running_var + momentum * var_cur
```

```
测试时：
running_mean = running_mean
running_var = running_var
```

**先更新running_mean和running_var，再计算BN；**



------



#### BN理论



**B x H x W，不涉及Channel，数据归一化方法；**

**BN的精髓在于归一之后，使用$\gamma, \beta$作为还原参数，在一定程度上保留原数据的分布；**



**提出原因**

**解决Internal Covariate Shift**：训练数据在经过网络的每一层后其分布都发生了变化；

**缓解过拟合**：



```
批规范化（Batch Normalization，BN）：在 minibatch维度 上在每次训练iteration时对隐藏层进行归一化
标准化（Standardization）：对输入 数据 进行归一化处理
正则化（Regularization）：通常是指对 参数 在量级和尺度上做约束，缓和过拟合情况，L1 L2正则化
```



**作用**

- 将数据规整到统一区间，减少数据的发散程度，可以加快模型训练时的收敛速度，使得模型训练过程更加稳定；
- 避免梯度爆炸或者梯度消失；
- 起到一定的正则化作用，防止过拟合；



 $Input: B=\left\{x_{1 \ldots m}\right\} ; \gamma, \beta($ parameters to be learned $)$

$\text { Output }:\left\{y_{i}=B N_{\gamma, \beta}\left(x_{i}\right)\right\} \\$
$$
\begin{array}{r}

\mu_{B} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\
\sigma_{B}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2} \\
\tilde{x}_{i} \leftarrow \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}} \\
y_{i} \leftarrow \gamma \tilde{x}_{i}+\beta
\end{array}
$$

均值的计算，就是在一个批次内，将每个通道中的数字单独加起来，再除以 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+H+%5Ctimes+W) ；举个例子：该批次内有10张图片，每张图片有三个通道RBG，每张图片的高、宽是H、W，那么均值就是计算**10张图片R通道的像素数值总和**除以**![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)** ，再计算B通道全部像素值总和除以![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)，最后计算G通道的像素值总和除以![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)。方差的计算类似；

可训练参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%E3%80%81%5Cbeta) 的维度等于**张量的通道数**，在上述例子中，RBG三个通道分别需要一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) ，所以 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec%7B%5Cgamma%7D%E3%80%81%5Cvec%7B%5Cbeta%7D) 的维度等于3；



------



#### **卷积层和BN层的融合**



**BN层最酷的地方是它可以用一个1x1卷积等效替换，更进一步地，我们可以将BN层合并到前面的卷积层中；**
$$
\begin{aligned}
y_{\text {conv }} &=w \cdot x+b \\
y_{b n} &=\gamma \cdot\left(\frac{y_{\text {conv }}-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
&=\gamma \cdot\left(\frac{w x+b-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
\hat{w} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot w \\
\hat{b} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot(b-E[x])+\beta \\
y_{b n} &=\hat{w} \cdot x+\hat{b}
\end{aligned}
$$



------



#### **BN和Dropdout同时使用出现的问题及解决方法**



**方差偏移现象**

Dropout 与 BN 之间冲突的关键是**网络状态切换过程中存在神经方差的不一致行为**。试想若有神经响应 X，当网络从训练转为测试时，Dropout 可以通过其随机失活保留率（即 p）来缩放响应，并在学习中改变神经元的方差，而 BN 仍然维持 X 的统计滑动方差。这种方差不匹配可能导致数值不稳定。而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能。事实上，如果没有 Dropout，那么实际前馈中的神经元方差将与 BN 所累计的滑动方差非常接近，这也保证了其较高的测试准确率。



**解决方案**

1.在所有 BN 层后使用 Dropout；

2.修改 Dropout 的公式让它对方差并不那么敏感，就是高斯Dropout、均匀分布Dropout；



------



### model.eval & torch.no_grad



**两者都在Inference时候使用，但是作用不相同：**

```
model.eval() 负责改变batchnorm、dropout的工作方式，如在eval()模式下，dropout是不工作的；
torch.no_grad() 负责关掉梯度计算，节省eval的时间；
```



**只进行Inference时，`model.eval()`是必须使用的，否则会影响结果准确性； 而`torch.no_grad()`并不是强制的，只影响运行效率；**



------



### 迭代器 & 生成器



**迭代器：**实现了__iter__和__next__方法的对象都称为迭代器。迭代器是一个有状态的对象，在调用next() 的时候返回下一个值，如果容器中没有更多元素了，则抛出StopIteration异常；



**生成器：**其实是一种特殊的迭代器，但是不需要像迭代器一样实现__iter__和__next__方法，**自动实现迭代器协议**；Python有两种不同的方式提供生成器：

​	1.**生成器函数：**常规函数定义，但是，使用yield语句而不是return语句返回结果。yield语句一次返回一个结果，在每个结果中间，挂起函数的状态，以便下次重它离开的地方继续执行

​	2.**生成器表达式：**类似于列表推导，但是，生成器返回按需产生结果的一个对象，而不是一次构建一个结果列表



**生成器的好处： **

​	1.**延迟计算：**一次返回一个结果，它不会一次生成所有的结果，这对于大数据量处理，将会非常有用；

​	2.**提高代码可读性**；



**生成器注意事项：**

​	**生成器只能遍历一次**



------



### 类实例方法 & 类方法 & 类静态方法



**采用 @classmethod 修饰的方法为类方法；**

**采用 @staticmethod 修饰的方法为类静态方法；**

**不用任何修改的方法为实例方法；**

其中 @classmethod 和 @staticmethod 都是函数装饰器



**Python类实例方法:**

在类中定义的方法默认都是实例方法，类的构造方法理论上也属于实例方法；

```python
class CLanguage:
    #类构造方法，也属于实例方法
    def __init__(self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
    # 下面定义了一个say实例方法
    def say(self):
        print("正在调用 say() 实例方法")
```

实例方法最大的特点就是，它最少也要包含一个 self 参数，用于绑定调用此方法的实例对象。实例方法通常会用类对象直接调用，例如：

```python
clang = CLanguage()
clang.say()
```



**Python类方法：**

Python 类方法和实例方法相似，它最少也要包含一个参数，只不过类方法中通常将其命名为 cls，Python 会自动将类本身绑定给 cls 参数（注意，绑定的不是类对象）。也就是说，我们在调用类方法时，无需显式为 cls 参数传参；

和实例方法最大的不同在于，类方法需要使用`＠classmethod`修饰符进行修饰（如果没有` ＠classmethod`，则 Python 解释器会将 info() 方法认定为实例方法，而不是类方法），例如：

```python
class CLanguage:
    #类构造方法，也属于实例方法
    def __init__(self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
    #下面定义了一个类方法
    @classmethod
    def info(cls):
        print("正在调用类方法",cls)
```

类方法推荐使用类名直接调用，当然也可以使用实例对象来调用：

```python
#使用类名直接调用类方法
CLanguage.info()
#使用类对象调用类方法
clang = CLanguage()
clang.info()
```



**Python类静态方法：**

静态方法，其实就是我们学过的函数，和函数唯一的区别是，静态方法定义在类命名空间中，而函数则定义在程序所在的全局命名空间中；

静态方法没有类似`self`、`cls` 这样的特殊参数，因此 Python 解释器不会对它包含的参数做任何类或对象的绑定。也正因为如此，类的静态方法中无法调用任何类属性和类方法；

静态方法需要使用`＠staticmethod`修饰，例如：

```python
class CLanguage:
    @staticmethod
    def info(name,add):
        print(name,add)
```

静态方法的调用，既可以使用类名，也可以使用类对象，例如：

```python
#使用类名直接调用静态方法
CLanguage.info("C语言中文网","http://c.biancheng.net")
#使用类对象调用静态方法
clang = CLanguage()
clang.info("Python教程","http://c.biancheng.net/python")
```



# 深度学习



### 梯度消失 & 梯度爆炸



目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要**链式法则（Chain Rule）**的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。

而链式法则是一个**连乘的形式**，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度**反向传播**的方式对深度网络权值进行更新时，得到的**梯度值接近0**或**特别大**，也就是**梯度消失**或**爆炸**。梯度消失或梯度爆炸在本质原理上其实是一样的。



**产生原因：**

```
梯度消失：1.深层网络 2.损失函数（Sigmoid）

梯度爆炸：1.深层网络 2.Weights初始化值太大
```



**解决方法：**

```
梯度归一化 / 梯度剪切 -> 让梯度值在合理范围内[1e-6, 1e3]

ReLU

ShortCut / LSTM门机制

Batch Normalization

合理的参数初始化（He,Xavier）-> 让每层均值和方差保持一致
权重正则化（权重衰减）
```



------



### **Kaiming初始化**



- **前向传播**的时候, 每一层的**卷积计算的方差为1**；
- **反向传播**的时候, 每一层的继续往**前传的梯度方差为1**(因为每层会有两个梯度的计算, 一个用来更新当前层的权重, 一个继续传播,用于前面层的梯度的计算)；



------



### NAN & INF



**INF：**数值太大、权重初始值太大 、Learning rate太大  

**NAN：**除数为0产生 

 

**INF解决方案：**

- 激活函数
- 权重初始均值为0，方差小
- learning不断减小



------



### L1 L2正则化



**正则化**之所以能够**降低过拟合**的原因在于，**正则化是结构风险最小化的一种策略实现**



- **权重衰减通过控制L2正则**项使得模型参数不会过大，从而控制模型复杂度；
- **正则项权重**是控制模型复杂的**超参数**；
- **正则化其他方法：1. EMA of Weights  2. Label Smoothing   3. RandAugment  4. Dropout on FC **
- 给loss function加上正则化项，能使得新得到的优化目标函数**h = f(w, b)+normal(w)**，需要在f和normal中做一个权衡，如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，normal引入使得最优解向原点移动，因此可以看出**加正则项能实现参数的稀疏，让解更加简单，通过降低模型复杂度防止过拟合，提升模型的泛化能力**；

 

|          |                                               |       特点1        |         特点2          |                                         |         作用         |                                                              |
| :------: | :-------------------------------------------: | :----------------: | :--------------------: | :-------------------------------------: | :------------------: | :----------------------------------------------------------: |
| L1正则化 |   在loss function后边所加**正则项为L1范数**   | 容易得到**稀疏解** | 容易产生**稀疏的权重** | 趋向于产生少量的特征，而其他的特征都是0 | 特征选择、防止过拟合 | 对异常值更鲁棒；在0点不可导，计算不方便；没有唯一解；输出稀疏，会将不重要的特征直接置0； |
| L2正则化 | loss function后边所加**正则项为L2范数的平方** | 容易得到**平滑解** | 容易产生**分散的权重** |  会选择更多的特征，这些特征都会接近于0  |      防止过拟合      |              计算方便；对异常值敏感；有唯一解；              |



L0 范数：向量中**非0元素的个数**；

L1 范数：向量中**各个元素绝对值的和；**

L2 范数：向量中**各元素平方和**再**求平方根；**



------



### 分类Loss采用CE而不是MSE



$z(x)=w * x+b, \quad a(z)=\sigma(z)=\frac{1}{1+e^{-z}}$

我们要学习的函数 $a(x)=\sigma(w * x+b)$ ，目标为使`a(x)`与`label y`越逼近越好



 MSE  Loss
$$
L_{m s e}=\frac{1}{2}(a-y)^{2}
$$
CE Loss
$$
L_{c e e}=-(y * \ln (a)+(1-y) * \ln (1-a))
$$



**两个Loss function对w的导数，也就是SGD梯度下降时，w的梯度**

**MSE：**

$\frac{\partial L_{m s e}}{\partial w}=\frac{\partial L}{\partial a} * \frac{\partial a}{\partial z} * \frac{\partial z}{\partial w}=(a-y) * \sigma^{\prime}(z) * x$

**CE：**

$\frac{\partial L_{c e e}}{\partial w}=\left(-\frac{y}{a}+\frac{1-y}{1-a}\right) * \sigma^{\prime}(z) * x$
由于 $\sigma^{\prime}(z)=\sigma(z) *(1-\sigma(z))=a *(1-a)$, 则:
$\frac{\partial L_{c e e}}{\partial w}=(a y-y+a-a y) * x=(a-y) * x$

sigmoid函数 $\sigma(z)$ 的导数sigmoid $\sigma^{\prime}(z)$ 在输出接近 0 和 1 的时候是非常小的，故导致在使用MSE Loss时，模型参数w会学习的非常慢；而使用CE Loss则没有这 个问题，为了更快的学习速度，分类问题一般采用交叉商损失函数；



------



### BN & LN & IN



对于`[B,C,W,H]`这样的训练数据而言

**BN** 是在`[B,W,H]`维度求均值方差进行规范化 -> CNN

**LN** 是对`[C,W,H]`维度求均值方差进行规范化   -> RNN，Transformer

**IN** 是对`[W,H]`维度求均值方差进行规范化  -> 图像风格化

**GN **先对通道进行分组，每个组内的所有[$C_i$,W,H]维度求均值方差进行规范化，与BatchSize无关



------



### NiN & ResNet & DenseNet



**NiN:** Conv + 1x1 Conv + 1x1 Conv 



**一.ResNet和DenseNet比较**



**ResNet：稀疏连接，  Add， 训练速度快， 参数量相对较多**

**DenseNet：密集连接，Concat， 训练速度慢（concat需要频繁读取内存）， 参数量相对较少**

- **DenseNet比传统的卷积网络所需要的参数更少：密集连接带来了特征重用**，不需要重新学习冗余的特征图，而且维度拼接的操作，带来了丰富的特征信息，利用更少的卷积就能获得很多的特征图;
- **DenseNet提升了整个网络中信息和梯度的流动，对训练十分有利：**密集连接使得每一层都可以直接从损失函数和原始输入信号获得梯度，对于训练更深的网络十分有利；
- **密集连接的网络结构有正则化的效果，能够减少过拟合风险**；
- **对显存需求**



**二. ResNet解决了什么问题**



- **网络性能退化能力：**单纯的堆积网络正确率不升反降：按理说，当我们堆叠一个模型时，理所当然的会认为效果会越堆越好。因为，假设一个比较浅的网络已经可以达到不错的效果，那么即使之后堆上去的网络什么也不做，模型的效果也不会变差。然而事实上，这却是问题所在。“什么都不做”恰好是当前神经网络最难做到的东西之一； ->  恒等映射

- **有效减少梯度相关性的衰减**：即使BN过后梯度的模稳定在了正常范围内，但梯度的相关性实际上是随着层数增加持续衰减的；
    对于L层的网络来说，没有残差表示的Plain Net梯度相关性的衰减在1 / 2^L ，而ResNet的衰减却只有 1 / sqrt(L)

- **稳定梯度**：在输出引入一个输入x的恒等映射，则梯度也会对应地引入一个常数1，这样的网络的确不容易出现梯度值异常，在某种意义上，起到了**稳定梯度**的作用；

- **shortcut相加可以实现不同分辨率特征的组合：**因为浅层容易有高分辨率但是低级语义的特征，而深层的特征有高级语义，但分辨率就很低了；

- **shortcut实际上让模型自身有了更加“灵活”的结构：**即在训练过程本身，模型可以选择在每一个部分是“更多进行卷积与非线性变换”还是“更多倾向于什么都不做”，抑或是将两者结合；



**三. ResNet两种结构具体怎么实现，BottleNeck的作用**



- **两个3x3卷积和一个shortcut**  
- **两个1x1卷积中间加一个3x3卷积，然后再加一个shortcut**



​	**BottleNeck作用：降低维度，模型压缩，减少计算量**

​	卷积核的尺寸是Dk×Dk×M，一共有N个，每一个都要进行Dw×Dh次运算，所以标准卷积的计算量是：Dk x Dk x M x N x Dw x Dh

​	FLOPs：10^9 级别



**四. DenseNet和ResNet哪个比较好**

​	**在小数据集，DenseNet比ResNet要好，因为小数据集的时候容易产生过拟合，但是DenseNet能够很好的解决过拟合的问题。**DenseNet 具有非常好的抗过拟合性能，尤其适合于训练数据相对匮乏的应用。这一点从论文中 DenseNet 在不做数据增强（data augmentation）的 CIFAR 数据集上的表现就能看出来。对于 DenseNet 抗过拟合的原因有一个比较直观的解释：神经网络每一层提取的特征都相当于对输入数据的一个非线性变换，而随着深度的增加，变换的复杂度也逐渐增加（更多非线性函数的复合）。相比于一般神经网络的分类器直接依赖于网络最后一层（复杂度最高）的特征，DenseNet 可以综合利用浅层复杂度低的特征，因而更容易得到一个光滑的具有更好泛化性能的决策函数；



------



### Inception



**Inception使用split-transform-merge策略把multi-scale filter生成的不同感受野的特征融合到一起，**有利于识别不同尺度的对象；

 现在这种思想已经作为基础组件使用，和它特别像的是SPP；

- v1：1x1 3x3 5x5 不同感受野；
- v2：使用BN；
- v3：两个3x3代替一个5x5，3x3=1x3 + 3x1；
- v4：残差连接；



------



### 移动端/轻量化模型



**MobileNet：**

- v1：
  - **深度可分离卷积 + ReLU6**：3x3深度卷积， 1x1升维；


- v2：
  - 深度卷积训出来的卷积核有不少是空的 -> 在低维度ReLU使得信息丢失，所以**Inverted residuals：**1x1先升维，3x3深度卷积， 1x1降维，最后的ReLU6替换为Add；


- v3：

  - 使用NAS 
  - v2的Inverted residuals
  - SE模块
  - h-swish激活函数




**ShuffleNet：**

- v1：
  - **pointwise group convolution**（降低1x1卷积的计算量）
  - **channel shuffle**（解决不同组之间的特征图不通信）
- v2：
  - **平衡输入输出通道**(in = out) 
  - **谨慎使用组卷积** 
  - **避免网络碎片化**(一些操作可以合并) 
  - **减少元素级运算**(concat代替)



------



### 数据集划分



**验证集要和训练集来自于同一个分布（shuffle数据集然后开始划分），测试集尽可能贴近真实数据**



- 通常80%为训练集，20%为测试集
- 当**数据量较小**时（万级别）的时候将训练集、验证集以及测试集划分为**6：2：2**；若是**数据量很大**，可以将训练集、验证集、测试集比例调整为**98：1：1**
- 当数据量很小时，可以采用**K折交叉验证**
- 刚开始的时候，用训练集训练，验证集验证，确定超参数和一些细节；在验证集调到最优后，再把验证集丢进来训练，在测试集上测试
- 划分数据集时可采用随机划分法（当样本比较均衡时），分层采样法（当样本分布极度不均衡时）



------



### 卷积



**一、卷积神经网络特点**

```
1.局部连接 2.权值共享 3.层次结构
```



- **每个通道之间不共享参数，希望每个通道学到不同的模式**
- 卷积**不具有旋转不变性**：要不是中心对称的像素团，旋转之后的卷积值肯定是不一样的；
- **卷积不具有平移不变性**：同一个像素团，只要卷积核对齐了，卷积值都是一样的，但是**加了padding的卷积网络平移不变性也是不存在的**，不带padding的网络每一层都必须进行严密的设计，如不带padding的UNet，通常为了网络设计简单，对训练样本做**平移增广**是很有必要的；
- 如果边长减去1后不能被stride整除，**卷积的降采样过程会丢弃边缘的像素**，**特征图像素与输入图像位置映射会产生偏移**，目前所有的深度学习框架都没有考虑这里的映射错位关系，训练时输入样本图像的大小和检测时切块的大小只能用最终特征图的尺寸反推回去，保证在卷积过程中不丢弃边缘；



------



**二、卷积有哪些**



- **3x3卷积**

   - 底层专门做过优化，已经成为主流组件；



- **1x1卷积**
  - 升维降维；
  - 减少参数；
  - 通道融合；
  - 增加非线性（利用后接的非线性激活函数如ReLU）；



- **空洞卷积**

  - kernal之间增加空洞，增加感受野，图森组针对空洞卷组专门做过研究：[1, 3, 5, 1, 3, 5]这样的空洞率；
  - 虽然增大了感受野，但是使得特征更加稀疏；
  - **解决了网格效应**；



- **转置卷积**

  **会出现棋盘效应：**由于转置卷积的“不均匀重叠”  -> **1.采取可以被步长整除的卷积核长度  2.插值**；

  对于同一个卷积核（因非其稀疏矩阵不是正交矩阵），结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状，**上采样常用双线性插值；**

  ![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpDiaSQ8lkIWnNEAtPYtPBXKm0Txqvm0BamZB6bTvAqibFMlgeSyHwJakB8M4fia1ibh4ekhwKKkkdx0w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  ![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpDiaSQ8lkIWnNEAtPYtPBXKkJYoWEF0iaPRTFbg33KhticTOwwTHhhEAmN5ZGyAsiayGNMWxPRQtPbGg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  

- **深度可分离卷积**

  **计算成本仅仅是2D卷积的12%左右**：对于规模较小的模型，如果将普通卷积替换为深度可分卷积，其模型大小可能会显著降低，模型的能力可能会变得不太理想，因此得到的模型可能是次优的；但如果使用得当，深度可分卷积能在不牺牲模型性能的前提下显著提高效率；

  

  **MobileNet v1中提出：深度卷积 + 1x1卷积**



**参数量**：卷积核的尺寸是Dk×Dk×M，一共有N个

```
**标准卷积**的参数量是：Dk x Dk x M x N
**深度卷积**的卷积核尺寸Dk×Dk×M；逐点卷积的卷积核尺寸为1×1×M，一共有N个，所以深度可分离卷积的参数量是：Dk x Dk x M + M x N
```

**计算量：**普通卷积核的尺寸是Dk×Dk×M，一共有N个，每一个都要进行Dw×Dh次运算

```
**标准卷积**的计算量是：Dk x Dk x M x N x Dw x Dh
**深度卷积**的卷积核尺寸Dk×Dk×M，一共要做Dw×Dh次乘加运算；逐点卷积的卷积核尺寸为1×1×M，有N个，一共要做Dw×Dh次乘加运算，所以深度可分离卷积的计算量是：Dk x Dk x M x Dw x Dh +  M x N x Dw x Dh
```



**参数量和运算量均下降为原来的：**$\frac{1}{N}+\frac{1}{D_k^2}$



------



**三、卷积和互相关的关系**



**卷积**是透过两个函数$f$和$g$生成第三个函数的一种数学算子，表征函数$f$与经过翻转和平移的$g$的乘积函数所围成曲边梯形的面积；

**互相关**是两个函数之间的滑动点积或滑动内积，**互相关中的过滤不经过反转**，而是直接滑过函数$f$，$f$与$g$之间的交叉区域即是互相关；

**严格意义上来说，深度学习中的“卷积”是互相关(Cross-correlation)运算，本质上执行逐元素乘法和加法**。但在之所以习惯性上将其称为卷积，是因为过滤器的权值是在训练过程中学习得到的；



------



### 感受野



感受野大小计算公式：
$$
r_{l}=r_{l-1}+\left(k_{l}-1\right) * \prod_{i=0}^{l-1} s_{i}
$$
其中 $r_{l-1}$ 为第 $l-1$ 层的感受野大小, $k_{l}$ 为第l层的卷积核大小 $($ 也可以是Pooling $), s_{i}$ 为第 $i$ 层的卷 积步长。一般来说 $r_{0}=1, s_{0}=1$ ；



**CNN中感受野定义及性质**

```
在深度神经网络中，每个神经元节点都对应着输入图像的某个确定区域，仅该区域的图像内容能对相应神经元的激活产生影响，那么这个区域称为该神经元的感受野；

- 越靠近感受野中心的区域越重要
- 各向同性
- 由中心向周围的重要性衰减速度可以通过网络结构控制
```



感受野是直接或者间接参与计算特征图像素值的输入图像像素的范围，直接感受野就是卷积核大小，随着卷积层数的加深之前层次的感受野会叠加进去。**感受野小了缺乏环境信息，感受野大了引入太多环境干扰**，所以**一个网络能够检测的目标框范围与特征图像素或者特征向量的感受野有关**，通常能够检测的目标框边长范围是感受野边长的0.1-0.5倍；拿到了一个网络**要做感受野分析，然后确定它能够检测多少像素的目标**。实际目标检测任务需要综合网络结构设计和图像分辨率选择。如果目标框的像素范围超过了网络的感受野，就需要将原始图像缩小后再检测；



------



### 池化



**作用：**

```
1. 抑制噪声，降低信息冗余
2. 提升模型的尺度不变性、旋转不变性
3. 降低模型计算量
4. 防止过拟合
```

**最大池化作用：保留主要特征，突出前景**

**平均池化作用：保留背景信息，突出背景**

------



**Mean Pooling：**

 在forward的时候，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值；**backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了；**（假设pooling的窗大小是2x2）

```
forward: [1 3; 2 2] -> [2]
backward: [2] -> [0.5 0.5; 0.5 0.5]
```



平均池化取每个块的平均值，提取特征图中所有特征的信息进入下一层。因此**当特征中所有信息都比较有用时，使用平均池化。如网络最后几层，最常见的是进入分类部分的全连接层前，常常都使用平均池化。这是因为最后几层都包含了比较丰富的语义信息，使用最大池化会丢失很多重要信息；**



**Max Pooling：**

在forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，**backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0；**

```
forward: [1 3; 2 2] -> [3]
backward: [3] -> [0 3; 0 0]
```



最大池化的操作，取每个块中的最大值，而其他元素将不会进入下一层。CNN卷积核可以理解为在提取特征，对于最大池化取最大值，可以理解为提取特征图中响应最强烈的部分进入下一层，而其他特征进入待定状态；

一般而言，前景的亮度会高于背景，因此，最大池化具有提取主要特征、突出前景的作用。但在个别场合，前景暗于背景时，最大池化就不具备突出前景的作用了；

**当特征中只有部分信息比较有用时，使用最大池化。如网络前面的层，图像存在噪声和很多无用的背景信息，常使用最大池化；**



- [x] https://mp.weixin.qq.com/s/2mEhUuHOeT4Y4NZZ3NlktQ



------



### 过采样 & 欠采样



原始数据大小为![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B1831%5Ctimes21%7D)，1831条数据，每条数据有21个特征：其中正例176个（9.6122%），反例1655个（90.3878%），类别不平衡;



**欠采样：**从反例中随机选择176个数据，与正例合并（ ![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B352%5Ctimes21%7D) ）

**过采样：**从正例中反复抽取并生成1655个数据（势必会重复），并与反例合并（ ![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B3310%5Ctimes21%7D) ）



- 采样方法一般比直接调整阈值的效果要好；
- 使用采样方法（过采样和欠采样）一般可以提升模型的泛化能力，但有一定的过拟合的风险，应搭配使用正则化模型；
- 过采样的结果较为稳定，过采样大部分时候比欠采样的效果好；




**过采样**带来**更大的运算开销**，当数据中噪音过大时，结果反而可能会更差因为**噪音也被重复使用**；

尝试**半监督学习**的方法；注意积累样本；数据增强；欠采样的时候可以训练多个模型，最后尝试模型投票的方法；



- [x]  https://www.zhihu.com/question/269698662/answer/352279936



------



### 过拟合 & 欠拟合



**过拟合**指的是在训练集error越来越低，但是在验证集和测试集error越来越高，模型拟合了训练样本中的噪声，导致泛化能力差；

```
数据增强
缩减模型表达能力
正则化（Weight Decay， L1，L2）
Early Stopping
Dropout / BN
```

**欠拟合**指的是训练集提取特征较少，导致模型不能很好拟合训练集；

```
增加模型复杂度  eg.ResNet-50 -> resNet-101；
减少正则化
错误分析：（训练集和测试集的分布偏差）测试时候出现问题进行分析，训练集缺少哪些情况导致错误，后续将在训练集中加入此类数据纠正偏差；
加入更多特征
```



### 优化学习方法

------



**一阶方法：**随机梯度下降（SGD）、动量（Momentum）、牛顿动量法（Nesterov动量）、AdaGrad（自适应梯度）、RMSProp（均方差传播）、Adam、Nadam

**二阶方法：**牛顿法、拟牛顿法、共轭梯度法（CG）、BFGS、L-BFGS

**自适应优化算法：**Adagrad（累积梯度平方）、RMSProp（累积梯度平方的滑动平均）、Adam（带动量的RMSProp，即同时使用梯度的一、二阶矩）

**梯度下降陷入局部最优有什么解决办法？** 可以用BGD、SGD、MBGD、momentum，RMSprop，Adam等方法来避免陷入局部最优；



- 二阶导是向量，学术上比较好，但难算，一阶实用；
-  只关注收敛在哪个地方，SGD一步一步来，二阶收敛效果不一定比一阶收敛效果好；



------

**马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）**

- **基于动量**的方法使得最优化过程看起来像是一个球滚下山的样子
- **SGD**很难突破对称性，一直卡在顶部
- **RMSProp之类**的方法能够看到马鞍方向有很低的梯度（因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进）

------

**梯度下降与拟牛顿法的异同**

- **参数更新模式相同**
- **梯度下降法利用误差的梯度**来更新参数，**拟牛顿法利用海塞矩阵**的近似来更新参数
- **梯度下降**是泰**勒级数的一阶展开**，而**拟牛顿法是泰勒级数的二阶展开**
- **SGD能保证收敛**，但是L-BFGS在非凸时不收敛

------



**一个框架来梳理所有的优化算法**



首先定义：待优化参数： $w$, 目标函数： $f(w)$, 初始学习率 $\alpha_{\circ}$
而后，开始进行迭代优化，在每个epoch $\boldsymbol{t}$ :

1. 计算目标函数关于当前参数的梯度： $g_{t}=\nabla f\left(w_{t}\right)$
2. 根据历史梯度计算一阶动量和二阶动量:
$m_{t}=\phi\left(g_{1}, g_{2}, \cdots, g_{t}\right) ; V_{t}=\psi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$
3. 计算当前时刻的下降梯度： $\eta_{t}=\alpha \cdot m_{t} / \sqrt{V_{t}}$
4. 根据下降梯度进行更新： $w_{t+1}=w_{t}-\eta_{t}$



------



**SGD（普通更新）**



最简单的沿着负梯度方向改变参数；假设有一个**参数向量x**及其**梯度dx**，那么最简单的更新的形式是：

```python
# 普通更新
x += - learning_rate * dx
```



------



**SGDM（动量更新）**



该方法从**物理角度**上对于最优化问题得到的启发：

从本质上说，动量法，就像我们从山上推下一个球，球在滚下来的过程中累积动量，变得越来越快（直到达到终极速度，如果有空气阻力的存在，则mu<1）；同样的事情也发生在参数的更新过程中：**对于在梯度点处具有相同的方向的维度，其动量项增大，对于在梯度点处改变方向的维度，其动量项减小。**因此，我们可以得到更快的收敛速度，同时可以减少摇摆



在SGD中，梯度影响**位置**；

而在这个版本的更新中，物理观点建议**梯度只是影响速度**，然后**速度再影响位置**：

```python
 # 动量更新
    v = mu * v - learning_rate * dx # 与速度融合，mu其物理意义与摩擦系数更一致
    x += v # 与位置融合
```

也可以理解为： 

```python
 # 动量更新
    v = mu * v - (1 - mu) * dx # 与速度融合，mu其物理意义与摩擦系数更一致
    x += v # 与位置融合
```

mu通常取值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向



------



**NAG（Nesterov动量）**



SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。



当参数向量位于某个位置 *x* 时，观察上面的动量更新公式，动量部分会通过啊$mu * v$改变参数向量；

因此，如要计算梯度，那么可以将**未来的近似位置**$ x+mu*v$ 看做是“**向前看**”，这个点在我们一会儿要停止的位置附近。因此，**计算** $ x+mu*v$**的梯度**而不是“旧”位置 *x* 的梯度，使用Nesterov动量，我们就在这个“向前看”的地方计算梯度

```python
x_ahead = x + mu * v
计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)
v = mu * v - learning_rate * dx_ahead
x += v  
```

上面的程序还得计算dx_ahead，通过对 x_ahead = x + mu * v 使用变量变换进行改写，然后用x_ahead而不是x来表示上面的更新，即：实际**存储**的参数向量总是**向前一步版本**。x_ahead 的公式（将其**重新命名为x**）就变成了：

```python
v_prev = v # 存储备份
v = mu * v - learning_rate * dx # 速度更新保持不变
x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
```

mu=0.9

------



**RMSprop**

引用自Geoff Hinton的Coursera课程，具体说来，就是它使用了一个**梯度平方的滑动平均**：

```python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

decay_rate=0.9，learning_rate=0.001，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，但**其更新不会让学习率单调变小**；（不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度，而指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：）



------



**Adam**



**Adam本质上实际是RMSProp+动量**：Adam对每一个参数都计算自适应的学习率。除了像RMSprop一样存储一个历史梯度平方的滑动平均$vt$，Adam同时还保存一个历史梯度的滑动平均$mt$，类似于动量：

```python
# 根据历史梯度计算一阶动量和二阶动量
m_t = beta1*m + (1-beta1)*dx
v_t = beta2*v + (1-beta2)*(dx**2)

# 当mt和vt初始化为0向量时，发现它们都偏向于0，尤其是在初始化的步骤和当衰减率很小的时候（例如beta1和beta2趋向于1）,通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差
m_hat = m_t / 1 - (beta1 ** t）
v_hat = v_t / 1 - (beta2 ** t)

x += - learning_rate * m_hat / (np.sqrt(v_hat) + eps)
```

eps=1e-8, beta1=0.9, beta2=0.999



- [x] https://blog.csdn.net/google19890102/article/details/69942970


- [x] https://www.zhihu.com/question/323747423/answer/790457991




------



### 激活函数



- **Sigmoid**

  更倾向于更新靠近输出层的参数，而不是靠近输入层的参数

  导数为$f(x) * (1 - (f(x)))$

  导数取值范围【0，0.25】

  左右两侧都是**近似饱和区**，导数太小，容易造成梯度消失

  涉及指数运算，容易溢出

  输出值不以零为中心，会导致模型收敛速度慢

  激活函数的**偏移现象**

  

- **ReLU**

  Dead ReLU：当 $x <0$ 时，ReLU 输出恒为零。反向传播时，梯度恒为零，参数永远不会更新；

  激活部分神经元，增加稀疏性；

  计算简单，收敛速度快；



-  **ReLu6**  

  1. ReLU6比ReLU能更早学习到稀疏特征 
  2. 增强浮点数的小数位表达能力（整数位最大是6，所以只占3个bit，其他bit全部用来表达小数位）

  

- **Leaky ReLU**

  $LeakyReLU(x) = max(0.01x, x)$

  解决ReLU Dead；

------



- **Swish**

  $f(x) = x * sogmoid(\beta x)$

  可以看做是**介于线性函数与ReLU函数之间的平滑函数。**β是个常数或可训练的参数，Swish 具备**无上界有下界、平滑、非单调**的特性;



- **Hard-Swish**（[Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)） YOLO v5使用后会有10%的推理速度损失；
  $$
  \mathrm{h}-\mathrm{swish}(x)=x \frac{\operatorname{ReLU} 6(x+3)}{6}
  $$


- **SiLU**
  $$
  f(x)=x \cdot \sigma(x) \quad 
  $$



- **GELU** 

  与 Swish 激活函数$ x * sogmoid(\beta x)$的函数形式和性质非常相像，一个是固定系数 1.702，另一个是可变系数 β

  

------



- **Mish**

  Mish是一个光滑非单调的激活函数，在Backbone使用后内存会增大：$f(x)=x \cdot \tanh (\ln \left(1+e^{x}\right))$




### **分类loss**



A loss function is a part of a cost function which is a type of an objective function.



**BCE Loss:**

$-\frac{1}{n} \sum\left(y_{n} \times \ln x_{n}+\left(1-y_{n}\right) \times \ln \left(1-x_{n}\right)\right)$



$\mathrm{CE}(p, y)=\left\{\begin{array}{ll}-\log (p) & \text { if } y=1 \\ -\log (1-p) & \text { otherwise }\end{array}\right.$

其中![[公式]](https://www.zhihu.com/equation?tex=y%5Cin%5C%7B-1%2C+1%5C%7D)为真实标签，1表示为正例，-1表示为负例；而![[公式]](https://www.zhihu.com/equation?tex=p%5Cin%5B0%2C+1%5D)为模型预测为正例的概率值；





**BCEWithLogitsLoss = Sigmoid + BCELoss**



**Focal Loss**

与抽样方法不同，Focal Loss从另外的视角来解决样本不平衡问题，那就是**根据置信度动态调整交叉熵loss**，当预测正确的置信度增加时，loss的权重系数会逐渐衰减至0，这样模型训练的loss更关注难例，而大量容易的例子其loss贡献很低

解决了one-stage算法中**正负样本的比例失衡**：在CE基础上增加了一个调节因子$(1-p_t)^{\gamma}$
$$
F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log p_{t}
$$
${\gamma=2}$最好，FL相比CE可以大大降低简单例子的loss，使模型训练更关注于难例；



------



### 回归loss



Smooth L1 Loss![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) IoU Loss![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) GIoU Loss ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) DIoU Loss ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) CIoU Loss

------



**一、Smooth L1 Loss**
$$
\text { smooth }_{L_{1}}(x)=\left\{\begin{array}{cc}
0.5 x^{2} & i f|x|<1 \\
|x|-0.5 & \text { otherswise }
\end{array}\right.
$$

从损失函数对x的导数可知：

 ![[公式]](https://www.zhihu.com/equation?tex=L_%7B1%7D)损失函数对x的导数为常数，在训练后期，x很小时，如果学习率不变，损失函数会在稳定值附近波动，很难收敛到更高的精度；

 ![[公式]](https://www.zhihu.com/equation?tex=L_%7B2%7D) 损失函数对x的导数在x值很大时，其导数也非常大，在训练初期不稳定；

![[公式]](https://www.zhihu.com/equation?tex=smooth_%7BL_%7B1%7D%7D%5Cleft%28+x+%5Cright%29+) **完美的避开了** ![[公式]](https://www.zhihu.com/equation?tex=L_%7B1%7D)**和** ![[公式]](https://www.zhihu.com/equation?tex=L_%7B2%7D)**损失的缺点**



上面的三种Loss用于计算目标检测的Bounding Box Loss时，独立的求出4个点的Loss，然后进行相加得到最终的Bounding Box Loss，这种做法的假设是4个点是相互独立的，实际是有一定相关性的；



------



**二、IoU Loss**

![img](https://pic2.zhimg.com/80/v2-090938dacc24098c4e54aa18968f375d_1440w.jpg)



------



**三、GIoU Loss**



- **当预测框和目标框不相交时**，IoU(A,B)=0，不能反映A,B距离的远近，此时损失函数不可导，IoU Loss 无法优化两个框不相交的情况；
- 假设预测框和目标框的大小都确定，只要两个框的相交值是确定的，**其IoU值是相同时，IoU值不能反映两个框是如何相交的**；




$$
G I o U=I o U - \frac{|C -(A \cup B)|}{|C|}
$$

- GIoU具有尺度不变性；
- 当 ![[公式]](https://www.zhihu.com/equation?tex=A%5Crightarrow+B) 时，两者相同都等于1，此时 ![[公式]](https://www.zhihu.com/equation?tex=GIoU) 等于1；当 ![[公式]](https://www.zhihu.com/equation?tex=A%E5%92%8CB) 不相交时， ![[公式]](https://www.zhihu.com/equation?tex=GIoU%5Cleft%28+A%2CB+%5Cright%29+%3D+-1)



------



**四、DIoU Loss**



![img](https://pic2.zhimg.com/80/v2-d32d8fd6e32ecca603ea9678695b7241_1440w.jpg)

​			**当目标框完全包裹预测框的时候，IoU和GIoU的值都一样，此时GIoU退化为IoU, 无法区分其相对位置关系；**



好的目标框回归损失应该考虑三个重要的几何因素：**重叠面积，中心点距离，长宽比；**

**DIoU Loss**，相对于GIoU Loss**收敛速度更快**，该Loss考虑了**重叠面积和中心点距离**，但没有考虑到长宽比；

**CIoU Loss**，**其收敛的精度更高**，以上**三个因素都考虑到了**；


$$
L_{D I o U}=1-I o U+\frac{\rho^{2}\left(b, b^{g t}\right)}{c^{2}}
$$
其中 ![[公式]](https://www.zhihu.com/equation?tex=b%E5%92%8Cb%5E%7Bgt%7D) 分别表示 ![[公式]](https://www.zhihu.com/equation?tex=B%E5%92%8CB%5E%7Bgt%7D) 的中心点， ![[公式]](https://www.zhihu.com/equation?tex=%5Crho%5Cleft%28+%5Ccdot+%5Cright%29) 表示欧式距离， ![[公式]](https://www.zhihu.com/equation?tex=c) 表示 ![[公式]](https://www.zhihu.com/equation?tex=B%E5%92%8CB%5E%7Bgt%7D) 的最小外界矩形的对角线距离；



- 尺度不变性；
- 当两个框完全重合时， ![[公式]](https://www.zhihu.com/equation?tex=L_%7BIoU%7D%3DL_%7BGIoU%7D%3DL_%7BDIoU%7D%3D0) ,当2个框不相交时![[公式]](https://www.zhihu.com/equation?tex=L_%7BGIoU%7D%3DL_%7BDIoU%7D%5Crightarrow+2)；
- DIoU Loss可以直接优化2个框直接的距离，比GIoU Loss收敛速度更快；



------


**五、CIoU Loss**



CIoU的惩罚项是在DIoU的惩罚项基础上加了一个影响因子 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha%5Cupsilon) ，这个因子把**预测框长宽比拟合目标框的长宽比**考虑进去；

![[公式]](https://www.zhihu.com/equation?tex=L_%7BCIoU%7D+%3D+1-+IoU+%2B%5Cfrac%7B%5Crho%5E%7B2%7D%5Cleft%28+b%2Cb%5E%7Bgt%7D+%5Cright%29%7D%7Bc%5E%7B2%7D%7D+++%2B+%5Calpha%5Cupsilon)



------



### 多类别loss & 多标签分类



多个sigmoid与一个softmax都可以进行多分类.如果多个类别之间是互斥的，就应该使用softmax，即这个东西只可能是几个类别中的一种。如果多个类别之间不是互斥的，使用多个sigmoid；



| 分类问题名称   | 输出层使用激活函数 | 对应的损失函数                                       |
| -------------- | ------------------ | ---------------------------------------------------- |
| **二分类**     | **sigmoid函数**    | **二分类交叉熵损失函数（binary_crossentropy）**      |
| **多分类**     | **Softmax函数**    | **多类别交叉熵损失函数（categorical_crossentropy）** |
| **多标签分类** | **sigmoid函数**    | **二分类交叉熵损失函数（binary_crossentropy）**      |



**多标签问题与二分类问题关系：**

- **历史原因：**类别之间不严格互斥，softmax -> BCE ,YOLOv3 中person和man；
- **方法：**把一个多标签问题，转化为了在每个标签上的二分类问题；



------



### 提高模型速度



- TensorRT、int8
- amp 混合精度 + float16
- 将训练时候的3x3conv， 1x1conv， Identity在推理时融合为一个3x3conv



------



### 降低网络复杂度但不影响精度



- 模型压缩：通道剪枝 / 权重剪枝

- 蒸馏

- 重新设计卷积代替普通卷积：深度可分离卷积，RepVGG

  

------



# 其他



### **余弦距离与欧式距离**



**余弦距离**也称为余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小的度量。如果两个向量的方向一致, 即夹角接近零，那么这两个向量就相近。
$\cos \theta=\frac{\langle x, y\rangle}{\|x\| \cdot\|y\|}$

**欧式距离**
$d(x, y)=\sqrt{\sum_{i=0}^{N}\left(x_{i}-y_{i}\right)^{2}}$



**余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小，相比欧氏距离，余弦距离更加注重两个向量在方向上的差异；当对向量进行归一化后，欧式距离与余弦距离一致；**



------



### 偏差和方差

- 偏差：描述**预测值的期望**与**真实值**之间的差别，偏差越大说明模型的预测结果越差；
- 方差：描述**预测值的变化范围**，方差越大说明模型的预测越不稳定；
- **高方差过拟合，高偏差欠拟合；**
- 常用交叉验证来权衡模型的方差和偏差；

![img](https://upload-images.jianshu.io/upload_images/10890732-e5e6f8884addec1f.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp)



### Sigmoid & CE & Softmax溢出

------



sigmoid，CE，softmax函数在计算中，都会用到指数运算$e^{-x}$ 或$e^{x}$,如果**在 $e^{-x}$中-x是一个很小的负数，或者在 $e^{x}$中x是一个很大的正数，这时有溢出的风险**；



**Sigmoid：**
$$
\begin{array}{l}
\text { 1.如果 } x>0 \text { 则 } y=\frac{1}{1+e^{-x}} \\
\text { 2.如果 } x<0 \text { 则 } y=\frac{e^{x}}{1+e^{x}}
\end{array}
$$
**CE：**

$crossentropy=y * −log(sigmoid(x)) + (1−y) * −log(1−sigmoid(x))$

**对于$x < 0$有溢出风险时**，变换x为$log(e^x)$ 



**Softmax:**
$$
y=\frac{e^{x_{i}}{ }^{}}{\sum_{i=1}^{n} e^{x_{i}}}
$$


取所有$x_{i}$中的最大值M，分子分母同时除以$e^{M}$即可，**解决了上溢出的问题**，因为中间项都是小于等于1的值
$$
\ y=\frac{e^{x_{i}-M}}{\sum_{i=1}^{n} e^{x_{i}-M}}
$$



------

