# 机器学习



https://www.bilibili.com/video/BV1dJ411B7gh?p=4&spm_id_from=pageDriver



监督学习：SVM、神经网络

无监督学习：EM、clustering、PCA

半监督学习

强化学习：自动驾驶





## SVM—支持向量机



支持向量机是一种**二分类模型**，它的基本模型是**定义在特征空间上的间隔最大的线性分类器**，**间隔最大使它有别于感知机**；SVM还包括**核技巧**，这使它成为**实质上的非线性分类器**，SVM的**学习策略就是间隔最大化**，可形式化为一个求解凸二次规划的问题，也**等价于正则化的合页损失函数的最小化问题**，**SVM的的学习算法就是求解凸二次规划的最优化算法**；



**1.SVM的原理**

SVM学习的基本想法是求解**能够正确划分训练数据集并且几何间隔最大的分离超平面**
$$
\boldsymbol{w} \cdot x+b=0
$$
即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的



**2. SVM的核函数了解哪些 & 为什么要用核函数**

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分：



**①线性核函数**
$$
\kappa\left(x, x_{i}\right)=x \cdot x_{i}
$$
线性核，主要用于**线性可分**的情况，我们可以看到**特征空间到输入空间的维度是一样**的，其**参数少**速度快，对于线性可分数据，其分类效果很理想



**②多项式核函数**
$$
\kappa\left(x, x_{i}\right)=\left(\left(x \cdot x_{i}\right)+1\right)^{d}
$$
多项式核函数可以实现**将低维的输入空间映射到高纬的特征空间**，但是**多项式核函数的参数多**，当多项式的**阶数比较高**的时候，**核矩阵的元素值将趋于无穷大或者无穷小**，**计算复杂度会大**到无法计算



**③高斯（RBF）核函数**
$$
\kappa\left(x, x_{i}\right)=\exp \left(-\frac{\left\|x-x_{i}\right\|^{2}}{\delta^{2}}\right)
$$


高斯径向基函数是一**种局部性强的核函数**，其可以**将一个样本映射到一个更高维的空间内**，该核函数是**应用最广**的一个，无论大样本还是小样本都有比较好的性能，而且**其相对于多项式核函数参数要少**，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数



**④sigmoid核函数**
$$
\kappa\left(x, x_{i}\right)=\tanh \left(\eta<x, x_{i}>+\theta\right)
$$


采用sigmoid核函数，支持向量机实现的就是一种多层神经网络



- 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；
- 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；
- 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况；



**3.SVM如何解决线性不可分问题**

**间隔最大化**，通过引入**软间隔**、**核函数**解决线性不可分问题



**4. SVM优化的目标是什么 & SVM的损失函数 **

优化目标：凸优化
$$
\begin{aligned}
&\left.\min _{v, b} \frac{1}{2} \| w\right]^{q} \\
\text { s.t. } & y_{i}\left(w^{T} x_{i}+b\right) \geq 1
\end{aligned}
$$
SVM的损失函数：合页损失函数加上正则化项
$$
\sum_{i}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$


**5.SVM为什么要对偶(优化复杂度转变，核化)**

①首先是我们有不等式约束方程，这就需要我们写成min max的形式来得到最优解，而这种写成这种形式对x不能求导，所以我们需要转换成max min的形式，这样就能对x求导了，而为了满足这种对偶变换成立，就需要满足KKT条件（KKT条件是原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件）；

②对偶将原始问题中的约束转为了对偶问题中的等式约束；

③方便核函数的引入；

④改变了问题的复杂度：由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度，在对偶问题下，只与样本数量有关；



**6.LR和SVM异同，SVM使用场景**

**相同点**：

- LR和SVM都是**分类算法**；
- **如果不考虑核函数，LR和SVM都是线性分类算法**，也就是说他们的分类决策面都是线性的；
- **LR和SVM都是监督学习算法**；
- **LR和SVM都是判别模型**；

**不同点：**

- **损失函数不同**（lr的损失函数是 cross entropy loss，adaboost的损失函数是expotional loss ,svm是hinge loss，常见的回归模型通常用 均方误差 loss）；

- **SVM只考虑局部的边界线附近的点**，而**逻辑回归考虑全局**（远离的点对边界线的确定也起作用）；

- 在**解决非线性问题**时，**SVM采用核函数的机制**，而**LR通常不采用核函数的方法**；

- **线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization**，LR不受其影响；

- **SVM的损失函数就自带正则**，而**LR必须另外在损失函数上添加正则项**；

  

**SVM主要用于分类问题,主要的应用场景有字符识别、面部识别、行人检测、文本分类等领域**



**7.支持向量回归原理(SVR)**

SVR（支持向量回归）是SVM（支持向量机）中的一个重要的应用分支，**SVR回归与SVM分类的区别在于，SVR的样本点最终只有一类，它所寻求的最优超平面不是SVM那样使两类或多类样本点分的“最开”，而是使所有的样本点离着超平面的总偏差最小；**



## **余弦距离与欧式距离**



**余弦距离**也称为余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小的度量。如果两个向量的方向一致, 即夹角接近零，那么这两个向量就相近。
$\cos \theta=\frac{\langle x, y\rangle}{\|x\| \cdot\|y\|}$

**欧式距离**
$d(x, y)=\sqrt{\sum_{i=0}^{N}\left(x_{i}-y_{i}\right)^{2}}$



**余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小，相比欧氏距离，余弦距离更加注重两个向量在方向上的差异；当对向量进行归一化后，欧式距离与余弦距离一致；**



------



## 偏差和方差



- 偏差：描述**预测值的期望**与**真实值**之间的差别，偏差越大说明模型的预测结果越差；
- 方差：描述**预测值的变化范围**，方差越大说明模型的预测越不稳定；
- **高方差过拟合，高偏差欠拟合；**
- 常用交叉验证来权衡模型的方差和偏差；

![img](https://upload-images.jianshu.io/upload_images/10890732-e5e6f8884addec1f.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp)



## Sigmoid & CE & Softmax溢出

------



sigmoid，CE，softmax函数在计算中，都会用到指数运算$e^{-x}$ 或$e^{x}$,如果**在 $e^{-x}$中-x是一个很小的负数，或者在 $e^{x}$中x是一个很大的正数，这时有溢出的风险**；



**Sigmoid：**
$$
\begin{array}{l}
\text { 1.如果 } x>0 \text { 则 } y=\frac{1}{1+e^{-x}} \\
\text { 2.如果 } x<0 \text { 则 } y=\frac{e^{x}}{1+e^{x}}
\end{array}
$$
**CE：**

$crossentropy=y * −log(sigmoid(x)) + (1−y) * −log(1−sigmoid(x))$

**对于$x < 0$有溢出风险时**，变换x为$log(e^x)$ 



**Softmax:**
$$
y=\frac{e^{x_{i}}{ }^{}}{\sum_{i=1}^{n} e^{x_{i}}}
$$


取所有$x_{i}$中的最大值M，分子分母同时除以$e^{M}$即可，**解决了上溢出的问题**，因为中间项都是小于等于1的值
$$
\ y=\frac{e^{x_{i}-M}}{\sum_{i=1}^{n} e^{x_{i}-M}}
$$



------



## 图像的特征提取算法



**SIFT：**尺度不变特征变换，SIFT是一种检测局部特征的算法，该算法通过求一幅图中的特征点及其有关scale 和 orientation 的描述子得到特征并进行图像特征点匹配，获得了良好效果；SIFT特征不只具有尺度不变性，即使改变旋转角度，图像亮度或拍摄视角，仍然能够得到好的检测效果；

- 构建高斯金字塔，差分高斯金字塔(尺度不变)；
- 获取极值点，认为其为潜在的特征点；
- 删除部分边缘以及不稳定的点；
- 确定关键点主方向(旋转不变)；
- 生成描述符；128维（8 * （4 * 4区域））



**SURF:**加速稳健特征，SURF是对SIFT算法的改进，其基本结构、步骤与SIFT相近，但具体实现的过程有所不同：

- 积分图像和盒子滤波器结合，取代了高斯金字塔；（从而构建了Hessian矩阵元素值，进而缩短了特征提取的时间）
- 使用近似Harr小波计算特征点描述子，维度降低为64维；（基于Hessian行列式）



**HOG:**方向梯度直方图



**Canny 边缘检测器**：用于检测任何输入图像的边缘，有以下步骤：

- 使用高斯滤波器去除输入图像中的噪声；
- 计算高斯滤波器的导数，计算图像像素的梯度，得到沿x和y维度的幅度；
- 考虑垂直于给定边缘方向的任何曲线的一组邻居，抑制非最大边缘贡献像素点；
- 使用滞后阈值方法保留高于梯度幅值的像素，忽略低于低阈值的像素；



------



















