# 机器学习



https://www.bilibili.com/video/BV1dJ411B7gh?p=4&spm_id_from=pageDriver



监督学习：SVM、神经网络

无监督学习：EM、clustering、PCA

半监督学习

强化学习：自动驾驶





## SVM—支持向量机



支持向量机是一种**二分类模型**，它的基本模型是**定义在特征空间上的间隔最大的线性分类器**，**间隔最大使它有别于感知机**；SVM还包括**核技巧**，这使它成为**实质上的非线性分类器**，SVM的**学习策略就是间隔最大化**，可形式化为一个求解凸二次规划的问题，也**等价于正则化的合页损失函数的最小化问题**，**SVM的的学习算法就是求解凸二次规划的最优化算法**；



**1.SVM的原理**

SVM学习的基本想法是求解**能够正确划分训练数据集并且几何间隔最大的分离超平面**
$$
\boldsymbol{w} \cdot x+b=0
$$
即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的



**2. SVM的核函数了解哪些 & 为什么要用核函数**

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分：



**①线性核函数**
$$
\kappa\left(x, x_{i}\right)=x \cdot x_{i}
$$
线性核，主要用于**线性可分**的情况，我们可以看到**特征空间到输入空间的维度是一样**的，其**参数少**速度快，对于线性可分数据，其分类效果很理想



**②多项式核函数**
$$
\kappa\left(x, x_{i}\right)=\left(\left(x \cdot x_{i}\right)+1\right)^{d}
$$
多项式核函数可以实现**将低维的输入空间映射到高纬的特征空间**，但是**多项式核函数的参数多**，当多项式的**阶数比较高**的时候，**核矩阵的元素值将趋于无穷大或者无穷小**，**计算复杂度会大**到无法计算



**③高斯（RBF）核函数**
$$
\kappa\left(x, x_{i}\right)=\exp \left(-\frac{\left\|x-x_{i}\right\|^{2}}{\delta^{2}}\right)
$$


高斯径向基函数是一**种局部性强的核函数**，其可以**将一个样本映射到一个更高维的空间内**，该核函数是**应用最广**的一个，无论大样本还是小样本都有比较好的性能，而且**其相对于多项式核函数参数要少**，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数



**④sigmoid核函数**
$$
\kappa\left(x, x_{i}\right)=\tanh \left(\eta<x, x_{i}>+\theta\right)
$$


采用sigmoid核函数，支持向量机实现的就是一种多层神经网络



- 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；
- 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；
- 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况；



**3.SVM如何解决线性不可分问题**

**间隔最大化**，通过引入**软间隔**、**核函数**解决线性不可分问题



**4. SVM优化的目标是什么 & SVM的损失函数 **

优化目标：凸优化
$$
\begin{aligned}
&\left.\min _{v, b} \frac{1}{2} \| w\right]^{q} \\
\text { s.t. } & y_{i}\left(w^{T} x_{i}+b\right) \geq 1
\end{aligned}
$$
SVM的损失函数：合页损失函数加上正则化项
$$
\sum_{i}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$


**5.SVM为什么要对偶(优化复杂度转变，核化)**

①首先是我们有不等式约束方程，这就需要我们写成min max的形式来得到最优解，而这种写成这种形式对x不能求导，所以我们需要转换成max min的形式，这样就能对x求导了，而为了满足这种对偶变换成立，就需要满足KKT条件（KKT条件是原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件）；

②对偶将原始问题中的约束转为了对偶问题中的等式约束；

③方便核函数的引入；

④改变了问题的复杂度：由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度，在对偶问题下，只与样本数量有关；



**6.LR和SVM异同，SVM使用场景**

**相同点**：

- LR和SVM都是**分类算法**；
- **如果不考虑核函数，LR和SVM都是线性分类算法**，也就是说他们的分类决策面都是线性的；
- **LR和SVM都是监督学习算法**；
- **LR和SVM都是判别模型**；

**不同点：**

- **损失函数不同**（lr的损失函数是 cross entropy loss，adaboost的损失函数是expotional loss ,svm是hinge loss，常见的回归模型通常用 均方误差 loss）；

- **SVM只考虑局部的边界线附近的点**，而**逻辑回归考虑全局**（远离的点对边界线的确定也起作用）；

- 在**解决非线性问题**时，**SVM采用核函数的机制**，而**LR通常不采用核函数的方法**；

- **线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization**，LR不受其影响；

- **SVM的损失函数就自带正则**，而**LR必须另外在损失函数上添加正则项**；

  

**SVM主要用于分类问题,主要的应用场景有字符识别、面部识别、行人检测、文本分类等领域**



**7.支持向量回归原理(SVR)**

SVR（支持向量回归）是SVM（支持向量机）中的一个重要的应用分支，**SVR回归与SVM分类的区别在于，SVR的样本点最终只有一类，它所寻求的最优超平面不是SVM那样使两类或多类样本点分的“最开”，而是使所有的样本点离着超平面的总偏差最小；**



















