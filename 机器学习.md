# 机器学习



https://www.bilibili.com/video/BV1dJ411B7gh?p=4&spm_id_from=pageDriver



监督学习：SVM、神经网络

无监督学习：EM、clustering、PCA

半监督学习

强化学习：自动驾驶





## SVM—支持向量机



支持向量机是一种**二分类模型**，它的基本模型是**定义在特征空间上的间隔最大的线性分类器**，**间隔最大使它有别于感知机**；SVM还包括**核技巧**，这使它成为**实质上的非线性分类器**，SVM的**学习策略就是间隔最大化**，可形式化为一个求解凸二次规划的问题，也**等价于正则化的合页损失函数的最小化问题**，**SVM的的学习算法就是求解凸二次规划的最优化算法**；



**1.SVM的原理**

SVM学习的基本想法是求解**能够正确划分训练数据集并且几何间隔最大的分离超平面**
$$
\boldsymbol{w} \cdot x+b=0
$$
即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的



**2. SVM的核函数了解哪些 & 为什么要用核函数**

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分：



**①线性核函数**
$$
\kappa\left(x, x_{i}\right)=x \cdot x_{i}
$$
线性核，主要用于**线性可分**的情况，我们可以看到**特征空间到输入空间的维度是一样**的，其**参数少**速度快，对于线性可分数据，其分类效果很理想



**②多项式核函数**
$$
\kappa\left(x, x_{i}\right)=\left(\left(x \cdot x_{i}\right)+1\right)^{d}
$$
多项式核函数可以实现**将低维的输入空间映射到高纬的特征空间**，但是**多项式核函数的参数多**，当多项式的**阶数比较高**的时候，**核矩阵的元素值将趋于无穷大或者无穷小**，**计算复杂度会大**到无法计算



**③高斯（RBF）核函数**
$$
\kappa\left(x, x_{i}\right)=\exp \left(-\frac{\left\|x-x_{i}\right\|^{2}}{\delta^{2}}\right)
$$


高斯径向基函数是一**种局部性强的核函数**，其可以**将一个样本映射到一个更高维的空间内**，该核函数是**应用最广**的一个，无论大样本还是小样本都有比较好的性能，而且**其相对于多项式核函数参数要少**，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数



**④sigmoid核函数**
$$
\kappa\left(x, x_{i}\right)=\tanh \left(\eta<x, x_{i}>+\theta\right)
$$


采用sigmoid核函数，支持向量机实现的就是一种多层神经网络



- 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；
- 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；
- 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况；



**3.SVM如何解决线性不可分问题**

**间隔最大化**，通过引入**软间隔**、**核函数**解决线性不可分问题



**4. SVM优化的目标是什么 & SVM的损失函数 **

优化目标：凸优化
$$
\begin{aligned}
&\left.\min _{v, b} \frac{1}{2} \| w\right]^{q} \\
\text { s.t. } & y_{i}\left(w^{T} x_{i}+b\right) \geq 1
\end{aligned}
$$
SVM的损失函数：合页损失函数加上正则化项
$$
\sum_{i}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$


**5.SVM为什么要对偶(优化复杂度转变，核化)**

①首先是我们有不等式约束方程，这就需要我们写成min max的形式来得到最优解，而这种写成这种形式对x不能求导，所以我们需要转换成max min的形式，这样就能对x求导了，而为了满足这种对偶变换成立，就需要满足KKT条件（KKT条件是原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件）；

②对偶将原始问题中的约束转为了对偶问题中的等式约束；

③方便核函数的引入；

④改变了问题的复杂度：由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度，在对偶问题下，只与样本数量有关；



**6.LR和SVM异同，SVM使用场景**

**相同点**：

- LR和SVM都是**分类算法**；
- **如果不考虑核函数，LR和SVM都是线性分类算法**，也就是说他们的分类决策面都是线性的；
- **LR和SVM都是监督学习算法**；
- **LR和SVM都是判别模型**；

**不同点：**

- **损失函数不同**（lr的损失函数是 cross entropy loss，adaboost的损失函数是expotional loss ,svm是hinge loss，常见的回归模型通常用 均方误差 loss）；

- **SVM只考虑局部的边界线附近的点**，而**逻辑回归考虑全局**（远离的点对边界线的确定也起作用）；

- 在**解决非线性问题**时，**SVM采用核函数的机制**，而**LR通常不采用核函数的方法**；

- **线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization**，LR不受其影响；

- **SVM的损失函数就自带正则**，而**LR必须另外在损失函数上添加正则项**；

  

**SVM主要用于分类问题,主要的应用场景有字符识别、面部识别、行人检测、文本分类等领域**



**7.支持向量回归原理(SVR)**

SVR（支持向量回归）是SVM（支持向量机）中的一个重要的应用分支，**SVR回归与SVM分类的区别在于，SVR的样本点最终只有一类，它所寻求的最优超平面不是SVM那样使两类或多类样本点分的“最开”，而是使所有的样本点离着超平面的总偏差最小；**



## **余弦距离与欧式距离**



**余弦距离**也称为余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小的度量。如果两个向量的方向一致, 即夹角接近零，那么这两个向量就相近。
$\cos \theta=\frac{\langle x, y\rangle}{\|x\| \cdot\|y\|}$

**欧式距离**
$d(x, y)=\sqrt{\sum_{i=0}^{N}\left(x_{i}-y_{i}\right)^{2}}$



**余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小，相比欧氏距离，余弦距离更加注重两个向量在方向上的差异；当对向量进行归一化后，欧式距离与余弦距离一致；**



------



## 决策树



- ID3，C4.5和CART树决策树；
- 呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策模型进行分类；



①ID3 ---- ID3算法最核心的思想是**采用信息增益来选择特征**

②C4.5**采用信息增益比**，用于减少ID3算法的局限（在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的）

③CART算法**采用gini系数，不仅可以用来分类，也可以解决回归问题**



**决策树的分类**：离散性决策树、连续性决策树。

离散性决策树：离散性决策树，其目标变量是离散的，如性别：男或女等；

连续性决策树：连续性决策树，其目标变量是连续的，如工资、价格、年龄等；



**决策树的优点**：（1）具有**可读性**，如果给定一个模型，根据所产生的决策树很容易推理出相应的逻辑表达。（2）**分类速度快**，能在相对短的时间内能够对大型数据源做出可行且效果良好的结果

**决策树的缺点**：（1）对未知的测试数据未必有好的分类、泛化能力，即**可能发生过拟合**现象，此时可采用剪枝或随机森林。

------



## 偏差 & 方差



**模型误差 = 偏差 + 方差+ 不可避免的误差**



- 偏差：描述**预测值的期望**与**真实值**之间的差别，偏差越大说明模型的预测结果越差；
- 方差：描述**预测值的变化范围**，方差越大说明模型的预测越不稳定；
- **高方差过拟合，高偏差欠拟合；**
- 常用交叉验证来权衡模型的方差和偏差；

![img](https://upload-images.jianshu.io/upload_images/10890732-e5e6f8884addec1f.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp)



------



## 贝叶斯公式 & 极大似然估计 & 最大后验估计


$$
P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}
$$


**极大似然估计（MLE）：**在已经得到试验结果（即样本）的情况下，估计满足这个样本分布的参数，将使这个样本出现的概率最大的那个参数Θ作为真参数Θ的估计。在样本固定的情况下，样本出现的概率与参数Θ之间的函数，称为似然函数;



**最大后验概率（MAP）：**最大后验估计是根据经验数据，获得对难以观察的量的点估计。与最大似然估计不同的是，最大后验估计融入了被估计量的先验分布，即模型参数本身的概率分布。



最大后验概率估计其实就是多了一个参数的先验概率，也可以认为最大似然估计就是把先验概率认为是一个定值；`后验概率 = 似然 * 先验概率`





## EM算法--最大期望算法



**在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量**

最大期望算法经过两个步骤交替进行计算：

- 第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；
- 第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。



## 降维方法



**主成分分析（PCA）**

主成分分析(PCA) 是最常用的**线性降维方法**，它的目标**是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大**，以此使用较少的数据维度，同时保留住较多的原数据点的特性。是将原空间变换到特征向量空间内，数学表示为AX = γX



**线性判别分析（LDA）**

LDA是一种**有监督的线性降维算法**。与PCA保持数据信息不同，核心思想：**往线性判别超平面的法向量上投影，使得区分度最大（高内聚，低耦合）**。LDA是为了使得**降维后的数据点尽可能地容易被区分**！





## **条件随机场**

CRF即条件随机场（Conditional Random Fields），是在给定一组输入随机变量条件下另外一组输出随机变量的条件概率分布模型，它是一种判别式的概率无向图模型，既然是判别式，那就是对条件概率分布建模



## 隐马尔科夫模型

隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别



## **伯努利分布**



伯努利分布(Bernoulli distribution)又名两点分布或0-1分布





## 假设检验



假设检验的基本思想是小概率反证法思想。小概率思想是指小概率事件(P<0．01或P<0．05)在一次试验中基本上不会发生。反证法思想是先提出假设(检验假设Ho)，再用适当的统计方法确定假设成立的可能性大小，如可能性小，则认为假设不成立，若可能性大，则还不能认为假设不成立。





## bagging和boosting的联系和区别



Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即**将弱分类器组装成强分类器的方法**。



boosting（提升法）：Boosting是一种可将弱学习器提升为强学习器的算法。其工作机制为：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

Bagging（套袋法）：Bagging是指采用Bootstrap（**有放回的均匀抽样**）的方式从训练数据中抽取部分数据训练多个分类器，每个分类器的权重是一致的，然后通过投票的方式取票数最高的分类结果最为最终结果。



区别：1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.Boosting：每一轮的训练集不变(个人觉得这里说的训练集不变是说的总的训练集，对于每个分类器的训练集还是在变化的，毕竟每次都是抽样)，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.

2）样例权重：Bagging：使用均匀取样，每个样例的权重相等Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.

3）预测函数：Bagging：所有预测函数的权重相等.Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.

4）并行计算：Bagging：各个预测函数可以并行生成Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT



## 随机森林

随机森林属于集成学习（Ensemble Learning）中的bagging算法。在集成学习中，主要分为bagging算法和boosting算法。

**Bagging + 决策树 = 随机森林**

**随机森林的随机性**体现在**每颗树的训练样本是随机的**，树中**每个节点的分裂属性集合也是随机选择确定的**。有了这2个随机的保证，随机森林就不会产生过拟合的现象了；

**调参**：一般采用网格搜索法优化超参数组合。这里将调参方法简单归纳为三条：1、分块调参（不同框架参数分开调参）；2、一次调参不超过三个参数；3、逐步缩小参数范围



## 树模型（Adaboost, GBDT, XGBOOST）

Adaboost与GBDT两者boosting的不同策略是两者的本质区别。

Adaboost强调Adaptive（自适应），通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting

而GBDT则是旨在不断减少残差（回归），通过不断加入新的树旨在在残差减少（负梯度）的方向上建立一个新的模型。——即损失函数是旨在最快速度降低残差。

而XGBoost的boosting策略则与GBDT类似，区别在于GBDT旨在通过不断加入新的树最快速度降低残差，而XGBoost则可以人为定义损失函数（可以是最小平方差、logistic loss function、hinge loss function或者人为定义的loss function），只需要知道该loss function对参数的一阶、二阶导数便可以进行boosting，其进一步增大了模型的泛华能力，其贪婪法寻找添加树的结构以及loss function中的损失函数与正则项等一系列策略也使得XGBoost预测更准确。





## #########################################################



## LR和SVM



**联系：**

- **都可以处理分类问题**，且一般都用于**处理线性二分类问题**（在改进的情况下**可以处理多分类问题**），**也可以用来做非线性分类（加核函数）**；；
- 两个方法**都可以增加不同的正则化项**，如l1、l2等；
- **都是线性模型**（不要考虑核函数）；
- **都属于判别模型**；



**区别：**

- LR是参数模型，SVM是非参数模型；
- 目标函数：逻辑回归采用的是logistical loss，SVM采用的是hinge loss （目的都是增加对分类影响较大的数据的权重，减少与分类关系较小的数据的权重）；
- 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便；SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算；