# Deep Learning Tricks



## æ¨¡å‹æ•ˆæœå·®çš„åŸå› 

```
ï¼ˆ1ï¼‰æ¨¡å‹è‡ªèº«ç»“æ„->æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼ˆæ·±åº¦å’Œå®½åº¦ï¼‰

ï¼ˆ2ï¼‰è¶…å‚æ•°é€‰æ‹© -å­¦ä¹ ç‡ï¼Œä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥

ï¼ˆ3ï¼‰æ•°æ®æ¨¡å‹ä¸åŒ¹é… 

ï¼ˆ4ï¼‰æ•°æ®é›†æ„é€ ï¼šæ²¡æœ‰è¶³å¤Ÿæ•°æ®ã€åˆ†ç±»ä¸å‡è¡¡ã€æœ‰å™ªå£°çš„æ ‡ç­¾ã€è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒä¸å‡è¡¡ï¼›
```



## è§£å†³æ¬ æ‹Ÿåˆ

```
ï¼ˆ1ï¼‰è®©æ¨¡å‹æ›´å¤§ï¼šç»™æ¨¡å‹åŠ å…¥æ›´å¤šçš„å±‚ eg.ResNet-50 -> resNet-101ï¼Œæ¯å±‚ä¸­æ›´å¤šçš„å•å…ƒï¼›

ï¼ˆ2ï¼‰å‡å°‘æ­£åˆ™åŒ–

ï¼ˆ3ï¼‰é”™è¯¯åˆ†æï¼šï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å¸ƒåå·®ï¼‰æµ‹è¯•æ—¶å€™å‡ºç°é—®é¢˜è¿›è¡Œåˆ†æï¼Œè®­ç»ƒé›†ç¼ºå°‘å“ªäº›æƒ…å†µå¯¼è‡´é”™è¯¯ï¼Œåç»­å°†åœ¨è®­ç»ƒé›†ä¸­åŠ å…¥æ­¤ç±»æ•°æ®çº æ­£åå·®ï¼›

ï¼ˆ4ï¼‰æ”¹è¿›æ¨¡å‹æ¶æ„

ï¼ˆ5ï¼‰è°ƒèŠ‚è¶…å‚æ•°ï¼šæ‰‹åŠ¨ä¼˜åŒ–ã€ç½‘æ ¼æœç´¢ã€éšæœºæœç´¢ã€ç”±ç²—åˆ°ç»†ã€è´å¶æ–¯ä¼˜åŒ–ï¼›

ï¼ˆ6ï¼‰åŠ å…¥æ›´å¤šç‰¹å¾
```



## è§£å†³è¿‡æ‹Ÿåˆ

```
å¢åŠ è®­ç»ƒé›†æ ·æœ¬ï¼Œæ­£åˆ™åŒ–
```



## å·¥ç¨‹è°ƒå‚

```
3x3 convæ˜¯CNNä¸»æµç»„ä»¶ï¼ˆ3x3Convæœ‰åˆ©äºä¿æŒå›¾åƒæ€§è´¨ï¼‰ï¼›

å·ç§¯æ ¸æƒé‡åˆå§‹åŒ–ä½¿ç”¨xavierï¼ˆTanhï¼‰æˆ–è€…He normalï¼ˆReLUï¼ŒPyTorché»˜è®¤ï¼‰ ï¼›

Batch Normalizationæˆ–è€…Group Normalizationï¼›

ä½¿ç”¨ACNetçš„å·ç§¯æ–¹å¼ï¼›

cv2è¦æ¯”osè¯»å–å›¾ç‰‡é€Ÿåº¦å¿«

åŠ é€Ÿè®­ç»ƒpin_memory=true,work_numbers=4(å¡çš„æ•°é‡x4)ï¼Œdata.to(device,  no_blocking=True)

å­¦ä¹ ç‡å’ŒåŠ¨é‡ï¼šä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡+å¤§çš„åŠ¨é‡å¯ä»¥åŠ å¿«æ¨¡å‹çš„è®­ç»ƒä¸”å¿«é€Ÿæ”¶æ•›

Adam learning rateï¼š 3e-4

L2æ­£åˆ™åŒ–ï¼šL2åƒä¸‡ä¸è¦è°ƒå¤ªå¤§ï¼Œä¸ç„¶ç‰¹åˆ«éš¾è®­ç»ƒï¼›L2ä¹Ÿä¸èƒ½å¤ªå°ï¼Œä¸ç„¶è¿‡æ‹Ÿåˆä¸¥é‡ï¼›å³ä½¿æ­£ç¡®åœ°ä½¿ç”¨æ­£åˆ™åŒ–å¼ºåº¦ï¼Œä¹Ÿä¼šå¯¼è‡´éªŒè¯é›†å‰æœŸä¸ç¨³å®šç”šè‡³å‘ˆç°è®­ç»ƒä¸åˆ°çš„ç°è±¡ï¼Œä½†æ˜¯ä¹‹åå°±ä¼šç¨³å®šä¸‹æ¥

ä¼˜åŒ–å™¨+å­¦ä¹ ç‡ç­–ç•¥+momentumï¼š

	1.SGD+momentumåœ¨å¤§å­¦ä¹ ç‡+å¤§åŠ¨é‡çš„æ—¶å€™æ•ˆæœæ›´å¥½

	2.ä¸ç®¡æ˜¯SGDè¿˜æ˜¯Adamè¿˜æ˜¯AdamWï¼Œå­¦ä¹ ç‡çš„è°ƒæ•´éƒ½å¯¹ä»–ä»¬æœ‰å¸®åŠ©

	3.å¸¦æœ‰momentumçš„SGDåŠ ä½™å¼¦é€€ç«æ”¶æ•›æ›´å¿«ä¸”æ›´åŠ ç¨³å®š

	4.å­¦ä¹ ç‡æœ€å¥½è®¾å®šå¥½ä¸‹é™ï¼Œä¸ç„¶åæœŸä¼šè®­ç»ƒä¸åŠ¨


æŠŠæ•°æ®æ”¾å†…å­˜é‡Œï¼Œé™ä½ io å»¶è¿Ÿ

sudo mount tmpfs /path/to/your/data -t tmpfs -o size=30G


åŠ¨æ€æŸ¥çœ‹GPUåˆ©ç”¨ç‡

watch -n 1 nvidia-smi


åœ¨æ˜¾å­˜å¤§å°å›ºå®šæƒ…å†µä¸‹num_workerså’Œbatchsizeæ˜¯åæ¯”ä¾‹å…³ç³»
```



## è¿ç§»å­¦ä¹ 

```
- å¦‚æœè®­ç»ƒé›†å°ï¼Œè®­ç»ƒæ•°æ®ä¸é¢„è®­ç»ƒæ•°æ®ç›¸ä¼¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å†»ä½å·ç§¯å±‚ï¼Œç›´æ¥è®­ç»ƒå…¨è¿æ¥å±‚ï¼›
- å¦‚æœè®­ç»ƒé›†å°ï¼Œè®­ç»ƒæ•°æ®ä¸é¢„è®­ç»ƒæ•°æ®ä¸ç›¸ä¼¼ï¼Œé‚£ä¹ˆå¿…é¡»ä»å¤´è®­ç»ƒå·ç§¯å±‚åŠå…¨è¿æ¥å±‚ï¼›
- å¦‚æœè®­ç»ƒé›†å¤§ï¼Œè®­ç»ƒæ•°æ®ä¸é¢„è®­ç»ƒæ•°æ®ç›¸ä¼¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„æƒé‡å‚æ•°åˆå§‹åŒ–ç½‘ç»œï¼Œç„¶åä»å¤´å¼€å§‹è®­ç»ƒï¼›
- å¦‚æœè®­ç»ƒé›†å¤§ï¼Œè®­ç»ƒæ•°æ®ä¸é¢„è®­ç»ƒæ•°æ®ä¸ç›¸ä¼¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„æƒé‡å‚æ•°åˆå§‹åŒ–ç½‘ç»œï¼Œç„¶åä»å¤´å¼€å§‹è®­ç»ƒæˆ–è€…å®Œå…¨ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œé‡æ–°å¼€å§‹ä»å¤´è®­ç»ƒï¼ˆæ¨èï¼‰ï¼›

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºå¤§æ•°æ®é›†ï¼Œä¸æ¨èå†»ä½å·ç§¯å±‚ï¼Œç›´æ¥è®­ç»ƒå…¨è¿æ¥å±‚çš„æ–¹å¼ï¼Œè¿™å¯èƒ½ä¼šå¯¹æ€§èƒ½é€ æˆå¾ˆå¤§å½±å“ï¼›
```



## Anaconda


```
# æ¸…ç†ç¼“å­˜
conda clean -a

# å®‰è£…requirementsé‡Œé¢çš„ç‰ˆæœ¬
conda install --yes --file requirements.txt

# æµ‹è¯•cudaæ˜¯å¦å¯ç”¨
import torch
import torchvision
print(torch.cuda.is_available())
print(torch.version.cuda)

# åˆ é™¤condaç¯å¢ƒ
conda remove -n name --all

# condaæ¢æºè®°å¾—å»æ‰defaultï¼Œæ·»åŠ pytorch
```



```
# conda åˆ›å»ºç¯å¢ƒ + è£…cuda + PyTorch

conda create -n name python=3.8
conda install cudatoolkit=10.1
conda install cudnn
ä½¿ç”¨pytorchå®˜ç½‘çš„pip/condaå‘½ä»¤è£…torchå’Œtorchvision

```





## Batch Normalization æ”¹è¿›

BNï¼ˆBatch Normalizationï¼‰å‡ ä¹æ˜¯ç›®å‰ç¥ç»ç½‘ç»œçš„å¿…é€‰ç»„ä»¶ï¼Œä½†æ˜¯ä½¿ç”¨BNæœ‰ä¸¤ä¸ªå‰æè¦æ±‚ï¼š

1. minibatchå’Œå…¨éƒ¨æ•°æ®åŒåˆ†å¸ƒã€‚å› ä¸ºè®­ç»ƒè¿‡ç¨‹æ¯ä¸ªminibatchä»æ•´ä½“æ•°æ®ä¸­å‡åŒ€é‡‡æ ·ï¼Œä¸åŒåˆ†å¸ƒçš„è¯minibatchçš„å‡å€¼å’Œæ–¹å·®å’Œè®­ç»ƒæ ·æœ¬æ•´ä½“çš„å‡å€¼å’Œæ–¹å·®æ˜¯ä¼šå­˜åœ¨è¾ƒå¤§å·®å¼‚çš„ï¼Œåœ¨æµ‹è¯•çš„æ—¶å€™ä¼šä¸¥é‡å½±å“ç²¾åº¦ã€‚
2. batchsizeä¸èƒ½å¤ªå°ï¼Œå¦åˆ™æ•ˆæœä¼šè¾ƒå·®ï¼Œè®ºæ–‡ç»™çš„ä¸€èˆ¬æ€§ä¸‹é™æ˜¯32ã€‚

BNæœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼š

- é™ä½å¯¹åˆå§‹åŒ–ã€å­¦ä¹ ç‡ç­‰è¶…å‚çš„æ•æ„Ÿç¨‹åº¦ï¼Œå› ä¸ºæ¯å±‚çš„è¾“å…¥è¢«BNæ‹‰æˆç›¸å¯¹ç¨³å®šçš„åˆ†å¸ƒï¼Œä¹Ÿèƒ½åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ã€‚
- åº”å¯¹æ¢¯åº¦é¥±å’Œå’Œæ¢¯åº¦å¼¥æ•£ï¼Œä¸»è¦æ˜¯å¯¹äºä½¿ç”¨sigmoidå’Œtanhçš„æ¿€æ´»å‡½æ•°çš„ç½‘ç»œã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfrkQgT3nibY56wze3Rx5w17KibyCvBLicZZ30icnGByAuiavhtFxBtDXwGV3uibia5rsJCRfPIPSmwdYypFA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**batchsizeè¿‡å°çš„åœºæ™¯**

å®é™…çš„é¡¹ç›®ä¸­ï¼Œç»å¸¸é‡åˆ°éœ€è¦å¤„ç†çš„å›¾ç‰‡å°ºåº¦è¿‡å¤§çš„åœºæ™¯ï¼Œä¾‹å¦‚æˆ‘ä»¬ä½¿ç”¨500wåƒç´ ç”šè‡³2000wåƒç´ çš„å·¥ä¸šç›¸æœºè¿›è¡Œæ•°æ®é‡‡é›†ï¼Œ500wçš„ç›¸æœºé‡‡é›†çš„å›¾ç‰‡å°ºåº¦å°±æ˜¯2500X2000å·¦å³ã€‚è€Œå¯¹äºå¾®å°çš„ç¼ºé™·æ£€æµ‹ã€é«˜ç²¾åº¦çš„å…³é”®ç‚¹æ£€æµ‹æˆ–å°ç‰©ä½“çš„ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¸å¤ªæƒ³ç²—æš´é™ä½è¾“å…¥å›¾ç‰‡çš„åˆ†è¾¨ç‡ï¼Œè¿™æ ·è¿èƒŒäº†æˆ‘ä»¬ä½¿ç”¨é«˜åˆ†è¾¨ç‡ç›¸æœºçš„åˆè¡·ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´ä¸¢å¤±æœ‰ç”¨ç‰¹å¾ã€‚åœ¨ç®—åŠ›æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„batchsizeå°±æ— æ³•è®¾ç½®å¤ªå¤§ï¼Œç”šè‡³åªèƒ½ä¸º1æˆ–2ã€‚å°çš„batchsizeä¼šå¸¦æ¥å¾ˆå¤šè®­ç»ƒä¸Šçš„é—®é¢˜ï¼Œå…¶ä¸­BNé—®é¢˜å°±æ˜¯æœ€çªå‡ºçš„ã€‚è™½ç„¶å¤§batchsizeè®­ç»ƒæ˜¯ä¸€ä¸ªå…±è¯†ï¼Œä½†æ˜¯ç°å®ä¸­å¯èƒ½æ— æ³•å…·æœ‰å……è¶³çš„èµ„æºï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€äº›å¤„ç†æ‰‹æ®µã€‚



é¦–å…ˆBatch Normalization ä¸­çš„Normalizationè¢«ç§°ä¸ºæ ‡å‡†åŒ–ï¼Œé€šè¿‡å°†æ•°æ®è¿›è¡Œå¹³å’Œç¼©æ”¾æ‹‰åˆ°ä¸€ä¸ªç‰¹å®šçš„åˆ†å¸ƒã€‚BNå°±æ˜¯åœ¨batchç»´åº¦ä¸Šè¿›è¡Œæ•°æ®çš„æ ‡å‡†åŒ–ã€‚BNçš„å¼•å…¥æ˜¯ç”¨æ¥è§£å†³ internal covariate shift é—®é¢˜ï¼Œå³è®­ç»ƒè¿­ä»£ä¸­ç½‘ç»œæ¿€æ´»çš„åˆ†å¸ƒçš„å˜åŒ–å¯¹ç½‘ç»œè®­ç»ƒå¸¦æ¥çš„ç ´åã€‚BNé€šè¿‡åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£çš„æ—¶å€™ï¼Œåˆ©ç”¨minibatchè®¡ç®—å‡ºçš„å½“å‰batchçš„å‡å€¼å’Œæ–¹å·®ï¼Œè¿›è¡Œæ ‡å‡†åŒ–æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚



**ä¸¤ç§è§£å†³æ–¹å¼ï¼šBRN + CBN**

**BRN**

æœ¬æ–‡çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºbatchsizeè¾ƒå°ï¼Œå½“å‰minibatchç»Ÿè®¡åˆ°çš„å‡å€¼å’Œæ–¹å·®ä¸å…¨éƒ¨æ•°æ®æœ‰å·®å¼‚ï¼Œé‚£ä¹ˆå°±å¯¹å½“å‰çš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œä¿®æ­£ã€‚ä¿®æ­£çš„æ–¹æ³•ä¸»è¦æ˜¯åˆ©ç”¨åˆ°é€šè¿‡æ»‘åŠ¨å¹³å‡æ”¶é›†åˆ°çš„å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®ã€‚

**CBN**

æœ¬æ–‡è®¤ä¸ºBRNçš„é—®é¢˜åœ¨äºå®ƒä½¿ç”¨çš„å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®ä¸æ˜¯å½“å‰ç½‘ç»œæƒé‡ä¸‹è·å–çš„ï¼Œå› æ­¤ä¸æ˜¯exactlyæ­£ç¡®çš„ï¼Œæ‰€ä»¥batchsizeå†å°ä¸€ç‚¹ï¼Œä¾‹å¦‚ä¸º1æˆ–2æ—¶å°±ä¸å¤ªworkäº†ã€‚æœ¬æ–‡ä½¿ç”¨æ³°å‹’å¤šé¡¹å¼é€¼è¿‘åŸç†æ¥ä¿®æ­£å½“å‰çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ŒåŒæ ·ä¹Ÿæ˜¯é—´æ¥åˆ©ç”¨äº†å…¨å±€çš„å‡å€¼å’Œæ–¹å·®ä¿¡æ¯ã€‚ç®€è¿°å°±æ˜¯ï¼šå½“å‰batchçš„å‡å€¼å’Œæ–¹å·®æ¥è‡ªä¹‹å‰çš„Kæ¬¡è¿­ä»£å‡å€¼å’Œæ–¹å·®çš„å¹³å‡ï¼Œç”±äºç½‘ç»œæƒé‡ä¸€ç›´åœ¨æ›´æ–°ï¼Œæ‰€ä»¥ä¸èƒ½ç›´æ¥ç²—æš´æ±‚å¹³å‡ã€‚æœ¬æ–‡è€Œæ˜¯åˆ©ç”¨æ³°å‹’å…¬å¼ä¼°è®¡å‰é¢çš„è¿­ä»£åœ¨å½“å‰æƒé‡ä¸‹çš„æ•°å€¼ã€‚



## Pytorchæé€Ÿ



**1.æ‰¾åˆ°è®­ç»ƒè¿‡ç¨‹çš„ç“¶é¢ˆ**

```
https://pytorch.org/docs/stable/bottleneck.html
```



**2.å›¾ç‰‡è§£ç **

PyTorchä¸­é»˜è®¤ä½¿ç”¨çš„æ˜¯Pillowè¿›è¡Œå›¾åƒçš„è§£ç ï¼Œä½†æ˜¯å…¶æ•ˆç‡è¦æ¯”Opencvå·®ä¸€äº›ï¼Œå¦‚æœå›¾ç‰‡å…¨éƒ¨æ˜¯JPEGæ ¼å¼ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨TurboJpegåº“è§£ç ã€‚å…·ä½“é€Ÿåº¦å¯¹æ¯”å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/mmbiz_png/SdQCib1UzF3szsSYrT2hU8JJhwlWibS4D4VHHTZKQXPuWDzfOiaaN26v6egU70QOWv5p4yUonYPPMqBnyXiaYlhqZg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)





**3.æ•°æ®å¢å¼ºåŠ é€Ÿ**

åœ¨PyTorchä¸­ï¼Œé€šå¸¸ä½¿ç”¨transformeråšå›¾ç‰‡åˆ†ç±»ä»»åŠ¡çš„æ•°æ®å¢å¼ºï¼Œè€Œå…¶è°ƒç”¨çš„æ˜¯CPUåšä¸€äº›Cropã€Flipã€Jitterç­‰æ“ä½œã€‚å¦‚æœä½ é€šè¿‡è§‚å¯Ÿå‘ç°ä½ çš„CPUåˆ©ç”¨ç‡éå¸¸é«˜ï¼ŒGPUåˆ©ç”¨ç‡æ¯”è¾ƒä½ï¼Œé‚£è¯´æ˜ç“¶é¢ˆåœ¨äºCPUé¢„å¤„ç†ï¼Œå¯ä»¥ä½¿ç”¨Nvidiaæä¾›çš„DALIåº“åœ¨GPUç«¯å®Œæˆè¿™éƒ¨åˆ†æ•°æ®å¢å¼ºæ“ä½œã€‚

```
https://github.com/NVIDIA/DALI

Daliæ–‡æ¡£ï¼šhttps://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/index.html
```



**4.data Prefetch**

```
https://zhuanlan.zhihu.com/p/66145913
https://zhuanlan.zhihu.com/p/97190313
```



**5.learning rate schedule**

**Cyclical Learning Rates** and the **1Cycle learning rate schedule** are both methods introduced by Leslie N. Smith. Essentially, the 1Cycle learning rate schedule looks something like this:

![img](https://efficientdl.com/content/images/2020/11/art5_lr_schedule.png)

Sylvain writes: 

> 1cycle consists of  two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.



**PyTorch implements** both of these methods `torch.optim.lr_scheduler.CyclicLR` and `torch.optim.lr_scheduler.OneCycleLR` see [the documentation](https://pytorch.org/docs/stable/optim.html).

**One drawback** of these schedulers is that they introduce a number of additional hyperparameters.

**Why does this work** One[ possible explanation](https://arxiv.org/pdf/1506.01186.pdf)might be that regularly increasing the learning rate helps to traverse [saddle points in the loss landscape ](https://papers.nips.cc/paper/2015/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf)more quickly.



**6.Use multiple workers and pinned memory in `DataLoader`**

When using [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), set `num_workers > 0`, rather than the default value of 0, and `pin_memory=True`, rather than the default value of `False`. 

A rule of thumb that [people are using ](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5)to choose **the number of workers is to set it to four times the number of available GPUs** with both **a larger and smaller number of workers leading to a slow down.**



**7.Max out the batch size**

It seems like using the largest batch size your GPU memory permits **will accelerate your training** . Note that you will also have to adjust other hyperparameters, such as the learning rate, if you modify the batch size. **A rule of thumb here is to double the learning rate as you double the batch size.**

 **Might lead to solutions that generalize worse than those trained with smaller batches.**



**8. Use Automatic Mixed Precision (AMP)**

The release of PyTorch 1.6 included a native implementation of Automatic Mixed Precision training to PyTorch. The main idea here is that certain operations can be run faster and without a loss of accuracy at semi-precision (FP16) rather than in the single-precision (FP32) used elsewhere. AMP, then, automatically decide which operation should be executed in which format. This allows both for faster training and a smaller memory footprint.



**9.Using another optimizer**

AdamW is Adam with weight decay (rather than L2-regularization) and is now available natively in PyTorch as 
`torch.optim.AdamW`. AdamW seems to consistently outperform Adam in terms of both the error achieved and the training time. 

Both Adam and AdamW work well with the 1Cycle policy described above.



**10.Turn on cudNN benchmarking**

If your model architecture remains fixed and your input size stays constant, setting `torch.backends.cudnn.benchmark = True` might be beneficial. 



**11.Beware of frequently transferring data between CPUs and GPUs**

Beware of frequently transferring tensors from a GPU to a CPU using`tensor.cpu()` and vice versa using `tensor.cuda()` as these are relatively expensive. The same applies for `.item()` and `.numpy()` â€“ use `.detach()` instead.

If you are creating a new tensor, you can also directly assign it to your GPU using the keyword argument `device=torch.device('cuda:0')`.

If you do need to transfer data, using `.to(device, non_blocking=True)`, might be useful [as long as you don't have any synchronization points](https://discuss.pytorch.org/t/should-we-set-non-blocking-to-true/38234/4) after the transfer.



**12.Use gradient/activation checkpointing**

> Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, **the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass.** It can be applied on any part of a model.

> Specifically, in the forward pass, `function` will run in [`torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad)manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the `function` parameter. In the backwards pass, the saved inputs and `function` is retrieved, and the forward pass is computed on `function` again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.

So while this will might slightly increase your run time for a given batch size, you'll significantly reduce your memory footprint. This in turn will allow you to further increase the batch size you're using allowing for better GPU utilization.

While checkpointing is implemented natively as `torch.utils.checkpoint`([docs](https://pytorch.org/docs/stable/checkpoint.html)), it does seem to take some thought and effort to implement properly. 



**13.Use gradient accumulation**

Another approach to increasing the batch size is to accumulate gradients across multiple `.backward()` passes before calling `optimizer.step()`.

This method was developed mainly to circumvent GPU memory limitations and I'm not entirely clear on the trade-off between having additional `.backward()` loops.



**14.Use Distributed Data Parallel for multi-GPU training**

one simple one is to use `torch.nn.DistributedDataParallel` rather than `torch.nn.DataParallel`. By doing so, each GPU will be driven by a dedicated CPU core avoiding the GIL issues of `DataParallel`.

https://pytorch.org/tutorials/beginner/dist_overview.html



**15.Set gradients to None rather than 0**

Use `.zero_grad(set_to_none=True)` rather than `.zero_grad()`.

Doing so will let the memory allocator handle the gradients rather than actively setting them to 0. This will lead to yield a *modest* speed-up as they say in the [documentation](https://pytorch.org/docs/stable/optim.html), so don't expect any miracles.

Watch out, **doing this is not side-effect free**! Check the docs for the details on this.



**16.Use `.as_tensor()` rather than `.tensor()`**

`torch.tensor()` always copies data. If you have a numpy array that you want to convert, use `torch.as_tensor()` or `torch.from_numpy()` to avoid copying the data.



**17.Use gradient clipping**

In PyTorch this can be done using `torch.nn.utils.clip_grad_norm_`([documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_)).



**18.Turn off bias before BatchNorm**

This is a very simple one: turn off the bias of layers before BatchNormalization layers. For a 2-D convolutional layer, this can be done by setting the bias keyword to False: `torch.nn.Conv2d(..., bias=False, ...)`.



**19.Turn off gradient computation during validation**

This one is straightforward: set `torch.no_grad()` during validation.



**20.Use input and batch normalization**

You're probably already doing this but you might want to double-check:

- Are you [normalizing](https://pytorch.org/docs/stable/torchvision/transforms.html) your input? 
- Are you using [batch-normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)?

And [here's](https://stats.stackexchange.com/questions/437840/in-machine-learning-how-does-normalization-help-in-convergence-of-gradient-desc) a reminder of why you probably should.



## 2020ç æ¸¯æ¾³äººå·¥æ™ºèƒ½ç®—æ³•å¤§èµ›



**æ•°æ®é›†**

```
1.å›¾åƒå°ºå¯¸ä¸ä¸€ã€è¿‘æ™¯å’Œè¿œæ™¯ç›®æ ‡å°ºåº¦å·®å¼‚å¤§ï¼š
		å›¾ç‰‡å°ºå¯¸ä¸ä¸€ï¼Œç›¸å·®è¾ƒå¤§ã€‚ä¸€æ–¹é¢ï¼Œç”±äºè®¡ç®—èµ„æºå’Œç®—æ³•æ€§èƒ½çš„é™åˆ¶ï¼Œå¤§å°ºå¯¸çš„å›¾åƒä¸èƒ½ä½œä¸ºç½‘ç»œçš„è¾“å…¥ï¼Œè€Œå•çº¯å°†åŸå›¾åƒç¼©æ”¾åˆ°å°å›¾ä¼šä½¿å¾—ç›®æ ‡ä¸¢å¤±å¤§é‡ä¿¡æ¯ã€‚å¦ä¸€æ–¹é¢ï¼Œå›¾åƒä¸­è¿‘æ™¯å’Œè¿œæ™¯çš„ç›®æ ‡å°ºåº¦å·®å¼‚å¤§ï¼Œå¯¹äºæ£€æµ‹å™¨æ¥è¯´ï¼Œæ˜¯ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚

2.ç›®æ ‡åœ¨å›¾åƒä¸­åˆ†å¸ƒå¯†é›†ï¼Œå¹¶ä¸”é®æŒ¡ä¸¥é‡ï¼š
		æ•°æ®é›†å‡æ˜¯åˆ©ç”¨æ‘„åƒå¤´ä»çœŸå®åœºæ™¯é‡‡é›†ï¼Œéƒ¨åˆ†æ•°æ®çš„ç›®æ ‡å¯†é›†åº¦è¾ƒå¤§ã€‚éƒ½å‡ºç°äº†é¢‘ç¹å‡ºç°é®æŒ¡ç°è±¡ï¼Œç›®æ ‡çš„æ¼æ£€æƒ…å†µç›¸å¯¹ä¸¥é‡ã€‚
```



```
anchor-basedï¼šğŸ”¥
1ï¼‰ä¼˜ç‚¹ï¼šåŠ å…¥äº†å…ˆéªŒçŸ¥è¯†ï¼Œæ¨¡å‹è®­ç»ƒç›¸å¯¹ç¨³å®šï¼›å¯†é›†çš„anchor boxå¯æœ‰æ•ˆæé«˜å¬å›ç‡ï¼Œå¯¹äºå°ç›®æ ‡æ£€æµ‹æ¥è¯´æå‡éå¸¸æ˜æ˜¾ã€‚
2ï¼‰ç¼ºç‚¹ï¼šå¯¹äºå¤šç±»åˆ«ç›®æ ‡æ£€æµ‹ï¼Œè¶…å‚æ•°scaleå’Œaspect ratioç›¸å¯¹éš¾è®¾è®¡ï¼›å†—ä½™boxéå¸¸å¤šï¼Œå¯èƒ½ä¼šé€ æˆæ­£è´Ÿæ ·æœ¬å¤±è¡¡ï¼›åœ¨è¿›è¡Œç›®æ ‡ç±»åˆ«åˆ†ç±»æ—¶ï¼Œè¶…å‚IOUé˜ˆå€¼éœ€æ ¹æ®ä»»åŠ¡æƒ…å†µè°ƒæ•´ã€‚

anchor-freeï¼š
1ï¼‰ä¼˜ç‚¹ï¼šè®¡ç®—é‡å‡å°‘ï¼›å¯çµæ´»ä½¿ç”¨ã€‚
2ï¼‰ç¼ºç‚¹ï¼šå­˜åœ¨æ­£è´Ÿæ ·æœ¬ä¸¥é‡ä¸å¹³è¡¡ï¼›ä¸¤ä¸ªç›®æ ‡ä¸­å¿ƒé‡å çš„æƒ…å†µä¸‹ï¼Œé€ æˆè¯­ä¹‰æ¨¡ç³Šæ€§ï¼›æ£€æµ‹ç»“æœç›¸å¯¹ä¸ç¨³å®šã€‚
```

è€ƒè™‘åˆ°é¡¹ç›®æƒ…å†µï¼š

1ï¼‰å±äºå°ç±»åˆ«æ£€æµ‹ï¼Œç›®æ ‡çš„scaleå’Œaspect ratioéƒ½åœ¨ä¸€å®šèŒƒå›´ä¹‹å†…ï¼Œå±å¯æ§å› ç´ ã€‚

2ï¼‰æ¯”èµ›æ•°æ®ä¸­å­˜åœ¨å¾ˆå¤šç›®æ ‡é®æŒ¡æƒ…å†µï¼Œè¿™æœ‰å¯èƒ½ä¼šé€ æˆç›®æ ‡ä¸­å¿ƒé‡æ–°ï¼Œå¦‚æœé‡‡ç”¨anchor-freeï¼Œä¼šé€ æˆè¯­ä¹‰æ¨¡ç³Šæ€§ï¼›

3ï¼‰scaleå’Œaspect ratioå¯æ§ï¼Œé‚£ä¹ˆè¶…å‚IOUè°ƒæ•´ç›¸å¯¹ç®€å•ï¼›

4ï¼‰å¯¹æ¨¡å‹éƒ¨ç½²æ²¡æœ‰ç‰¹æ®Šè¦æ±‚ï¼Œå› æ­¤ï¼Œéƒ¨ç½²æ–¹æ¡ˆç›¸å¯¹è¾ƒå¤šï¼Œæ¨¡å‹æ€§èƒ½æœ‰å¾ˆå¤§æ”¹è¿›ã€‚



**é¡¹ç›®åˆ†æ**

```
é¦–å…ˆæ ¹æ®è®­ç»ƒæ•°æ®é›†è¿›è¡Œåˆ†æï¼Œåœ¨10537å¼ è®­ç»ƒå›¾åƒä¸­ï¼Œæ€»å…±æœ‰12ä¸ªç»„åˆç±»åˆ«ã€15ä¸ªåœºæ™¯ã€18304ä¸ªç›®æ ‡æ¡†ã€‚å­˜åœ¨ä»¥ä¸‹ä¸‰ç§æƒ…å†µï¼š
ï¼ˆ1ï¼‰æ ·æœ¬ä¸å¹³è¡¡ï¼Œ12ä¸ªç»„åˆä¸­ï¼Œä»…é•¿è¢–-é•¿è£¤ç»„åˆå æ€»æ•°æ®çš„76.45%ï¼›
ï¼ˆ2ï¼‰åœºæ™¯æ ·æœ¬ä¸å‡è¡¡ï¼Œå•†åœºã€å·¥å‚å’Œè¡—æ‹ç­‰äº”ä¸ªåœºæ™¯ä¸­å æ¯”86.18%ï¼›
ï¼ˆ3ï¼‰å¤šç§çŠ¶æ€è¡Œäººï¼Œä¾‹å¦‚é‡å ã€æ®‹ç¼ºã€å’Œå æ¯”å°ä¸”é®æŒ¡ã€‚


å¦å¤–ï¼Œè¦æƒè¡¡æ£€æµ‹åˆ†ç±»çš„ç²¾åº¦å’Œæ¨¡å‹è¿è¡Œçš„é€Ÿåº¦ï¼Œå› æ­¤æˆ‘ä»¬å†³å®šé€‰ç”¨æ£€æµ‹åˆ†ç±»ç²¾åº¦è¾ƒå¥½çš„ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼ŒåŒæ—¶ä½¿ç”¨æ¨¡å‹å‹ç¼©å’Œæ¨¡å‹åŠ é€Ÿæ–¹æ³•å®ŒæˆåŠ é€Ÿã€‚å…¶ä¸»ä½“æ€è·¯ä¸ºï¼š
ï¼ˆ1ï¼‰ ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼šåŸºäºYOLOv5çš„one-stageæ£€æµ‹æ¡†æ¶ï¼›
ï¼ˆ2ï¼‰ æ¨¡å‹å‹ç¼©ï¼šåŸºäºBNæ”¾ç¼©å› å­ä¿®å‰ªä¸»å¹²ç½‘ç»œï¼›Slimmingåˆ©ç”¨é€šé“ç¨€ç–åŒ–çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°1ï¼‰å‡å°‘æ¨¡å‹å¤§å°ï¼›2ï¼‰å‡å°‘è¿è¡Œæ—¶å†…å­˜å ç”¨ï¼›3ï¼‰åœ¨ä¸å½±å“ç²¾åº¦çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æ“ä½œæ•°ã€‚
ï¼ˆ3ï¼‰ æ¨¡å‹åŠ é€Ÿï¼šTensorRTå°è£…éƒ¨ç½²ã€‚åœ¨ç¡®ä¿ç²¾åº¦ç›¸å¯¹ä¸å˜çš„æƒ…å†µä¸‹ï¼Œé‡‡ç”¨FP16æ¯”FP32é€Ÿåº¦å¯æå‡1.5å€å·¦å³ã€‚å¦å¤–ï¼ŒTensorRTæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ æ¨ç†ä¼˜åŒ–å™¨ï¼Œå¯ä»¥ä¸ºæ·±åº¦å­¦ä¹ åº”ç”¨æä¾›ä½å»¶è¿Ÿã€é«˜ååçš„éƒ¨ç½²æ¨ç†ã€‚

ä½¿ç”¨albumentationså®Œæˆæ•°æ®å¢å¼ºï¼ˆmosaicæ•°æ®å¢å¼ºä¼šå‡ºç°æ­£æ ·æœ¬æ•°æ®è¢«ç ´åæƒ…å†µï¼‰



æ¨¡å‹é¢„æµ‹å­˜åœ¨å¤§é‡çš„è¯¯æ£€å’Œæ¼æ£€ã€‚è¿™äº›æ¼æ£€å’Œæ— æ„ä¹‰çš„æ£€æµ‹ç»“æœå¤§å¹…é™ä½äº†æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¸Šè¿°é—®é¢˜å½’çº³ä¸ºä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢çš„åŸå› ï¼š
1ã€YOLOv5sæ— è®ºæ˜¯ç½‘ç»œå®½åº¦å’Œç½‘ç»œæ·±åº¦éƒ½è¾ƒå°ï¼Œå­¦ä¹ èƒ½åŠ›ç›¸å¯¹è¾ƒå¼±ã€‚å°æ‘Šä½å é“å’Œå…¶ä»–æ­£å¸¸è½¦è¾†ååˆ†ç›¸ä¼¼ï¼Œå®¹æ˜“å¯¹åˆ†ç±»å™¨é€ æˆæ··æ·†ï¼Œä»è€Œäº§ç”Ÿè¯¯æ£€ï¼›
2ã€è®­ç»ƒå’Œæµ‹è¯•æ—¶è¾“å…¥æ¨¡å‹çš„å›¾åƒå°ºåº¦ä¸åˆé€‚ã€‚å›¾åƒç»è¿‡ç¼©æ”¾åï¼Œç›®æ ‡çš„å°ºåº¦ä¹Ÿéšä¹‹å˜å°ï¼Œå¯¼è‡´è¿œæ™¯ä¸­äººçš„å°æ‘Šè´©ç­‰åŒºåŸŸè¢«å¤§é‡é—æ¼ï¼›

é¦–å…ˆï¼Œä»å›¾åƒé¢„å¤„ç†æ–¹é¢ï¼Œä½¿ç”¨éšæœºä¸­å¿ƒè£å‰ªæ–¹å¼åˆ‡å›¾è¿›è¡Œè®­ç»ƒã€‚éšæœºçª—å£åˆ‡å›¾æ˜¯ä¸€ç§å¸¸ç”¨çš„å¤§å›¾åƒå¤„ç†æ–¹å¼ï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆåœ°ä¿ç•™å›¾åƒçš„é«˜åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œä¸åŒå¤§å°çš„ç›®æ ‡ï¼Œå¦ä¸€æ–¹é¢é‡‡ç”¨å¤šå°ºåº¦è®­ç»ƒï¼Œè¿™æ ·ä½¿å¾—ç½‘ç»œè·å¾—çš„ä¿¡æ¯æ›´åŠ ä¸°å¯Œã€‚å¦‚æœæŸä¸ªç›®æ ‡å¤„äºåˆ‡å›¾è¾¹ç•Œï¼Œæ ¹æ®ç›®æ ‡æ¡†çš„ä¸å›¾ç‰‡çš„å¤§å°æ¯”ä¾‹æ¥å†³å®šæ˜¯å¦ä¿ç•™ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†éšæœºå‡ ä½•å˜æ¢ã€é¢œè‰²æ‰°åŠ¨ã€ç¿»è½¬ã€å¤šå°ºåº¦ã€mixupã€GridMaskã€Mosaicç­‰æ•°æ®å¢å¹¿æ–¹å¼ï¼Œéƒ½å¯æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå°ç›®æ ‡æ£€æµ‹ç‡ã€‚

å…¶æ¬¡ï¼Œä»ä¼˜åŒ–å™¨å±‚é¢æ¥è®²ï¼Œæˆ‘ä»¬å°è¯•äº†ä¼˜åŒ–å™¨æ¢¯åº¦å½’ä¸€åŒ–å’ŒSAMä¼˜åŒ–å™¨ã€‚
ä¼˜åŒ–å™¨æ¢¯åº¦å½’ä¸€åŒ–æœ‰ä¸‰ä¸ªå¥½å¤„ï¼šï¼ˆ1ï¼‰åŠ é€Ÿæ”¶æ•›ï¼›ï¼ˆ2ï¼‰é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼›ï¼ˆ3ï¼‰é˜²æ­¢è¿‡æ‹Ÿåˆï¼›
SAMä¼˜åŒ–å™¨å…·æœ‰å›ºæœ‰çš„é²æ£’æ€§ã€‚


3255å¼ æµ‹è¯•é›†ä¸­1080*1920å°ºå¯¸çš„å›¾åƒä¸å…¶ä»–å°ºå¯¸çš„å›¾åƒæ¯”ä¾‹çº¦ä¸º7:3ã€‚äºæ˜¯æˆ‘ä»¬TensorRTéƒ¨ç½²æ—¶ï¼Œæ¨¡å‹ä½¿ç”¨è¾“å…¥å¤§å°ä¸º384*640æ¯”640*640æ£€æµ‹ç‡æ›´ä¼˜ã€‚å› ä¸º1080*1920ç›´æ¥resizeä¸º640*640ï¼Œä¸€æ–¹é¢ä¼šåˆ°å€¼ç›®æ ‡å˜å½¢ï¼Œå¦ä¸€é¢ï¼Œç›®æ ‡å˜å¾—æ›´å°ã€‚
```



**ç»“è®º**

```
1ã€ æ•°æ®åˆ†æå¯¹äºè®­ç»ƒæ¨¡å‹è‡³å…³é‡è¦ã€‚æ•°æ®ä¸å¹³è¡¡ã€å›¾åƒå°ºå¯¸å’Œç›®æ ‡å¤§å°ä¸ä¸€ã€ç›®æ ‡å¯†é›†å’Œé®æŒ¡ç­‰é—®é¢˜ï¼Œåº”é€‰ç”¨å¯¹åº”çš„baselineå’Œåº”å¯¹ç­–ç•¥ã€‚ä¾‹å¦‚ï¼Œæ•°æ®ä¸å¹³è¡¡å¯å°è¯•è¿‡é‡‡æ ·ã€focal lossã€æ•°æ®å¢å¼ºç­‰ç­–ç•¥ï¼›å›¾åƒå°ºå¯¸å’Œç›®æ ‡å¤§å°ä¸ä¸€å¯é‡‡ç”¨å¤šå°ºåº¦ã€æ•°æ®è£å‰ªç­‰æ–¹æ³•ã€‚
2ã€ é’ˆå¯¹ç®—æ³•ç²¾åº¦å’Œæ€§èƒ½ä¸¤è€…å–èˆæ¥è¯´ï¼Œå¯å…ˆå®éªŒç½‘ç»œå¤§å°å’Œè¾“å…¥å›¾ç‰‡å¤§å°å¯¹æ¨¡å‹ç»“æœçš„å½±å“ï¼Œä¸åŒä»»åŠ¡å’Œä¸åŒæ•°æ®æƒ…å†µï¼Œä¸¤è€…ç›¸å·®è¾ƒå¤§ã€‚æ‰€ä»¥ä¸èƒ½ä¸€å‘³ä¸ºäº†æé«˜é€Ÿåº¦ï¼Œå•çº¯å‹ç¼©ç½‘ç»œå¤§å°ï¼›
3ã€ é’ˆå¯¹æ€§èƒ½è¦æ±‚æ—¶ï¼Œå¯é‡‡ç”¨TensorRTç­‰æ–¹å¼éƒ¨ç½²æ¨¡å‹ï¼Œä¹Ÿå¯é‡‡ç”¨æ¨¡å‹å‹ç¼©ç­‰æ–¹å¼ï¼Œè¿™æ ·å¯åœ¨ä¿è¯é€Ÿåº¦çš„å‰æä¸‹ï¼Œä½¿ç”¨è¾ƒå¤§ç½‘ç»œï¼Œæå‡æ¨¡å‹ç²¾åº¦ã€‚
```

