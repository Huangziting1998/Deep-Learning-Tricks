 

# PyTorch & Python



```
from torch.utils import data
from torch import nn   # loss， optim， layer
```



------



### Pytorch默认梯度累积



- 当显存小，batchsize不够时，可以使用梯度累积变相增大batchsize；
- weight在不同模型之间交互时候有好处；（动手学习深度学习v2）

```py
accumulation_steps = batch_size // opt.batch_size

loss = loss / accumulation_steps
running_loss += loss.item()
loss.backward()

if ((i + 1) % accumulation_steps) == 0:
	optimizer.step()
	scheduler.step()
	optimizer.zero_grad()
```



------



### PyTorch提速

- **图片解码**：cv2要比Pillow读取图片速度快
- 加速训练**pin_memory=true / work_numbers=x(卡的数量x4) / prefetch_factor=2 / data.to(device,  no_blocking=True)**
- DALI库在GPU端完成这部分**数据增强**，而不是transformer做图片分类任务的数据增强
- OneCycleLR + SGD / AdamW
- `torch.nn.Conv2d(..., bias=False, ...)`
- DP or DDP 
- 不要频繁在CPU和GPU之间转移数据
- `torch.backends.cudnn.benchmark = True`
- `from torch.cuda import amp`使用FP16



------



### nn.Module & nn.Functional



**nn.Module**实现的layer是由class Layer(nn.Module)定义的特殊类，**会自动提取可学习参数nn.Parameter**

**nn.functional**中的函数更像是**纯函数**，由def function(input)定义，一般只定义一个操作，因为其无法保存参数



**Function**需要定义三个方法：**init, forward, backward**（需要自己写求导公式） 

**Module**只需定义 __init__和**forward**，而backward的计算由自动求导机制



**对于激活函数和池化层，由于没有可学习参数，一般使用nn.functional完成，其他的有学习参数的部分则使用nn.Module**

但是**Droupout**由于在训练和测试时操作不同，所以**建议使用nn.Module实现**，它能够通过**model.eval**加以区分



- [x] https://blog.csdn.net/wzy_zju/article/details/81262472?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242


- [x] https://blog.csdn.net/andyjkt/article/details/107428618




------



### nn.Sequential & nn.ModuleList



**区别：**

- **nn.Sequential内部实现了forward函数**，因此可以不用写forward函数（或者继承nn.Module类的话，就要写出forward函数）；而nn.ModuleList则没有实现内部forward函数； -> 如果完全直接用 nn.Sequential，确实是可以的，但这么做的代价就是失去了部分灵活性，不能自己去定制 forward 函数里面的内容了;
- **nn.Sequential可以使用OrderedDict对每层进行命名**;
- **nn.Sequential里面的模块按照顺序进行排列的**，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而**nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言**。



**nn.Sequential**

- nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的；
- nn.Sequential中可以使用OrderedDict来指定每个module的名字，而不是采用默认的命名方式；



```python
import torch
import torch.nn as nn
import torch.nn.functional as F
class net_seq(nn.Module):
    def __init__(self):
        super(net2, self).__init__()
        self.seq = nn.Sequential(
                        nn.Conv2d(1,20,5),
                         nn.ReLU(),
                          nn.Conv2d(20,64,5),
                       nn.ReLU()
                       )      
    def forward(self, x):
        return self.seq(x)
net_seq = net_seq()
```

```python
from collections import OrderedDict

class net_seq(nn.Module):
    def __init__(self):
        super(net_seq, self).__init__()
        self.seq = nn.Sequential(OrderedDict([
                        ('conv1', nn.Conv2d(1,20,5)),
                         ('relu1', nn.ReLU()),
                          ('conv2', nn.Conv2d(20,64,5)),
                       ('relu2', nn.ReLU())
                       ]))
    def forward(self, x):
        return self.seq(x)
net_seq = net_seq()
```

**nn.ModuleList**

- nn.ModuleList，它是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。你可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中。使用 Python 的 list 添加的卷积层和它们的 parameters 并没有自动注册到我们的网络中；
- 

```python
class net_modlist(nn.Module):
    def __init__(self):
        super(net_modlist, self).__init__()
        self.modlist = nn.ModuleList([
                       nn.Conv2d(1, 20, 5),
                       nn.ReLU(),
                        nn.Conv2d(20, 64, 5),
                        nn.ReLU()
                        ])

    def forward(self, x):
        for m in self.modlist:
            x = m(x)
        return x

net_modlist = net_modlist()
```



------



### DataLoader & Sampler & DataSet 



假设我们的数据是一组图像，每一张图像对应一个index，那么如果我们要读取数据就只需要对应的index即可，即上面代码中的`indices`，而选取index的方式有多种，有按顺序的，也有乱序的，所以这个工作需要`Sampler`完成，`DataLoader`和`Sampler`在这里产生关系

我们已经拿到了indices，那么下一步我们只需要根据index对数据进行读取即可了，这时`Dataset`和`DataLoader`产生关系

```
-------------------------------------
| DataLoader						|				
|									|							
|			Sampler -----> Indices	|  													
|                       |			|	
|      DataSet -----> Data			|
|						|			|			
------------------------|------------                    
						|s						
                        Training
```



DataLoader 的源代码初始化参数里有两种sampler：`sampler`和`batch_sampler`，都默认为`None`；前者的作用是生成一系列的index，而batch_sampler则是将sampler生成的indices打包分组，得到一个又一个batch的index



Pytorch中已经实现的`Sampler`有如下几种：`SequentialSampler` 	`RandomSampler`	 `WeightedSampler` 	`SubsetRandomSampler`

- 如果你自定义了`batch_sampler`,那么这些参数都必须使用默认值：`batch_size`, `shuffle`,`sampler`,`drop_last`.
- 如果你自定义了`sampler`，那么`shuffle`需要设置为`False`
- 如果`sampler`和`batch_sampler`都为`None`,那么`batch_sampler`使用Pytorch已经实现好的`BatchSampler`,而`sampler`分两种情况：
  - 若`shuffle=True`,则`sampler=RandomSampler(dataset)`
  - 若`shuffle=False`,则`sampler=SequentialSampler(dataset)`



Dataset定义方式如下：

```python
class Dataset(object):
	def __init__(self):
		...
		
	def __getitem__(self, index): # 能让该类可以像list一样通过索引值对数据进行访问
		return ...
	
	def __len__(self):
		return ...
```



- [x] https://www.cnblogs.com/marsggbo/p/11308889.html




------



### BN & Dropout



#### **Dropout在训练和测试时候的区别**



`torch.nn.Dropout2d(p=0.5, inplace=False)`：input shape: (N, C, H, W)， output shape: (N, C, H, W)



**Dropout在层与层之间加噪声，是一种正则**；**在全连接使用，CNN用BN**

Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0来控制模型复杂度，以提高模型的泛化能力，减少过拟合；



**Dropout 在训练时采用**，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险；而在测试时，应该用整个训练好的模型，因此**测试时不需要dropout**；

=> 在测试时如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，用户可能认为模型预测不准。那么**一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的**。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望；

```python
mask = (torch.Tensor(X.shape).uniform_(0, 1) > dropout).float()
return X * Mask / (1 - p)
```



------



#### PyTorch中BN



在PyTorch中**将gamma和beta改叫weight、bias**，使得打印网络参数时候只会打印出weight和bias（PyTorch中只有可学习的参数才称为Parameter）,但是`Net.state_dict()`是有running_mean和running_var的**因为running_mean和running_var不是可以学习的变量，只是训练过程对很多batch的数据统计;**



BN层的**输出Y与输入X之间的关系**：**Y = (X - running_mean) / sqrt(running_var + eps) * gamma + beta**，其中**gamma、beta为可学习参数（在PyTorch中分别改叫weight和bias），训练时通过反向传播更新**；而**running_mean、running_var则是在前向时先由X计算出mean和var，再由mean和var以动量momentum来更新running_mean和running_var**，所以**在训练阶段，running_mean和running_var在每次前向时更新一次**；在**测试阶段，则通过`net.eval()`固定该BN层的running_mean和running_var，此时这两个值即为训练阶段最后一次前向时确定的值，并在整个测试阶段保持不变；**



```
训练时：
running_mean = (1 - momentum) * running_mean + momentum * mean_cur
running_var = (1 - momentum) * running_var + momentum * var_cur
```

```
测试时：
running_mean = running_mean
running_var = running_var
```

**先更新running_mean和running_var，再计算BN；**



------



#### BN理论



**B x H x W，不涉及Channel，数据归一化方法；**

**BN的精髓在于归一之后，使用$\gamma, \beta$作为还原参数，在一定程度上保留原数据的分布；**



**提出原因**

**解决Internal Covariate Shift**：训练数据在经过网络的每一层后其分布都发生了变化；

**缓解过拟合**：



```
批规范化（Batch Normalization，BN）：在 minibatch维度 上在每次训练iteration时对隐藏层进行归一化
标准化（Standardization）：对输入 数据 进行归一化处理
正则化（Regularization）：通常是指对 参数 在量级和尺度上做约束，缓和过拟合情况，L1 L2正则化
```



**作用**

- 将数据规整到统一区间，减少数据的发散程度，可以加快模型训练时的收敛速度，使得模型训练过程更加稳定；
- 避免梯度爆炸或者梯度消失；
- 起到一定的正则化作用，防止过拟合；



 $Input: B=\left\{x_{1 \ldots m}\right\} ; \gamma, \beta($ parameters to be learned $)$

$\text { Output }:\left\{y_{i}=B N_{\gamma, \beta}\left(x_{i}\right)\right\} \\$
$$
\begin{array}{r}

\mu_{B} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\
\sigma_{B}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2} \\
\tilde{x}_{i} \leftarrow \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}} \\
y_{i} \leftarrow \gamma \tilde{x}_{i}+\beta
\end{array}
$$

均值的计算，就是在一个批次内，将每个通道中的数字单独加起来，再除以 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+H+%5Ctimes+W) ；举个例子：该批次内有10张图片，每张图片有三个通道RBG，每张图片的高、宽是H、W，那么均值就是计算**10张图片R通道的像素数值总和**除以**![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)** ，再计算B通道全部像素值总和除以![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)，最后计算G通道的像素值总和除以![[公式]](https://www.zhihu.com/equation?tex=10+%5Ctimes+H+%5Ctimes+W)。方差的计算类似；

可训练参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%E3%80%81%5Cbeta) 的维度等于**张量的通道数**，在上述例子中，RBG三个通道分别需要一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 和一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) ，所以 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec%7B%5Cgamma%7D%E3%80%81%5Cvec%7B%5Cbeta%7D) 的维度等于3；



------



#### **卷积层和BN层的融合**



**BN层最酷的地方是它可以用一个1x1卷积等效替换，更进一步地，我们可以将BN层合并到前面的卷积层中；**
$$
\begin{aligned}
y_{\text {conv }} &=w \cdot x+b \\
y_{b n} &=\gamma \cdot\left(\frac{y_{\text {conv }}-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
&=\gamma \cdot\left(\frac{w x+b-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
\hat{w} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot w \\
\hat{b} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot(b-E[x])+\beta \\
y_{b n} &=\hat{w} \cdot x+\hat{b}
\end{aligned}
$$



------



#### **BN和Dropdout同时使用出现的问题及解决方法**



**方差偏移现象**

Dropout 与 BN 之间冲突的关键是**网络状态切换过程中存在神经方差的不一致行为**。试想若有神经响应 X，当网络从训练转为测试时，Dropout 可以通过其随机失活保留率（即 p）来缩放响应，并在学习中改变神经元的方差，而 BN 仍然维持 X 的统计滑动方差。这种方差不匹配可能导致数值不稳定。而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能。事实上，如果没有 Dropout，那么实际前馈中的神经元方差将与 BN 所累计的滑动方差非常接近，这也保证了其较高的测试准确率。



**解决方案**

1.在所有 BN 层后使用 Dropout；

2.修改 Dropout 的公式让它对方差并不那么敏感，就是高斯Dropout、均匀分布Dropout；



------



### Model.Eval & Torch.No_Grad



**两者都在Inference时候使用，但是作用不相同：**

```
model.eval() 负责改变batchnorm、dropout的工作方式，如在eval()模式下，dropout是不工作的；
torch.no_grad() 负责关掉梯度计算，节省eval的时间；
```



**只进行Inference时，`model.eval()`是必须使用的，否则会影响结果准确性； 而`torch.no_grad()`并不是强制的，只影响运行效率；**



------



### Tensor

扩大张量：`torch.Tensor.expand(*sizes) → Tensor`

压缩张量：`torch.squeeze(input, dim=None, out=None) → Tensor`



------



### 动态图 & 静态图



**TensorFlow:** Build graph once, then run many times (static)
**PyTorch:** Each forward pass defines a new graph (dynamic)



动态图是运算和搭建同时进行，也就是可以先计算前面的节点的值，再根据这些值搭建后面的计算图。优点是灵活，易调节，易调试。PyTorch 里的很多写法跟其他 Python 库的代码的使用方法是完全一致的，没有任何额外的学习成本。

静态图是先搭建图，然后再输入数据进行运算。优点是高效，因为静态计算是通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度会比动态图更快。但是不灵活。TensorFlow 每次运行的时候图都是一样的，是不能够改变的，所以不能直接使用 Python 的 while 循环语句，需要使用辅助函数 tf.while_loop 写成 TensorFlow 内部的形式。



------



### 迭代器 & 生成器



**迭代器：**实现了__iter__和__next__方法的对象都称为迭代器。迭代器是一个有状态的对象，在调用next() 的时候返回下一个值，如果容器中没有更多元素了，则抛出StopIteration异常；



**生成器：**其实是一种特殊的迭代器，但是不需要像迭代器一样实现__iter__和__next__方法，**自动实现迭代器协议**；Python有两种不同的方式提供生成器：

​	1.**生成器函数：**常规函数定义，但是，使用yield语句而不是return语句返回结果。yield语句一次返回一个结果，在每个结果中间，挂起函数的状态，以便下次重它离开的地方继续执行

​	2.**生成器表达式：**类似于列表推导，但是，生成器返回按需产生结果的一个对象，而不是一次构建一个结果列表



**生成器的好处： **

​	1.**延迟计算：**一次返回一个结果，它不会一次生成所有的结果，这对于大数据量处理，将会非常有用；

​	2.**提高代码可读性**；



**生成器注意事项：**

​	**生成器只能遍历一次**



------



### 类实例方法 & 类方法 & 类静态方法



**采用 @classmethod 修饰的方法为类方法；**

**采用 @staticmethod 修饰的方法为类静态方法；**

**不用任何修改的方法为实例方法；**

其中 @classmethod 和 @staticmethod 都是函数装饰器



**Python类实例方法:**

在类中定义的方法默认都是实例方法，类的构造方法理论上也属于实例方法；

```python
class CLanguage:
    #类构造方法，也属于实例方法
    def __init__(self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
    # 下面定义了一个say实例方法
    def say(self):
        print("正在调用 say() 实例方法")
```

实例方法最大的特点就是，它最少也要包含一个 self 参数，用于绑定调用此方法的实例对象。实例方法通常会用类对象直接调用，例如：

```python
clang = CLanguage()
clang.say()
```



**Python类方法：**

Python 类方法和实例方法相似，它最少也要包含一个参数，只不过类方法中通常将其命名为 cls，Python 会自动将类本身绑定给 cls 参数（注意，绑定的不是类对象）。也就是说，我们在调用类方法时，无需显式为 cls 参数传参；

和实例方法最大的不同在于，类方法需要使用`＠classmethod`修饰符进行修饰（如果没有` ＠classmethod`，则 Python 解释器会将 info() 方法认定为实例方法，而不是类方法），例如：

```python
class CLanguage:
    #类构造方法，也属于实例方法
    def __init__(self):
        self.name = "C语言中文网"
        self.add = "http://c.biancheng.net"
    #下面定义了一个类方法
    @classmethod
    def info(cls):
        print("正在调用类方法",cls)
```

类方法推荐使用类名直接调用，当然也可以使用实例对象来调用：

```python
#使用类名直接调用类方法
CLanguage.info()
#使用类对象调用类方法
clang = CLanguage()
clang.info()
```



**Python类静态方法：**

静态方法，其实就是我们学过的函数，和函数唯一的区别是，静态方法定义在类命名空间中，而函数则定义在程序所在的全局命名空间中；

静态方法没有类似`self`、`cls` 这样的特殊参数，因此 Python 解释器不会对它包含的参数做任何类或对象的绑定。也正因为如此，类的静态方法中无法调用任何类属性和类方法；

静态方法需要使用`＠staticmethod`修饰，例如：

```python
class CLanguage:
    @staticmethod
    def info(name,add):
        print(name,add)
```

静态方法的调用，既可以使用类名，也可以使用类对象，例如：

```python
#使用类名直接调用静态方法
CLanguage.info("C语言中文网","http://c.biancheng.net")
#使用类对象调用静态方法
clang = CLanguage()
clang.info("Python教程","http://c.biancheng.net/python")
```



# 深度学习



### 梯度消失 & 梯度爆炸



目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要**链式法则（Chain Rule）**的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。

而链式法则是一个**连乘的形式**，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度**反向传播**的方式对深度网络权值进行更新时，得到的**梯度值接近0**或**特别大**，也就是**梯度消失**或**爆炸**。梯度消失或梯度爆炸在本质原理上其实是一样的。



**产生原因：**

```
梯度消失：1.深层网络 2.损失函数（Sigmoid）

梯度爆炸：1.深层网络 2.Weights初始化值太大
```



**解决方法：**

```
梯度归一化 / 梯度剪切 -> 让梯度值在合理范围内[1e-6, 1e3]

合理的激活函数：ReLU

残差结构（乘法改成加法，固定梯度1） / LSTM门机制

Batch Normalization

合理的参数初始化（He,Xavier）-> 让每层均值和方差保持一致

权重正则化（权重衰减）

预训练 + 微调
```



------



### **Kaiming初始化**



- **前向传播**的时候, 每一层的**卷积计算的方差为1**；
- **反向传播**的时候, 每一层的继续往**前传的梯度方差为1**(因为每层会有两个梯度的计算, 一个用来更新当前层的权重, 一个继续传播,用于前面层的梯度的计算)；

`torch.nn.init.kaiming_normal_(layer.weight,mode='fan_out', nonlinearity='relu')`



------



### 神经网络不收敛



**数据输入输出**

- 忘记进行数据进行归一化

- 没有对数据进行预处理
- 忘记检查输出



**超参数**

- 没有使用正则化
- 使用大batch size
- 使用错误学习率
- 最后一层使用错误的激活函数



**网络**

- 网络包含坏的梯度
- 网络权重没有正确的初始化
- 使用了一个太深的神经网络
- 隐藏层神经元数量设置不正确



------



### NAN & INF



**INF：**数值太大、权重初始值太大 、Learning rate太大  

**NAN：**除数为0产生 

 

**INF解决方案：**

- 激活函数
- 权重初始均值为0，方差小
- learning不断减小



------



### L1 L2正则化



**正则化**之所以能够**降低过拟合**的原因在于，**正则化是结构风险最小化的一种策略实现**



- **权重衰减通过控制L2正则**项使得模型参数不会过大，从而控制模型复杂度；
- **正则项权重**是控制模型复杂的**超参数**；
- **正则化其他方法：1. EMA of Weights  2. Label Smoothing   3. RandAugment  4. Dropout on FC **
- 给loss function加上正则化项，能使得新得到的优化目标函数**h = f(w, b)+normal(w)**，需要在f和normal中做一个权衡，如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，normal引入使得最优解向原点移动，因此可以看出**加正则项能实现参数的稀疏，让解更加简单，通过降低模型复杂度防止过拟合，提升模型的泛化能力**；

 

|          |                                               |       特点1        |         特点2          |                                         |         作用         |                                                              |
| :------: | :-------------------------------------------: | :----------------: | :--------------------: | :-------------------------------------: | :------------------: | :----------------------------------------------------------: |
| L1正则化 |   在loss function后边所加**正则项为L1范数**   | 容易得到**稀疏解** | 容易产生**稀疏的权重** | 趋向于产生少量的特征，而其他的特征都是0 | 特征选择、防止过拟合 | 对异常值更鲁棒；在0点不可导，计算不方便；没有唯一解；输出稀疏，会将不重要的特征直接置0； |
| L2正则化 | loss function后边所加**正则项为L2范数的平方** | 容易得到**平滑解** | 容易产生**分散的权重** |  会选择更多的特征，这些特征都会接近于0  |      防止过拟合      |              计算方便；对异常值敏感；有唯一解；              |



L0 范数：向量中**非0元素的个数**；

L1 范数：向量中**各个元素绝对值的和；**

L2 范数：向量中**各元素平方和**再**求平方根；**



------



### BN和它的“后浪”们



BN的缺陷

1. **依赖Batch size**  ->  GN 
2. 2**.对于RNN这样的动态网络效果不明显**，且当推理序列长度超过训练的所有序列长度时，容易出问题  -> LN
3. 当**mini-batch中的样本非独立同分布时**，性能比较差  ->  BRN



**对特征图做归一化，**对于`[B,C,W,H]`这样的训练数据而言

- **BN** 是在`[B,W,H]`维度求均值方差进行规范化 -> CNN                内部协变量偏移
- **LN** 是对`[C,W,H]`维度求均值方差进行规范化   -> RNN，Transformer
- **IN** 是对`[W,H]`维度求均值方差进行规范化  -> 图像风格化：GAN，style transfer
- **GN **先对通道进行分组，每个组内的所有[$C_i$,W,H]维度求均值方差进行规范化，与BatchSize无关  - > 在目标检测，语义分割等要求尽可能大的分辨率的任务,**但GN有两个缺陷，其中一个是在batchsize大时略低于BN，另一个是由于它是在通道上分组，因此它要求通道数是分组数g的倍数**
- **BRN **提出在训练过程中就不断学习修正整个数据集的均值和方差，使其尽可能逼近整个数据集的均值和方差，并最终用于推理阶段
- **Cross-GPU BN **例如batchsize=32，用四张卡训练，实际上只在32/4=8个样本上做归一化；
- **CBN **将前k-1个iteration的样本参与当前均值和方差的计算：但由于前k-1次iteration的数据更新，因此无法直接拿来使用，论文提出了一个处理方式是通过泰勒多项式来近似计算出前k-1次iteration的数据；
- **CmBN**

![图片](https://mmbiz.qpic.cn/mmbiz_png/V2E1ll6kaTUZVVjHxeypRx1iam1zve0dNNDnbxZyV4w0SH7nxf6nYtJibf3puP8lqAaXXtTW1uIlSBCk2dOefNog/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**对Weights做归一化**

- **WN **将权重向量w分解为一个标量g和一个向量v，标量g表示权重向量w的长度，向量v表示权重向量的方向，这种方式改善了优化问题的条件，并加速了随机梯度下降的收敛，不依赖于batch size的特点，适用于循环模型（如 LSTM）和噪声敏感应用（如深度强化学习或生成模型），而批量归一化不太适合这些应用，Weight Normalization也有个明显的**缺陷：WN不像BN有归一化特征尺度的作用**，因此WN的初始化需要慎重，为此作者提出了对向量v和标量g的初始化方法



------



### NiN & ResNet & DenseNet



**NiN:** Conv + 1x1 Conv + 1x1 Conv 



**一.ResNet和DenseNet比较**



**ResNet：稀疏连接，  Add， 训练速度快， 参数量相对较多**

**DenseNet：密集连接，Concat， 训练速度慢（concat需要频繁读取内存）， 参数量相对较少**

- **DenseNet比传统的卷积网络所需要的参数更少：密集连接带来了特征重用**，不需要重新学习冗余的特征图，而且维度拼接的操作，带来了丰富的特征信息，利用更少的卷积就能获得很多的特征图;
- **DenseNet提升了整个网络中信息和梯度的流动，对训练十分有利：**密集连接使得每一层都可以直接从损失函数和原始输入信号获得梯度，对于训练更深的网络十分有利；
- **密集连接的网络结构有正则化的效果，能够减少过拟合风险**；
- **对显存需求**



**二. ResNet解决了什么问题**



- **网络性能退化能力：**单纯的堆积网络正确率不升反降：按理说，当我们堆叠一个模型时，理所当然的会认为效果会越堆越好。因为，假设一个比较浅的网络已经可以达到不错的效果，那么即使之后堆上去的网络什么也不做，模型的效果也不会变差。然而事实上，这却是问题所在。“什么都不做”恰好是当前神经网络最难做到的东西之一； ->  恒等映射

- **有效减少梯度相关性的衰减**：即使BN过后梯度的模稳定在了正常范围内，但梯度的相关性实际上是随着层数增加持续衰减的；
    对于L层的网络来说，没有残差表示的Plain Net梯度相关性的衰减在1 / 2^L ，而ResNet的衰减却只有 1 / sqrt(L)

- **稳定梯度**：在输出引入一个输入x的恒等映射，则梯度也会对应地引入一个常数1，这样的网络的确不容易出现梯度值异常，在某种意义上，起到了**稳定梯度**的作用；

- **shortcut相加可以实现不同分辨率特征的组合：**因为浅层容易有高分辨率但是低级语义的特征，而深层的特征有高级语义，但分辨率就很低了；

- **shortcut实际上让模型自身有了更加“灵活”的结构：**即在训练过程本身，模型可以选择在每一个部分是“更多进行卷积与非线性变换”还是“更多倾向于什么都不做”，抑或是将两者结合；



**三. ResNet两种结构具体怎么实现，BottleNeck的作用**



- **两个3x3卷积和一个shortcut**  
- **两个1x1卷积中间加一个3x3卷积，然后再加一个shortcut**



​	**BottleNeck作用：降低维度，模型压缩，减少计算量**

​	卷积核的尺寸是Dk×Dk×M，一共有N个，每一个都要进行Dw×Dh次运算，所以标准卷积的计算量是：Dk x Dk x M x N x Dw x Dh

​	FLOPs：10^9 级别



**四. DenseNet和ResNet哪个比较好**

​	**在小数据集，DenseNet比ResNet要好，因为小数据集的时候容易产生过拟合，但是DenseNet能够很好的解决过拟合的问题。**DenseNet 具有非常好的抗过拟合性能，尤其适合于训练数据相对匮乏的应用。这一点从论文中 DenseNet 在不做数据增强（data augmentation）的 CIFAR 数据集上的表现就能看出来。对于 DenseNet 抗过拟合的原因有一个比较直观的解释：神经网络每一层提取的特征都相当于对输入数据的一个非线性变换，而随着深度的增加，变换的复杂度也逐渐增加（更多非线性函数的复合）。相比于一般神经网络的分类器直接依赖于网络最后一层（复杂度最高）的特征，DenseNet 可以综合利用浅层复杂度低的特征，因而更容易得到一个光滑的具有更好泛化性能的决策函数；



------



### Inception



**Inception使用split-transform-merge策略把multi-scale filter生成的不同感受野的特征融合到一起，**有利于识别不同尺度的对象；

 现在这种思想已经作为基础组件使用，和它特别像的是SPP；

- v1：1x1 3x3 5x5 不同感受野；
- v2：使用BN；
- v3：两个3x3代替一个5x5，3x3=1x3 + 3x1；
- v4：残差连接；



------



### 移动端/轻量化模型



**MobileNet：**

- v1：
  - **深度可分离卷积 + ReLU6**：3x3深度卷积， 1x1升维；


- v2：
  - 深度卷积训出来的卷积核有不少是空的 -> 在低维度ReLU使得信息丢失，所以**Inverted residuals：**1x1先升维，3x3深度卷积， 1x1降维，最后的ReLU6替换为Add；


- v3：

  - 使用NAS 
  - v2的Inverted residuals
  - SE模块
  - h-swish激活函数




**ShuffleNet：**

- v1：
  - **pointwise group convolution**（降低1x1卷积的计算量）
  - **channel shuffle**（解决不同组之间的特征图不通信）
- v2：
  - **平衡输入输出通道**(in = out) 
  - **谨慎使用组卷积** 
  - **避免网络碎片化**(一些操作可以合并) 
  - **减少元素级运算**(concat代替)



------



### 数据集划分



**验证集要和训练集来自于同一个分布（shuffle数据集然后开始划分），测试集尽可能贴近真实数据**



- 通常80%为训练集，20%为测试集
- 当**数据量较小**时（万级别）的时候将训练集、验证集以及测试集划分为**6：2：2**；若是**数据量很大**，可以将训练集、验证集、测试集比例调整为**98：1：1**
- 当数据量很小时，可以采用**K折交叉验证**
- 刚开始的时候，用训练集训练，验证集验证，确定超参数和一些细节；在验证集调到最优后，再把验证集丢进来训练，在测试集上测试
- 划分数据集时可采用随机划分法（当样本比较均衡时），分层采样法（当样本分布极度不均衡时）



------



### 卷积



#### 卷积特点

```
1.局部连接 2.权值共享 3.层次结构
```



- **每个通道之间不共享参数，希望每个通道学到不同的模式**
- 卷积**不具有旋转不变性**：要不是中心对称的像素团，旋转之后的卷积值肯定是不一样的；
- **卷积不具有平移不变性**：同一个像素团，只要卷积核对齐了，卷积值都是一样的，但是**加了padding的卷积网络平移不变性也是不存在的**，不带padding的网络每一层都必须进行严密的设计，如不带padding的UNet，通常为了网络设计简单，对训练样本做**平移增广**是很有必要的；
- 如果边长减去1后不能被stride整除，**卷积的降采样过程会丢弃边缘的像素**，**特征图像素与输入图像位置映射会产生偏移**，目前所有的深度学习框架都没有考虑这里的映射错位关系，训练时输入样本图像的大小和检测时切块的大小只能用最终特征图的尺寸反推回去，保证在卷积过程中不丢弃边缘；



------



#### 卷积类型



- **3x3卷积**

   - 底层专门做过优化，已经成为主流组件；



- **1x1卷积**
  - 升维降维；
  - 减少参数；
  - 通道融合；
  - 增加非线性（利用后接的非线性激活函数如ReLU）；



- **空洞卷积**

  - kernal之间增加空洞，增加感受野，图森组针对空洞卷组专门做过研究：[1, 3, 5, 1, 3, 5]这样的空洞率；
  - 虽然增大了感受野，但是使得特征更加稀疏；
  - **解决了网格效应**；



- **转置卷积**

  **会出现棋盘效应：**由于转置卷积的“不均匀重叠”  -> **1.采取可以被步长整除的卷积核长度  2.插值**；

  对于同一个卷积核（因非其稀疏矩阵不是正交矩阵），结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状，**上采样常用双线性插值；**

  ![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpDiaSQ8lkIWnNEAtPYtPBXKm0Txqvm0BamZB6bTvAqibFMlgeSyHwJakB8M4fia1ibh4ekhwKKkkdx0w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  ![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpDiaSQ8lkIWnNEAtPYtPBXKkJYoWEF0iaPRTFbg33KhticTOwwTHhhEAmN5ZGyAsiayGNMWxPRQtPbGg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  

- **深度可分离卷积**

  **计算成本仅仅是2D卷积的12%左右**：对于规模较小的模型，如果将普通卷积替换为深度可分卷积，其模型大小可能会显著降低，模型的能力可能会变得不太理想，因此得到的模型可能是次优的；但如果使用得当，深度可分卷积能在不牺牲模型性能的前提下显著提高效率；

  

  **MobileNet v1中提出：深度卷积 + 1x1卷积**



**参数量**：卷积核的尺寸是Dk×Dk×M，一共有N个

```
**标准卷积**的参数量是：Dk x Dk x M x N
**深度卷积**的卷积核尺寸Dk×Dk×M；逐点卷积的卷积核尺寸为1×1×M，一共有N个，所以深度可分离卷积的参数量是：Dk x Dk x M + M x N
```

**计算量：**普通卷积核的尺寸是Dk×Dk×M，一共有N个，每一个都要进行Dw×Dh次运算

```
**标准卷积**的计算量是：Dk x Dk x M x N x Dw x Dh
**深度卷积**的卷积核尺寸Dk×Dk×M，一共要做Dw×Dh次乘加运算；逐点卷积的卷积核尺寸为1×1×M，有N个，一共要做Dw×Dh次乘加运算，所以深度可分离卷积的计算量是：Dk x Dk x M x Dw x Dh +  M x N x Dw x Dh
```



**参数量和运算量均下降为原来的：**$\frac{1}{N}+\frac{1}{D_k^2}$



------



#### 卷积 & 互相关



**卷积**是透过两个函数$f$和$g$生成第三个函数的一种数学算子，表征函数$f$与经过翻转和平移的$g$的乘积函数所围成曲边梯形的面积；

**互相关**是两个函数之间的滑动点积或滑动内积，**互相关中的过滤不经过反转**，而是直接滑过函数$f$，$f$与$g$之间的交叉区域即是互相关；

**严格意义上来说，深度学习中的“卷积”是互相关(Cross-correlation)运算，本质上执行逐元素乘法和加法**。但在之所以习惯性上将其称为卷积，是因为过滤器的权值是在训练过程中学习得到的；



------



### 感受野



感受野大小计算公式：
$$
r_{l}=r_{l-1}+\left(k_{l}-1\right) * \prod_{i=0}^{l-1} s_{i}
$$
其中 $r_{l-1}$ 为第 $l-1$ 层的感受野大小, $k_{l}$ 为第l层的卷积核大小 $($ 也可以是Pooling $), s_{i}$ 为第 $i$ 层的卷 积步长。一般来说 $r_{0}=1, s_{0}=1$ ；



**CNN中感受野定义及性质**

```
在深度神经网络中，每个神经元节点都对应着输入图像的某个确定区域，仅该区域的图像内容能对相应神经元的激活产生影响，那么这个区域称为该神经元的感受野；

- 越靠近感受野中心的区域越重要
- 各向同性
- 由中心向周围的重要性衰减速度可以通过网络结构控制
```



感受野是直接或者间接参与计算特征图像素值的输入图像像素的范围，直接感受野就是卷积核大小，随着卷积层数的加深之前层次的感受野会叠加进去。**感受野小了缺乏环境信息，感受野大了引入太多环境干扰**，所以**一个网络能够检测的目标框范围与特征图像素或者特征向量的感受野有关**，通常能够检测的目标框边长范围是感受野边长的0.1-0.5倍；拿到了一个网络**要做感受野分析，然后确定它能够检测多少像素的目标**。实际目标检测任务需要综合网络结构设计和图像分辨率选择。如果目标框的像素范围超过了网络的感受野，就需要将原始图像缩小后再检测；



------



### Padding

- **保持边界信息**：如果没有加padding的话，输入图片最边缘的像素点信息只会被卷积核操作一次，但是图像中间的像素点会被扫描到很多遍，那么就会在一定程度上降低边界信息的参考程度，但是在加入padding之后，在实际处理过程中就会从新的边界进行操作，就从一定程度上解决了这个问题；
- 可以利用padding对输入尺寸有差异图片进行补齐，**使得输入图片尺寸一致**；
- 在卷积神经网络的卷积层加入Padding，可以使得**卷积层的输入维度和输出维度一致**；



------



### Pooling



**作用：**

```
1. 抑制噪声，降低信息冗余
2. 提升模型的尺度不变性、旋转不变性
3. 降低模型计算量
4. 防止过拟合
```

**缺点：**造成梯度稀疏、丢失信息



**最大池化作用：保留主要特征，突出前景**

**平均池化作用：保留背景信息，突出背景**

------



**Mean Pooling：**

 在forward的时候，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值；**backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了；**（假设pooling的窗大小是2x2）

```
forward: [1 3; 2 2] -> [2]
backward: [2] -> [0.5 0.5; 0.5 0.5]
```



平均池化取每个块的平均值，提取特征图中所有特征的信息进入下一层。因此**当特征中所有信息都比较有用时，使用平均池化。如网络最后几层，最常见的是进入分类部分的全连接层前，常常都使用平均池化。这是因为最后几层都包含了比较丰富的语义信息，使用最大池化会丢失很多重要信息；**



**Max Pooling：**

在forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，**backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0；**

```
forward: [1 3; 2 2] -> [3]
backward: [3] -> [0 3; 0 0]
```



最大池化的操作，取每个块中的最大值，而其他元素将不会进入下一层。CNN卷积核可以理解为在提取特征，对于最大池化取最大值，可以理解为提取特征图中响应最强烈的部分进入下一层，而其他特征进入待定状态；

一般而言，前景的亮度会高于背景，因此，最大池化具有提取主要特征、突出前景的作用。但在个别场合，前景暗于背景时，最大池化就不具备突出前景的作用了；

**当特征中只有部分信息比较有用时，使用最大池化。如网络前面的层，图像存在噪声和很多无用的背景信息，常使用最大池化；**



- [x] https://mp.weixin.qq.com/s/2mEhUuHOeT4Y4NZZ3NlktQ



------



### 过采样 & 欠采样



原始数据大小为![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B1831%5Ctimes21%7D)，1831条数据，每条数据有21个特征：其中正例176个（9.6122%），反例1655个（90.3878%），类别不平衡;



**欠采样：**从反例中随机选择176个数据，与正例合并（ ![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B352%5Ctimes21%7D) ）

**过采样：**从正例中反复抽取并生成1655个数据（势必会重复），并与反例合并（ ![[公式]](https://www.zhihu.com/equation?tex=%5CRe%5E%7B3310%5Ctimes21%7D) ）



- 采样方法一般比直接调整阈值的效果要好；
- 使用采样方法（过采样和欠采样）一般可以提升模型的泛化能力，但有一定的过拟合的风险，应搭配使用正则化模型；
- 过采样的结果较为稳定，过采样大部分时候比欠采样的效果好；




**过采样**带来**更大的运算开销**，当数据中噪音过大时，结果反而可能会更差因为**噪音也被重复使用**；

尝试**半监督学习**的方法；注意积累样本；数据增强；欠采样的时候可以训练多个模型，最后尝试模型投票的方法；



- [x]  https://www.zhihu.com/question/269698662/answer/352279936



------



### 过拟合 & 欠拟合



**过拟合**指的是在训练集error越来越低，但是在验证集和测试集error越来越高，模型拟合了训练样本中的噪声，导致泛化能力差；

```
数据增强
缩减模型表达能力
正则化（Weight Decay， L1，L2）
Early Stopping
Dropout / BN
```

**欠拟合**指的是训练集提取特征较少，导致模型不能很好拟合训练集；

```
增加模型复杂度  eg.ResNet-50 -> resNet-101；
减少正则化
错误分析：（训练集和测试集的分布偏差）测试时候出现问题进行分析，训练集缺少哪些情况导致错误，后续将在训练集中加入此类数据纠正偏差；
加入更多特征
```



### 优化学习方法

------



**一阶方法：**随机梯度下降（SGD）、动量（Momentum）、牛顿动量法（Nesterov动量）、AdaGrad（自适应梯度）、RMSProp（均方差传播）、Adam、Nadam

**二阶方法：**牛顿法、拟牛顿法、共轭梯度法（CG）、BFGS、L-BFGS

**自适应优化算法：**Adagrad（累积梯度平方）、RMSProp（累积梯度平方的滑动平均）、Adam（带动量的RMSProp，即同时使用梯度的一、二阶矩）

**梯度下降陷入局部最优有什么解决办法？** 可以用BGD、SGD、MBGD、momentum，RMSprop，Adam等方法来避免陷入局部最优；



- 二阶导是向量，学术上比较好，但难算，一阶实用；
-  只关注收敛在哪个地方，SGD一步一步来，二阶收敛效果不一定比一阶收敛效果好；



------

**马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）**

- **基于动量**的方法使得最优化过程看起来像是一个球滚下山的样子
- **SGD**很难突破对称性，一直卡在顶部
- **RMSProp之类**的方法能够看到马鞍方向有很低的梯度（因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进）

------

**梯度下降与拟牛顿法的异同**

- **参数更新模式相同**
- **梯度下降法利用误差的梯度**来更新参数，**拟牛顿法利用海塞矩阵**的近似来更新参数
- **梯度下降**是泰**勒级数的一阶展开**，而**拟牛顿法是泰勒级数的二阶展开**
- **SGD能保证收敛**，但是L-BFGS在非凸时不收敛

------



**一个框架来梳理所有的优化算法**



首先定义：待优化参数： $w$, 目标函数： $f(w)$, 初始学习率 $\alpha_{\circ}$
而后，开始进行迭代优化，在每个epoch $\boldsymbol{t}$ :

1. 计算目标函数关于当前参数的梯度： $g_{t}=\nabla f\left(w_{t}\right)$
2. 根据历史梯度计算一阶动量和二阶动量:
$m_{t}=\phi\left(g_{1}, g_{2}, \cdots, g_{t}\right) ; V_{t}=\psi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$
3. 计算当前时刻的下降梯度： $\eta_{t}=\alpha \cdot m_{t} / \sqrt{V_{t}}$
4. 根据下降梯度进行更新： $w_{t+1}=w_{t}-\eta_{t}$



------



**SGD（普通更新）**



最简单的沿着负梯度方向改变参数；假设有一个**参数向量x**及其**梯度dx**，那么最简单的更新的形式是：

```python
# 普通更新
x += - learning_rate * dx
```



------



**SGDM（动量更新）**



该方法从**物理角度**上对于最优化问题得到的启发：

从本质上说，动量法，就像我们从山上推下一个球，球在滚下来的过程中累积动量，变得越来越快（直到达到终极速度，如果有空气阻力的存在，则mu<1）；同样的事情也发生在参数的更新过程中：**对于在梯度点处具有相同的方向的维度，其动量项增大，对于在梯度点处改变方向的维度，其动量项减小。**因此，我们可以得到更快的收敛速度，同时可以减少摇摆



在SGD中，梯度影响**位置**；

而在这个版本的更新中，物理观点建议**梯度只是影响速度**，然后**速度再影响位置**：

```python
 # 动量更新
    v = mu * v - learning_rate * dx # 与速度融合，mu其物理意义与摩擦系数更一致
    x += v # 与位置融合
```

也可以理解为： 

```python
 # 动量更新
    v = mu * v - (1 - mu) * dx # 与速度融合，mu其物理意义与摩擦系数更一致
    x += v # 与位置融合
```

mu通常取值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向



------



**NAG（Nesterov动量）**



SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。



当参数向量位于某个位置 *x* 时，观察上面的动量更新公式，动量部分会通过啊$mu * v$改变参数向量；

因此，如要计算梯度，那么可以将**未来的近似位置**$ x+mu*v$ 看做是“**向前看**”，这个点在我们一会儿要停止的位置附近。因此，**计算** $ x+mu*v$**的梯度**而不是“旧”位置 *x* 的梯度，使用Nesterov动量，我们就在这个“向前看”的地方计算梯度

```python
x_ahead = x + mu * v
计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)
v = mu * v - learning_rate * dx_ahead
x += v  
```

上面的程序还得计算dx_ahead，通过对 x_ahead = x + mu * v 使用变量变换进行改写，然后用x_ahead而不是x来表示上面的更新，即：实际**存储**的参数向量总是**向前一步版本**。x_ahead 的公式（将其**重新命名为x**）就变成了：

```python
v_prev = v # 存储备份
v = mu * v - learning_rate * dx # 速度更新保持不变
x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
```

mu=0.9

------



**RMSprop**

引用自Geoff Hinton的Coursera课程，具体说来，就是它使用了一个**梯度平方的滑动平均**：

```python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

decay_rate=0.9，learning_rate=0.001，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，但**其更新不会让学习率单调变小**；（不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度，而指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：）



------



**Adam**



**Adam本质上实际是RMSProp+动量**：Adam对每一个参数都计算自适应的学习率。除了像RMSprop一样存储一个历史梯度平方的滑动平均$vt$，Adam同时还保存一个历史梯度的滑动平均$mt$，类似于动量：

```python
# 根据历史梯度计算一阶动量和二阶动量
m_t = beta1*m + (1-beta1)*dx
v_t = beta2*v + (1-beta2)*(dx**2)

# 当mt和vt初始化为0向量时，发现它们都偏向于0，尤其是在初始化的步骤和当衰减率很小的时候（例如beta1和beta2趋向于1）,通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差
m_hat = m_t / 1 - (beta1 ** t）
v_hat = v_t / 1 - (beta2 ** t)

x += - learning_rate * m_hat / (np.sqrt(v_hat) + eps)
```

eps=1e-8, beta1=0.9, beta2=0.999



- [x] https://blog.csdn.net/google19890102/article/details/69942970


- [x] https://www.zhihu.com/question/323747423/answer/790457991




------



### 激活函数



- **Sigmoid**

  更倾向于更新靠近输出层的参数，而不是靠近输入层的参数

  导数为$f(x) * (1 - (f(x)))$

  导数取值范围【0，0.25】

  左右两侧都是**近似饱和区**，导数太小，容易造成梯度消失

  涉及指数运算，容易溢出

  输出值不以零为中心，会导致模型收敛速度慢

  激活函数的**偏移现象**

  

- **ReLU**

  Dead ReLU：当 $x <0$ 时，ReLU 输出恒为零。反向传播时，梯度恒为零，参数永远不会更新；

  激活部分神经元，增加稀疏性；

  计算简单，收敛速度快；



-  **ReLu6**  

  1. ReLU6比ReLU能更早学习到稀疏特征 
  2. 增强浮点数的小数位表达能力（整数位最大是6，所以只占3个bit，其他bit全部用来表达小数位）

  

- **Leaky ReLU**

  $LeakyReLU(x) = max(0.01x, x)$

  解决ReLU Dead；

------



- **Swish**

  $f(x) = x * sogmoid(\beta x)$

  可以看做是**介于线性函数与ReLU函数之间的平滑函数。**β是个常数或可训练的参数，Swish 具备**无上界有下界、平滑、非单调**的特性;



- **Hard-Swish**（[Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)） YOLO v5使用后会有10%的推理速度损失；
  $$
  \mathrm{h}-\mathrm{swish}(x)=x \frac{\operatorname{ReLU} 6(x+3)}{6}
  $$


- **SiLU**
  $$
  f(x)=x \cdot \sigma(x) \quad 
  $$



- **GELU** 

  与 Swish 激活函数$ x * sogmoid(\beta x)$的函数形式和性质非常相像，一个是固定系数 1.702，另一个是可变系数 β

  

------



- **Mish**

  Mish是一个光滑非单调的激活函数，在Backbone使用后内存会增大：$f(x)=x \cdot \tanh (\ln \left(1+e^{x}\right))$



------



### 损失函数

- Loss function，即**损失函数**：用于定义单个训练样本与真实值之间的误差；
- Cost function，即代价函数：用于定义单个批次/整个训练集样本与真实值之间的误差；、
- Objective function，即**目标函数**：泛指任意可以被优化的函数；

损失函数是用于衡量模型所作出的预测离真实值（**Ground Truth**）之间的偏离程度。通常，我们都会**最小化目标函数**，最常用的算法便是“**梯度下降法**”。俗话说，任何事情必然有它的两面性，因此，并**没有一种万能的损失函数能够适用于所有的机器学习任务**，所以在这里我们需要知道每一种损失函数的优点和局限性，才能更好的利用它们去解决实际的问题。损失函数大致可分为两种：回归损失（针对**连续型**变量）和分类损失（针对**离散型**变量）



#### 熵 & KL散度

##############################################################################################################

总的来说：**一个事件的不确定性就越大，其信息量越大，它的熵值就越高；相反，如果一个时间的不确定性越小，其信息量越小，它的熵值就越低；**



**熵定义为：信息的数学期望**，表示成**真实概率分布p**的函数和**预测概率分布q**的函数
$$
H=-\sum_{i=0}^{n} p\left(x_{i}\right) \log _{2} q\left(x_{i}\right)
$$


**KL散度**也叫做**相对熵**，用于**度量两个概率分布之间的差异程度**
$$
D(p \| q)=H(p, q)-H(p)=\sum_{x} P(x) \log \frac{p(x)}{q(x)}
$$

##############################################################################################################

#### 分类Loss



**BCE Loss:**



$-\frac{1}{n} \sum\left(y_{n} \times \ln x_{n}+\left(1-y_{n}\right) \times \ln \left(1-x_{n}\right)\right)$



$\mathrm{CE}(p, y)=\left\{\begin{array}{ll}-\log (p) & \text { if } y=1 \\ -\log (1-p) & \text { otherwise }\end{array}\right.$



其中![[公式]](https://www.zhihu.com/equation?tex=y%5Cin%5C%7B-1%2C+1%5C%7D)为真实标签，1表示为正例，-1表示为负例；而![[公式]](https://www.zhihu.com/equation?tex=p%5Cin%5B0%2C+1%5D)为模型预测为正例的概率值；





**BCEWithLogitsLoss = Sigmoid + BCELoss**

**torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss**



**Focal Loss**

与抽样方法不同，Focal Loss从另外的视角来解决样本不平衡问题，那就是**根据置信度动态调整交叉熵loss**，当预测正确的置信度增加时，loss的权重系数会逐渐衰减至0，这样模型训练的loss更关注难例，而大量容易的例子其loss贡献很低

解决了one-stage算法中**正负样本的比例失衡**：在CE基础上增加了一个调节因子$(1-p_t)^{\gamma}$
$$
F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log p_{t}
$$
${\gamma=2}$最好，FL相比CE可以大大降低简单例子的loss，使模型训练更关注于难例；



**Tversky loss**
$$
T(A, B)=\frac{|A \cap B|}{|A \cap B|+\alpha|A-B|+\beta|B-A|}
$$
它是结合了Dice系数（F1-score）以及Jaccard系数（IoU）的一种广义形式，如：

- 当 α = β = 0.5时，此时Tversky loss便退化为Dice系数（分子分母同乘于2）
- 当 α = β = 1时，此时Tversky loss便退化为Jaccard系数（交并比）

因此，我们只需控制 α 和 β 便可以控制**假阴性**和**假阳性**之间的平衡，比如在医学领域我们要检测肿瘤时，更多时候我们是希望Recall值更高。因此，我们可以通过增大 β 的取值，来提高网络对肿瘤检测的灵敏度。其中，α + β 的取值我们一般会令其1；



------



#### 回归Loss



Smooth L1 Loss![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) IoU Loss![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) GIoU Loss ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) DIoU Loss ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) CIoU Loss

------



**一、Smooth L1 Loss**
$$
\text { smooth }_{L_{1}}(x)=\left\{\begin{array}{cc}
0.5 x^{2} & i f|x|<1 \\
|x|-0.5 & \text { otherswise }
\end{array}\right.
$$

从损失函数对x的导数可知：

 ![[公式]](https://www.zhihu.com/equation?tex=L_%7B1%7D)损失函数对x的导数为常数，在训练后期，x很小时，如果学习率不变，损失函数会在稳定值附近波动，很难收敛到更高的精度；

 ![[公式]](https://www.zhihu.com/equation?tex=L_%7B2%7D) 损失函数对x的导数在x值很大时，其导数也非常大，在训练初期不稳定；

![[公式]](https://www.zhihu.com/equation?tex=smooth_%7BL_%7B1%7D%7D%5Cleft%28+x+%5Cright%29+) **完美的避开了** ![[公式]](https://www.zhihu.com/equation?tex=L_%7B1%7D)**和** ![[公式]](https://www.zhihu.com/equation?tex=L_%7B2%7D)**损失的缺点**



上面的三种Loss用于计算目标检测的Bounding Box Loss时，独立的求出4个点的Loss，然后进行相加得到最终的Bounding Box Loss，这种做法的假设是4个点是相互独立的，实际是有一定相关性的；



------



**二、IoU Loss**

![img](https://pic2.zhimg.com/80/v2-090938dacc24098c4e54aa18968f375d_1440w.jpg)



------



**三、GIoU Loss**



- **当预测框和目标框不相交时**，IoU(A,B)=0，不能反映A,B距离的远近，此时损失函数不可导，IoU Loss 无法优化两个框不相交的情况；
- 假设预测框和目标框的大小都确定，只要两个框的相交值是确定的，**其IoU值是相同时，IoU值不能反映两个框是如何相交的**；




$$
G I o U=I o U - \frac{|C -(A \cup B)|}{|C|}
$$

- GIoU具有尺度不变性；
- 当 ![[公式]](https://www.zhihu.com/equation?tex=A%5Crightarrow+B) 时，两者相同都等于1，此时 ![[公式]](https://www.zhihu.com/equation?tex=GIoU) 等于1；当 ![[公式]](https://www.zhihu.com/equation?tex=A%E5%92%8CB) 不相交时， ![[公式]](https://www.zhihu.com/equation?tex=GIoU%5Cleft%28+A%2CB+%5Cright%29+%3D+-1)



------



**四、DIoU Loss**



![img](https://pic2.zhimg.com/80/v2-d32d8fd6e32ecca603ea9678695b7241_1440w.jpg)



- **当目标框完全包裹预测框的时候，IoU和GIoU的值都一样，此时GIoU退化为IoU, 无法区分其相对位置关系；**
- GIoU损失一般会增加预测框的大小使其能和目标框重叠，而DIoU损失则直接使目标框和预测框之间的中心点归一化距离最小，即让预测框的中心快速的向目标中心收敛；



好的目标框回归损失应该考虑三个重要的几何因素：**重叠面积，中心点距离，长宽比；**

**DIoU Loss**，相对于GIoU Loss**收敛速度更快**，该Loss考虑了**重叠面积和中心点距离**，但**没有考虑到长宽比**；

**CIoU Loss**，**其收敛的精度更高**，以上**三个因素都考虑到了**；

$$
L_{D I o U}=1-I o U+\frac{\rho^{2}\left(b, b^{g t}\right)}{c^{2}}
$$
其中 ![[公式]](https://www.zhihu.com/equation?tex=b%E5%92%8Cb%5E%7Bgt%7D) 分别表示 ![[公式]](https://www.zhihu.com/equation?tex=B%E5%92%8CB%5E%7Bgt%7D) 的中心点， ![[公式]](https://www.zhihu.com/equation?tex=%5Crho%5Cleft%28+%5Ccdot+%5Cright%29) 表示欧式距离， ![[公式]](https://www.zhihu.com/equation?tex=c) 表示 ![[公式]](https://www.zhihu.com/equation?tex=B%E5%92%8CB%5E%7Bgt%7D) 的最小外界矩形的对角线距离；



- 尺度不变性；
- 当两个框完全重合时， ![[公式]](https://www.zhihu.com/equation?tex=L_%7BIoU%7D%3DL_%7BGIoU%7D%3DL_%7BDIoU%7D%3D0) ,当2个框不相交时![[公式]](https://www.zhihu.com/equation?tex=L_%7BGIoU%7D%3DL_%7BDIoU%7D%5Crightarrow+2)；
- DIoU Loss可以直接优化2个框直接的距离，比GIoU Loss收敛速度更快；



------


**五、CIoU Loss**



CIoU的惩罚项是在DIoU的惩罚项基础上加了一个影响因子 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha%5Cupsilon) ，这个因子把**预测框长宽比拟合目标框的长宽比**考虑进去；

![[公式]](https://www.zhihu.com/equation?tex=L_%7BCIoU%7D+%3D+1-+IoU+%2B%5Cfrac%7B%5Crho%5E%7B2%7D%5Cleft%28+b%2Cb%5E%7Bgt%7D+%5Cright%29%7D%7Bc%5E%7B2%7D%7D+++%2B+%5Calpha%5Cupsilon)



------



### 分类和回归



|      |          |                           常见模型                           |
| :--: | :------: | :----------------------------------------------------------: |
| 分类 | 离散变量 | 感知机、朴素贝叶斯、**逻辑回归(LR)**、支持向量机(SVM)、神经网络 |
| 回归 | 连续变量 | 线性回归、多项式回归、岭回归（L2正则化）、Lasso回归（L1正则化） |
|      |          |                                                              |



**线性回归 & 逻辑回归（LR）**

|              |                                                              |             |                    区别                    |
| :----------: | :----------------------------------------------------------: | ----------- | :----------------------------------------: |
| **线性回归** | 利用数理统计中回归分析，来确定**两种或两种以上变量间相互依赖的定量关系**的一种统计分析方法 |             |   用于回归，使用的是**均方误差损失函数**   |
| **逻辑回归** | 用于**处理因变量为分类变量的回归问题**，常见的是**二分类或二项分布问题**，也可以处理**多分类问题**，它实际上是**属于一种分类方法** | sigmoid函数 | 用于**二分类**，使用的是**交叉熵损失函数** |
|              |                                                              |             |                                            |



------



### 分类Loss采用CE而不是MSE



$z(x)=w * x+b, \quad a(z)=\sigma(z)=\frac{1}{1+e^{-z}}$

我们要学习的函数 $a(x)=\sigma(w * x+b)$ ，目标为使`a(x)`与`label y`越逼近越好



 MSE  Loss
$$
L_{m s e}=\frac{1}{2}(a-y)^{2}
$$
CE Loss
$$
L_{c e e}=-(y * \ln (a)+(1-y) * \ln (1-a))
$$



**两个Loss function对w的导数，也就是SGD梯度下降时，w的梯度**

**MSE：**

$\frac{\partial L_{m s e}}{\partial w}=\frac{\partial L}{\partial a} * \frac{\partial a}{\partial z} * \frac{\partial z}{\partial w}=(a-y) * \sigma^{\prime}(z) * x$

**CE：**

$\frac{\partial L_{c e e}}{\partial w}=\left(-\frac{y}{a}+\frac{1-y}{1-a}\right) * \sigma^{\prime}(z) * x$
由于 $\sigma^{\prime}(z)=\sigma(z) *(1-\sigma(z))=a *(1-a)$, 则:
$\frac{\partial L_{c e e}}{\partial w}=(a y-y+a-a y) * x=(a-y) * x$

sigmoid函数 $\sigma(z)$ 的导数sigmoid $\sigma^{\prime}(z)$ 在输出接近 0 和 1 的时候是非常小的，故导致在使用MSE Loss时，模型参数w会学习的非常慢；而使用CE Loss则没有这 个问题，为了更快的学习速度，分类问题一般采用交叉商损失函数；



------



### 多类别loss & 多标签分类



多个sigmoid与一个softmax都可以进行多分类.如果多个类别之间是互斥的，就应该使用softmax，即这个东西只可能是几个类别中的一种。如果多个类别之间不是互斥的，使用多个sigmoid；



| 分类问题名称   | 输出层使用激活函数 | 对应的损失函数                                       |
| -------------- | ------------------ | ---------------------------------------------------- |
| **二分类**     | **sigmoid函数**    | **二分类交叉熵损失函数（binary_crossentropy）**      |
| **多分类**     | **Softmax函数**    | **多类别交叉熵损失函数（categorical_crossentropy）** |
| **多标签分类** | **sigmoid函数**    | **二分类交叉熵损失函数（binary_crossentropy）**      |



**多标签问题与二分类问题关系：**

- **历史原因：**类别之间不严格互斥，softmax -> BCE ,YOLOv3 中person和man；
- **方法：**把一个多标签问题，转化为了在每个标签上的二分类问题；



------



### 提高模型速度



- TensorRT、int8
- amp 混合精度 + float16
- 将训练时候的3x3conv， 1x1conv， Identity在推理时融合为一个3x3conv



------



### 降低网络复杂度但不影响精度



- 模型压缩：通道剪枝 / 权重剪枝

- 蒸馏

- 重新设计卷积代替普通卷积：深度可分离卷积，RepVGG




------



### Attention

减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度，让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息，从而提高输出的质量，帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系；



传统：梯度、亮度、饱和度

CNN：SENet（GAP + 2个FC + Sigmoid）、SKNet、CBAM

Transformer：Attention 、 Self-Attention、Multi-head Self-Attention



**Attention有什么缺点**

Attention模块的参数都是通过label和预测值的loss反向传播进行更新，没有引入其他监督信息，因而其**受到的监督有局限，容易对label过拟合**



------



# 图像处理



## 图像滤波

**滤波**：在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制

**平滑**：也称模糊, 是一项简单且使用频率很高的图像处理方法。平滑的一种作用就是用来减弱噪声



### 常见的线性滤波器

低通滤波器 允许低频通过

高通滤波器 允许高频通过

带通滤波器 允许一定范围的频率通过

带阻滤波器 允许一定范围的频率通过并阻止其他的频率通过

全通滤波器 允许所有频率通过，只改变相位

陷波滤波器 阻止一个狭窄频率范围通过



### 线性滤波与非线性滤波

**线性滤波**：方框滤波 均值滤波 高斯滤波  **非线性滤波**：中值滤波 双边滤波



①**方框滤波**：每一个输出像素的是内核邻域像素值的平均值得到

②**均值滤波**：用均值代替原图像中的各个像素值，把每个像素都用周围的8个像素来做均值操作         **缺点**：不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变模糊，不能很好去除噪声点（椒盐噪声）

③**高斯滤波**：是一种线性平滑滤波，适用于消除高斯噪声，广泛应用于图像处理的减噪过程，高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到，对于**抑制服从正态分布的噪声**非常有效



④**中值滤波**：中值滤波是一种典型的非线性滤波技术，基本思想是用像素点邻域灰度值的中值来代替该像素点的灰度值，该方法在**去除脉冲噪声、椒盐噪声**的同时又能保留图像边缘细节

⑤**双边滤波**：是结合图像的空间邻近度和像素值相似度的一种折衷处理，同时考虑空域信息和灰度相似性，达到保边去噪的目的        **缺点：**是由于双边滤波保证了边缘信息，所以**其保存了过多的高频信息**，对于彩色图像里的高频噪声，双边滤波器不能够干净地滤去，**只能对于低频信息进行较好地滤波**

⑥**维纳滤波：**是一种自适应最小均方差滤波器。维纳滤波的方法是一种统计方法，它用的最优准则是基于图像和噪声各自相关的相关矩阵，它能根据图像的局部方差调整滤波器的输出，局部方差越大，滤波器的平滑作用就越强



## 边缘检测

边缘检测的目的就是找到图像中亮度变化剧烈的像素点构成的集合，表现出来往往是轮廓。如果图像中边缘能够精确的测量和定位，那么，就意味着实际的物体能够被定位和测量，包括物体的面积、物体的直径、物体的形状等



**边缘检测算子**有哪些：

一阶： Sobel算子, Canny算子

二阶：Laplacian算子



### **canny边缘检测**

**首先，图像降噪**。我们知道梯度算子可以用于增强图像，本质上是通过增强边缘轮廓来实现的，也就是说是可以检测到边缘的。但是，它们受噪声的影响都很大。那么，我们第一步就是想到要先去除噪声，因为噪声就是灰度变化很大的地方，所以容易被识别为伪边缘。

**第二步，计算图像梯度，得到可能边缘**。

**第三步，非极大值抑制**。通常灰度变化的地方都比较集中，将局部范围内的梯度方向上，灰度变化最大的保留下来，其它的不保留，这样可以剔除掉一大部分的点。将有多个像素宽的边缘变成一个单像素宽的边缘。

**第四步，双阈值筛选**。通过非极大值抑制后，仍然有很多的可能边缘点，进一步的设置一个双阈值，即低阈值（low），高阈值（high）。灰度变化大于high的，设置为强边缘像素，低于low的，剔除。在low和high之间的设置为弱边缘。进一步判断，如果其领域内有强边缘像素，保留，如果没有，剔除。

这样做的目的是只保留强边缘轮廓的话，有些边缘可能不闭合，需要从满足low和high之间的点进行补充，使得边缘尽可能的闭合。



**canny算子是怎么做的？简述Canny算子的计算步骤**

①将彩色图像转化为灰度图；②使用高斯滤波器平滑图像；③计算图像梯度的幅值和方向；④对梯度幅值进行非极大值抑制；⑤使用双阈值进行边缘的检测和连接；



### **sobel算子**

Sobel算子是一个主要用作边缘检测的**离散微分算子**，Sobel算子**结合了高斯平滑和微分求导**，用来计算图像灰度函数的近似梯度。在图像的任何一点使用此算子，将会产生对应的梯度矢量或是其法矢量。

当内核大小为 3 时, 我们的Sobel内核可能产生比较明显的误差(毕竟，Sobel算子只是求取了导数的近似值而已)。为解决这一问题，OpenCV提供了Scharr 函数，但该函数仅作用于大小为3的内核。该函数的运算与Sobel函数一样快，但结果却更加精确。



### 传统边缘检测的步骤

①滤波：滤波去除噪声；②增强：增强边缘的特征；③将边缘通过某种方式提取出来，完成边缘检测。



### 如何求边缘

Sobel算子实现水平边缘检测、垂直边缘检测；45度、135度角边缘检测



### SIFT

尺度不变特征变换是计算机视觉中一种**检测、描述和匹配图像局部特征点**的方法，通过在**不同的尺度空间**中**检测极值点或特征点**，**提取出其位置、尺度和旋转不变量，并生成特征描述子，最后用于图像的特征点匹配**  		4x4x8=**128维度**

- 构建高斯金字塔，差分高斯金字塔(尺度不变)；
- 获取极值点，认为其为潜在的特征点；
- 删除部分边缘以及不稳定的点；
- 确定关键点主方向(旋转不变)；
- 生成描述符；128维（8 * （4 * 4区域））



**SIFT特征是如何保持旋转不变性的？**

SIFT特征通过将坐标轴旋转至关键点的主方向来保持旋转不变性，关键点的主方向是通过统计关键点局部邻域内像素梯度的方向分布直方图的最大值得到的



**SIFT特征匹配**

对两幅图像中检测到的特征点，可采用**特征向量的欧式距离**作为特征点相似性的度量，取图像1中某个关键点，并在图像2中找到与其距离最近的两个关键点，若最近距离与次近距离的比值小于某个阈值，则认为距离最近的这一对关键点为匹配点。降低比例阈值，SIFT匹配点数量会减少，但相对而言会更加稳定。阈值ratio的取值范围一般为0.4~0.6



**SIFT特征的特点**

SIFT是一种检测、描述、匹配图像局部特征点的算法，通过在尺度空间中检测极值点，提取位置、尺度、旋转不变量，并抽象成特征向量加以描述，最后用于图像特征点的匹配。SIFT特征对**灰度、对比度变换、旋转、尺度缩放**等保持不变性，对**视角变化、仿射变化、噪声**也具有一定的鲁棒性。但其**实时性不高**，**对边缘光滑的目标无法准确提取特征点**



### SURF

加速鲁棒特征(Speed Up Robust Feature, SURF)和SIFT特征类似，同样是一个用于检测、描述、匹配图像局部特征点的特征描述子。**SIFT很难达到实时的要求**，SURF相当于SIFT的加速改进版本，**将Hessian矩阵的高斯二阶微分模板进行了简化，借助于积分图，使得模板对图像的滤波只需要进行几次简单的加减法运算，并且这种运算与滤波模板的尺寸无关。**

- 积分图像和盒子滤波器结合，取代了高斯金字塔；（从而构建了Hessian矩阵元素值，进而缩短了特征提取的时间）
- 使用近似Harr小波计算特征点描述子，维度降低为64维；（基于Hessian行列式）



### SIFT和SURF的异同



**①尺度空间**：SIFT使用DoG金字塔与图像进行卷积操作，而且对图像有做降采样处理；SURF是用近似DoH金字塔(即不同尺度的box filters)与图像做卷积，借助积分图，实际操作只涉及到数次简单的加减运算，而且不改变图像大小。

**②特征点检测**：SIFT是先进行非极大值抑制，去除对比度低的点，再通过Hessian矩阵剔除边缘点。而SURF是计算Hessian矩阵的行列式值(DoH)，再进行非极大值抑制。

**③特征点主方向**：SIFT在方形邻域窗口内统计梯度方向直方图，并对梯度幅值加权，取最大值对应的方向；SURF是在圆形区域内，计算各个扇形范围内x、y方向的Haar小波响应值，确定响应累加和值最大的扇形方向。

**④特征描述子**：SIFT将关键点附近的邻域划分为4×4的区域，统计每个子区域的梯度方向直方图，连接成一个4×4×8=128维的特征向量；SURF将邻域划分为4×4个子块，计算每个子块的Haar小波响应，并统计4个特征量，得到4×4×4=64维的特征向量。

总体来说，SURF和SIFT算法在特征点的检测取得了相似的性能，**SURF借助积分图，将模板卷积操作近似转换为加减运算，在计算速度方面要优于SIFT特征**



### LBP特征

局部二值模式(Local Binary Patter, LBP)是一种用**来描述图像局部纹理特征的算子**，LBP特征具有**灰度不变性和旋转不变性**等显著优点，它将图像中的各个像素与其邻域像素值进行比较，将结果保存为二进制数，并将得到的二进制比特串作为中心像素的编码值，也就是LBP特征值。LBP提供了一种衡量像素间邻域关系的特征模式，因此可以有效地提取图像的局部特征，而且由于其计算简单，可用于基于纹理分类的实时应用场景，例如目标检测、人脸识别等。



### HOG特征

方向梯度直方图(Histogram of Oriented Gradient, HOG)特征是一种在计算机视觉和图像处理中**用来进行物体检测的特征描述子**。它通过**计算和统计图像局部区域的梯度方向直方图来构成特征**。Hog特征结合SVM分类器已经被广泛应用于图像识别中，尤其在行人检测。



### SIFT，HOG和LBP比较

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpH4QN9iaib3ovLhCOLRB7ueeEOhdvMyhh5qiav1PfASUlUTQL6c3FogfuT9iavDXLiaaE9Vr2cZicAib4ibQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)





### 常用的特征检测算法

①LBP	②HOG	③SIFT	④SURF	⑤ORB:是Oriented Brief的简称，是brief算法的改进版，综合性能相对较好的算法



### opencv中主要有哪些模块



- Core —— 核心组件模块

包括基本数据结构、动态数据结构、绘图函数、数组操作相关函数、辅助功能与系统函数和宏、XML/YML、聚类、与OpenGL 的交互操作

- Imgproc 图像处理模块

包括图像滤波、几何图像变换、混合图像变换、直方图、结构分析及形状描述、运动分析及目标跟踪、特征及目标检测

- Highgui——顶层GUI及视频I/O

包括用户界面、读/写图像及视频、QT新功能

- Video——视频分析

包括运动分析及目标跟踪

- Calib3d——摄像机标定及3维重建

包括摄像机标定及3维重建

- Features2d——2维特征框架

包括特征检测与描述、特征检测提取匹配接口、关键点与匹配点绘图及对象分类

- Objdetect——目标检测

包括级联分类器及SVM

- MI——机器学习

包括统计模型、贝叶斯分类器、最近邻分类器、支持向量机、决策树、提升、梯度提升树、随机树、超随机树、最大期望、神经网络及机器学习数据

- FLann——聚类及多维空间搜索

快速最近邻搜索及聚类

- Gpu——计算机视觉中GPU加速
- Photo——计算图像

图像修复及去噪

- Stitching——图像拼接

图像拼接顶层操作函数、旋转、自动标定、仿射变换、接缝估计、曝光补充及图像融合技术



### opencv中CV_8UC3

8表示8位，UC--代表--unsigned int--无符号整形，3 -代表一张图片的通道数3



### opencv中的Scalar类

Scalar（）表示具有4个元素的数组，在opencv中被大量用于传递像素值，比如RGB颜色值。如果用不到第四个参数，则不需要写出来，若只写三个参数，则opencv会认为只需要传递三个参数



### 简述.hpp和.h的区别

.hpp，本质就是将.cpp的实现代码混入.h头文件当中，定义与实现都包含在同一文件，则该类的调用者只需要include该.hpp文件即可，无需再将cpp加入到project中进行编译。而实现代码将直接编译到调用者的obj文件中，不再生成单独的obj，采用hpp将大幅度减少调用project中的cpp文件数与编译次数，也不用再发布lib与dll文件，因此非常适合用来编写公用的开源库。



### 什么是光流

光流是关于视域中的**物体运动检测**中的概念，用来描述**相对于观察者的运动所造成的观测目标、表面或边缘的运动**



### 常见的颜色系统

①**RGB**是最常见的颜色系统,采用**人眼相似的工作机制**，也被显示设备所采用

②**HSV和HLS**把颜色分解成**色调，饱和度和亮度**/明度，描述颜色更加自然，可以通过抛弃最后一个元素使算法对输入图像的光照条件不敏感

③**YCrCb**颜色系统在**JPEG**图像格式中广泛使用

④**CIELab**是一种在感知上均匀的颜色空间，适合用来**度量两个颜色之间的距离**



### 膨胀和腐蚀操作

- 膨胀和腐蚀都是对白色（高亮）部分进行操作的
- 膨胀是图像中的高亮部分进行膨胀，效果图拥有比原图更大的高亮区域，腐蚀是原图中的高亮部分被腐蚀，效果图拥有比原图更小的高亮区域
- 从数学原理上说，膨胀就是求局部最大值，并把这个最大值赋值给参考点指定像素，这样会使图像中高亮区域逐渐增长，腐蚀与之相反。



### 开运算的流程

开运算就是**先腐蚀后膨胀**的过程。可以用来**消除小物体**，在**纤细点处分离物体**，并且在**平滑较大物体的边界**的同时不明显改变其面积



### 闭运算的流程

闭运算就是**先膨胀后腐蚀**的过程，闭运算能够**排除小型黑色区域**



### 形态学梯度定义

形态学梯度是**膨胀图与腐蚀图之差**，对二值图进行这一操作可以**将团块的边缘突出出来**，可以**用形态学梯度来保留物体的边缘轮廓**



### 顶帽（礼帽）运算的定义

**原图像与开运算的结果图之差**，得到的效果图突出了比原图轮廓周围区域**更加明亮的区域**，顶帽运算常用来分离比临近点亮一些的斑块。可以使用顶帽运算进行**背景提取**



### 黑帽运算的定义

**闭运算结果图与原图像之差**，黑帽运算后的效果图突出了比原图轮廓周围的区域**更暗的区域**，黑帽运算用来分离比邻近点暗一些的斑块。效果图有着**非常完美的轮廓**



### 漫水填充法

漫水填充法是一种**用特定的颜色填充算法填充连通区域**，通过设置可连通像素的上下限以及连通方式来达到不同的填充效果的方法。简单说，就是自动选中和种子点相连的区域，接着将该区域替换成指定的颜色。



### 仿射变换的定义

仿射变换是指在几何中，**一个向量空间进行一次线性变换并接上一个平移**，变换为另一个向量空间的过程。它保持了二维图形的**平直性**（直线变换之后依然是直线）和**平行性**。**一个任意的仿射变换都能表示为乘以一个矩阵（线性变换）接着再加上一个向量（平移）的形式**



### 图像金字塔的种类和区别

**高斯金字塔**: 用来向下采样

**拉普拉斯金字塔**: 用来向上采样



### 凸包的定义

给定二维平面上的点集，凸包就是将最外层的点连接起来构成的凸多边形，它能包含点集中所有的点。理解物体形状或轮廓的一种比较有用的方法便是计算一个物体的凸包



### 反向投影的定义

就是首先计算某一特征的直方图模型，然后使用模型去寻找图像中存在的该特征

反向投影用于在输入图像（通常较大）中查找与特定图像（通常较小）最匹配的点或者区域，也就是定位模板图像出现在输入图像的位置



### harris角点检测

harris角点检测时一种**直接基于灰度图像的角点提取算法**，稳定性高，尤其对于L型角点检测精度高。但是**由于采用了高斯滤波，运算速度相对比较的慢，角点信息有丢失和位置偏移的现象，而且角点提取有聚簇现象**



### 分水岭算法

分水岭算法是一种图像区域分割法，在分割的过程中，它会把**跟临近像素间的相似性作为重要的参考依据**，从而**将在空间位置上相近并且灰度值相近的像素点互相连接起来构成一个封闭的轮廓**，**封闭性**是分水岭算法的一个重要特征。

分水岭算法常用的操作步骤：彩色图像灰度化，然后再求梯度图，最后在梯度图的基础上进行分水岭算法，求得分段图像的边缘线